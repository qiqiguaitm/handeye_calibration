#!/usr/bin/env python3
"""
æ‰‹åŠ¨æ‰‹çœ¼æ ‡å®š - å¢å¼ºç‰ˆ
åŠŸèƒ½ï¼šæ‰‹åŠ¨æ¨¡å¼ + è½¨è¿¹é‡æ’­æ¨¡å¼

Version: 2.1.0
Changelog:
  - 2.1.0: é‡æ„ä»£ç æ¶ˆé™¤é‡å¤é€»è¾‘ï¼Œæå– _prepare_calibration_data å…¬å…±æ–¹æ³•
  - 2.0.0: ç»Ÿä¸€æ•°æ®è·¯å¾„ç®¡ç†ï¼Œæ‰€æœ‰æ•°æ®ä¿å­˜åˆ° calibration_data/ï¼Œæ”¯æŒ verified_data/ è¯»å–
  - 1.0.0: åˆå§‹ç‰ˆæœ¬
"""

__version__ = "2.1.0"

import sys
import os
import cv2
import numpy as np
import json
import yaml
import glob
import time
from datetime import datetime
from scipy.spatial.transform import Rotation as R

sys.path.append('/home/agilex/MobileManipulator/arm_robot/src')
from robot_piper import PiperRobot, create_config


class ManualHandEyeCalibrator:
    def __init__(self):
        self.robot = None
        self.pipeline = None
        self.pipeline_started = False

        # æ•°æ®è·¯å¾„é…ç½® - æ¶ˆé™¤è·¯å¾„ç¡¬ç¼–ç 
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.calibration_data_dir = os.path.join(self.script_dir, "calibration_data")
        self.verified_data_dir = os.path.join(self.script_dir, "verified_data")
        os.makedirs(self.calibration_data_dir, exist_ok=True)
        os.makedirs(self.verified_data_dir, exist_ok=True)

        # åˆå§‹ä½ç½®å’Œé›¶ç‚¹ä½ç½®
        self.initial_position = [300, 0, 300, 180, 60, 180]  # åˆå§‹å®‰å…¨ä½ç½®

        # æ£‹ç›˜æ ¼å‚æ•°
        self.board_size = (6, 4)  # 6x4æ£‹ç›˜æ ¼
        self.criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
        self.chessboard_size_mm = 50.0  # å•ä¸ªæ ¼å­50mm

        # ç›¸æœºå†…å‚
        self.camera_matrix = None
        self.dist_coeffs = None

        # ç¨³å®šæ€§ä¼˜åŒ–é…ç½®
        self.motion_config = {
            'warmup_speed': 40,          # é¢„çƒ­é€Ÿåº¦ (ä»60é™åˆ°40,å‡å°‘æœºæ¢°ç£¨æŸ)
            'normal_speed': 25,          # æ­£å¸¸è¿åŠ¨é€Ÿåº¦ (ä»30é™åˆ°25,å‡å°‘æƒ¯æ€§)
            'capture_speed': 20,         # é‡‡é›†æ—¶é€Ÿåº¦ (ä»50é™åˆ°20,æœ€å°åŒ–æŒ¯åŠ¨)
            'stability_wait': 5.0,       # ç¨³å®šç­‰å¾…æ—¶é—´ (ä»3.0å¢åŠ åˆ°5.0ç§’)
            'extra_settle_time': 2.0,    # é¢å¤–é™æ­¢æ—¶é—´ (æ–°å¢,è®©æŒ¯åŠ¨å®Œå…¨æ¶ˆå¤±)
            'warmup_duration': 900       # é¢„çƒ­æ—¶é•¿15åˆ†é’Ÿ (900ç§’,è¾¾åˆ°<0.5Â°ç²¾åº¦çš„æœ€å°è¦æ±‚)
        }

        print("ğŸ¯ æ‰‹åŠ¨æ‰‹çœ¼æ ‡å®šç³»ç»Ÿ")
        print("=" * 60)

    def normalize_angle_deg(self, angle_deg):
        """å½’ä¸€åŒ–è§’åº¦åˆ° [-180, 180] åº¦

        æ¶ˆé™¤è¾¹ç•Œæƒ…å†µ - Good TasteåŸåˆ™
        """
        while angle_deg > 180.0:
            angle_deg -= 360.0
        while angle_deg <= -180.0:
            angle_deg += 360.0
        return angle_deg

    def angle_difference_deg(self, angle1_deg, angle2_deg):
        """è®¡ç®—ä¸¤ä¸ªè§’åº¦çš„æœ€å°å·®å¼‚ï¼ˆåº¦ï¼‰

        æ­£ç¡®å¤„ç†è·¨è¶Š Â±180Â° è¾¹ç•Œçš„æƒ…å†µ
        """
        angle1 = self.normalize_angle_deg(angle1_deg)
        angle2 = self.normalize_angle_deg(angle2_deg)
        diff = abs(angle1 - angle2)
        if diff > 180.0:
            diff = 360.0 - diff
        return diff

    def filter_by_yaw_deviation(self, collected_data, reference_yaw_deg, max_deviation=30.0):
        """åŸºäºYawåå·®è¿‡æ»¤æ•°æ®

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            reference_yaw_deg: å‚è€ƒYawè§’åº¦ï¼ˆåº¦ï¼‰
            max_deviation: æœ€å¤§å…è®¸åå·®ï¼ˆåº¦ï¼‰

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
        """
        if not collected_data:
            return []

        filtered_data = []
        removed_frames = []

        print(f"\nğŸ” Yawåå·®è¿‡æ»¤ (å‚è€ƒ: {reference_yaw_deg:.1f}Â°, é™åˆ¶: Â±{max_deviation:.1f}Â°)")

        for i, data in enumerate(collected_data):
            # è·å–ä½å§¿ï¼ˆæ ¼å¼ï¼š[x, y, z, roll, pitch, yaw] å•ä½ï¼šç±³å’Œå¼§åº¦ï¼‰
            pose = data.get('pose', [])
            if len(pose) < 6:
                continue

            # Yawè§’åº¦ï¼ˆå¼§åº¦è½¬åº¦ï¼‰
            yaw_rad = pose[5] if len(pose) == 6 else pose[2]
            yaw_deg = np.rad2deg(yaw_rad)

            # è®¡ç®—åå·®
            deviation = self.angle_difference_deg(yaw_deg, reference_yaw_deg)

            if deviation <= max_deviation:
                filtered_data.append(data)
            else:
                frame_id = data.get('frame_id', i)
                removed_frames.append((frame_id, yaw_deg, deviation))
                print(f"   âŒ å¸§ {frame_id}: Yaw={yaw_deg:.1f}Â°, åå·®={deviation:.1f}Â° (è¶…é™)")

        if removed_frames:
            print(f"\nğŸ”§ Yawè¿‡æ»¤ç»“æœ: ç§»é™¤ {len(removed_frames)}/{len(collected_data)} å¸§")
            print(f"   ä¿ç•™: {len(filtered_data)} å¸§")
        else:
            print(f"   âœ… æ‰€æœ‰å¸§Yawåå·®å‡åœ¨é™åˆ¶èŒƒå›´å†…")

        return filtered_data

    def filter_insufficient_motion(self, collected_data, min_motion_mm=5.0, min_rotation_deg=2.0, require_both=True):
        """è¿‡æ»¤è¿åŠ¨ä¸è¶³çš„å¸§

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            min_motion_mm: æœ€å°ä½ç§»é˜ˆå€¼ï¼ˆæ¯«ç±³ï¼‰
            min_rotation_deg: æœ€å°æ—‹è½¬é˜ˆå€¼ï¼ˆåº¦ï¼‰
            require_both: æ˜¯å¦è¦æ±‚ä½ç§»å’Œæ—‹è½¬éƒ½æ»¡è¶³ï¼ˆæ¨èTrueï¼‰

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
            removed_frames: è¢«ç§»é™¤çš„å¸§ä¿¡æ¯

        è¯´æ˜:
            - require_both=True: ä½ç§»ANDæ—‹è½¬éƒ½è¦æ»¡è¶³ï¼ˆæ¨èï¼Œç¡®ä¿6è‡ªç”±åº¦éƒ½æœ‰çº¦æŸï¼‰
            - require_both=False: ä½ç§»ORæ—‹è½¬æ»¡è¶³ä¸€ä¸ªå³å¯ï¼ˆå¯èƒ½å¯¼è‡´é€€åŒ–ï¼‰
        """
        if len(collected_data) < 2:
            return collected_data, []

        filtered_data = [collected_data[0]]  # ä¿ç•™ç¬¬ä¸€å¸§
        removed_frames = []
        last_kept_pose = collected_data[0]['pose']

        for i in range(1, len(collected_data)):
            current_data = collected_data[i]
            current_pose = current_data['pose']

            # è®¡ç®—ä¸ä¸Šä¸€ä¸ªä¿ç•™å¸§çš„ä½ç§»ï¼ˆç±³è½¬æ¯«ç±³ï¼‰
            pos_diff = np.linalg.norm(
                np.array(current_pose[:3]) - np.array(last_kept_pose[:3])
            ) * 1000

            # è®¡ç®—æ—‹è½¬å·®å¼‚
            R1 = R.from_euler('xyz', last_kept_pose[3:])
            R2 = R.from_euler('xyz', current_pose[3:])
            R_diff = R2 * R1.inv()
            angle_diff = np.degrees(R_diff.magnitude())

            # åˆ¤æ–­è¿åŠ¨æ˜¯å¦å……åˆ†
            if require_both:
                # åŒæ—¶æ»¡è¶³ä½ç§»å’Œæ—‹è½¬è¦æ±‚ï¼ˆæ¨èï¼‰
                motion_sufficient = (pos_diff >= min_motion_mm and angle_diff >= min_rotation_deg)
                if not motion_sufficient:
                    # åˆ†æå…·ä½“åŸå› 
                    if pos_diff < min_motion_mm and angle_diff < min_rotation_deg:
                        reason = 'ä½ç§»å’Œæ—‹è½¬éƒ½ä¸è¶³'
                    elif pos_diff < min_motion_mm:
                        reason = f'ä½ç§»ä¸è¶³({pos_diff:.1f}mm<{min_motion_mm}mm)'
                    else:
                        reason = f'æ—‹è½¬ä¸è¶³({angle_diff:.2f}Â°<{min_rotation_deg}Â°)'
            else:
                # ä½ç§»æˆ–æ—‹è½¬æ»¡è¶³ä¸€ä¸ªå³å¯ï¼ˆä¸æ¨èï¼‰
                motion_sufficient = (pos_diff >= min_motion_mm or angle_diff >= min_rotation_deg)
                if not motion_sufficient:
                    reason = 'ä½ç§»å’Œæ—‹è½¬éƒ½ä¸è¶³'

            if motion_sufficient:
                filtered_data.append(current_data)
                last_kept_pose = current_pose
            else:
                removed_frames.append({
                    'frame_id': current_data['frame_id'],
                    'reason': reason,
                    'motion_mm': pos_diff,
                    'rotation_deg': angle_diff
                })

        return filtered_data, removed_frames

    def filter_workspace_boundary(self, collected_data, boundary_margin_ratio=0.05, min_boundary_axes=2):
        """æ™ºèƒ½è¿‡æ»¤å·¥ä½œç©ºé—´è¾¹ç•Œçš„å¸§

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            boundary_margin_ratio: è¾¹ç•Œä½™é‡æ¯”ä¾‹ï¼ˆ0.05è¡¨ç¤ºè¾¹ç•Œ5%åŒºåŸŸï¼‰
            min_boundary_axes: æœ€å°‘å‡ ä¸ªè½´åœ¨è¾¹ç•Œæ‰è¿‡æ»¤ï¼ˆ1=ä»»æ„è½´, 2=ä¸¤ä¸ªè½´, 3=ä¸‰ä¸ªè½´ï¼‰

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
            removed_frames: è¢«ç§»é™¤çš„å¸§ä¿¡æ¯

        è¯´æ˜:
            - min_boundary_axes=1: ä¸¥æ ¼æ¨¡å¼ï¼Œä»»ä½•è½´æ¥è¿‘è¾¹ç•Œéƒ½è¿‡æ»¤ï¼ˆå¯èƒ½è¿‡æ¿€ï¼‰
            - min_boundary_axes=2: å¹³è¡¡æ¨¡å¼ï¼Œè‡³å°‘2ä¸ªè½´åŒæ—¶æ¥è¿‘è¾¹ç•Œæ‰è¿‡æ»¤ï¼ˆæ¨èï¼‰
            - min_boundary_axes=3: å®½æ¾æ¨¡å¼ï¼Œ3ä¸ªè½´éƒ½æ¥è¿‘è¾¹ç•Œæ‰è¿‡æ»¤
        """
        if len(collected_data) < 3:
            return collected_data, []

        # æå–æ‰€æœ‰ä½ç½®
        positions = np.array([d['pose'][:3] for d in collected_data])

        # è®¡ç®—å·¥ä½œç©ºé—´èŒƒå›´
        x_range = [positions[:, 0].min(), positions[:, 0].max()]
        y_range = [positions[:, 1].min(), positions[:, 1].max()]
        z_range = [positions[:, 2].min(), positions[:, 2].max()]

        # è®¡ç®—è¾¹ç•Œä½™é‡
        x_margin = (x_range[1] - x_range[0]) * boundary_margin_ratio
        y_margin = (y_range[1] - y_range[0]) * boundary_margin_ratio
        z_margin = (z_range[1] - z_range[0]) * boundary_margin_ratio

        filtered_data = []
        removed_frames = []

        for data in collected_data:
            x, y, z = data['pose'][:3]

            # æ£€æŸ¥æ¯ä¸ªè½´æ˜¯å¦åœ¨è¾¹ç•Œ
            boundary_axes = []

            if x < x_range[0] + x_margin or x > x_range[1] - x_margin:
                boundary_axes.append('X')

            if y < y_range[0] + y_margin or y > y_range[1] - y_margin:
                boundary_axes.append('Y')

            if z < z_range[0] + z_margin or z > z_range[1] - z_margin:
                boundary_axes.append('Z')

            boundary_count = len(boundary_axes)

            # æ ¹æ®è¾¹ç•Œè½´æ•°é‡åˆ¤æ–­æ˜¯å¦è¿‡æ»¤
            should_filter = boundary_count >= min_boundary_axes

            if should_filter:
                removed_frames.append({
                    'frame_id': data['frame_id'],
                    'reason': f"è¾¹ç•Œä½ç½®({'+'.join(boundary_axes)}è½´)",
                    'position': [x*1000, y*1000, z*1000],  # è½¬ä¸ºæ¯«ç±³
                    'boundary_count': boundary_count
                })
            else:
                filtered_data.append(data)

        return filtered_data, removed_frames

    def filter_extreme_poses(self, collected_data, max_pitch_deg=70.0, verbose=True):
        """è¿‡æ»¤æç«¯å§¿æ€è§’çš„å¸§ï¼ˆåŸºäºç»å¯¹é˜ˆå€¼ï¼‰

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            max_pitch_deg: æœ€å¤§å…è®¸Pitchè§’ç»å¯¹å€¼ï¼ˆåº¦ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
            removed_frames: è¢«ç§»é™¤çš„å¸§ä¿¡æ¯

        è¯´æ˜:
            æç«¯Pitchè§’ä¼šå¯¼è‡´ï¼š
            1. ç›¸æœºå†…å‚ç•¸å˜æ¨¡å‹è¯¯å·®æ”¾å¤§
            2. æ£‹ç›˜æ ¼æŠ•å½±å˜å½¢ï¼Œè§’ç‚¹æ£€æµ‹ç²¾åº¦ä¸‹é™
            3. æœºæ¢°è‡‚é‡å¤æ€§é™ä½ï¼ˆæ¥è¿‘å¥‡å¼‚ç‚¹ï¼‰
        """
        if len(collected_data) < 3:
            return collected_data, []

        filtered_data = []
        removed_frames = []

        for data in collected_data:
            roll, pitch, yaw = data['pose'][3:6]  # æ¬§æ‹‰è§’ (rad)

            # è½¬æ¢ä¸ºè§’åº¦
            roll_deg = np.degrees(roll)
            pitch_deg = np.degrees(pitch)
            yaw_deg = np.degrees(yaw)

            # åˆ¤æ–­Pitchæ˜¯å¦è¿‡å¤§
            pitch_extreme = abs(pitch_deg) > max_pitch_deg

            if pitch_extreme:
                removed_frames.append({
                    'frame_id': data['frame_id'],
                    'reason': f"Pitch={pitch_deg:.1f}Â°è¶…é™(>{max_pitch_deg}Â°)",
                    'roll_deg': roll_deg,
                    'pitch_deg': pitch_deg,
                    'yaw_deg': yaw_deg
                })
            else:
                filtered_data.append(data)

        return filtered_data, removed_frames

    def filter_consecutive_outliers(self, collected_data, known_outliers=None):
        """è¿‡æ»¤è¿ç»­å¼‚å¸¸å¸§

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            known_outliers: å·²çŸ¥çš„å¼‚å¸¸å¸§IDåˆ—è¡¨

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
            removed_frames: è¢«ç§»é™¤çš„å¸§ä¿¡æ¯
        """
        if known_outliers is None or len(known_outliers) == 0:
            return collected_data, []

        # æ‰¾å‡ºè¿ç»­çš„å¼‚å¸¸å¸§
        known_outliers_sorted = sorted(known_outliers)
        consecutive_groups = []
        current_group = [known_outliers_sorted[0]]

        for i in range(1, len(known_outliers_sorted)):
            if known_outliers_sorted[i] == current_group[-1] + 1:
                current_group.append(known_outliers_sorted[i])
            else:
                if len(current_group) >= 2:  # è‡³å°‘2ä¸ªè¿ç»­
                    consecutive_groups.append(current_group)
                current_group = [known_outliers_sorted[i]]

        # æ£€æŸ¥æœ€åä¸€ç»„
        if len(current_group) >= 2:
            consecutive_groups.append(current_group)

        # ç§»é™¤è¿ç»­å¼‚å¸¸å¸§
        frames_to_remove = set()
        for group in consecutive_groups:
            frames_to_remove.update(group)

        filtered_data = []
        removed_frames = []

        for data in collected_data:
            if data['frame_id'] in frames_to_remove:
                removed_frames.append({
                    'frame_id': data['frame_id'],
                    'reason': 'è¿ç»­å¼‚å¸¸å¸§',
                    'group': next(g for g in consecutive_groups if data['frame_id'] in g)
                })
            else:
                filtered_data.append(data)

        return filtered_data, removed_frames

    def apply_quality_filters(self, collected_data, verbose=True):
        """åº”ç”¨æ‰€æœ‰æ•°æ®è´¨é‡è¿‡æ»¤å™¨

        Args:
            collected_data: åŸå§‹é‡‡é›†æ•°æ®
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            filtered_data: è¿‡æ»¤åçš„æ•°æ®
            filter_report: è¿‡æ»¤æŠ¥å‘Š
        """
        if verbose:
            print("\n" + "="*60)
            print("ğŸ” æ•°æ®è´¨é‡è¿‡æ»¤")
            print("="*60)
            print(f"åŸå§‹æ•°æ®: {len(collected_data)} å¸§")

        original_count = len(collected_data)
        all_removed = []

        # ç¬¬1å±‚ï¼šè¿‡æ»¤è¿åŠ¨ä¸è¶³çš„å¸§
        if verbose:
            print("\næ­¥éª¤1: è¿‡æ»¤è¿åŠ¨ä¸è¶³çš„å¸§ (ä½ç§»ANDæ—‹è½¬éƒ½éœ€æ»¡è¶³)")

        filtered_data, removed = self.filter_insufficient_motion(
            collected_data,
            min_motion_mm=5.0,
            min_rotation_deg=2.0,
            require_both=True  # è¦æ±‚ä½ç§»å’Œæ—‹è½¬éƒ½æ»¡è¶³
        )

        if verbose and removed:
            print(f"  ç§»é™¤ {len(removed)} å¸§:")
            for r in removed[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    å¸§{r['frame_id']}: ä½ç§»={r['motion_mm']:.1f}mm, æ—‹è½¬={r['rotation_deg']:.1f}Â°")
            if len(removed) > 5:
                print(f"    ... è¿˜æœ‰ {len(removed)-5} å¸§")

        all_removed.extend(removed)

        # ç¬¬2å±‚ï¼šè¿‡æ»¤å·¥ä½œç©ºé—´è¾¹ç•Œçš„å¸§
        if verbose:
            print("\næ­¥éª¤2: è¿‡æ»¤å·¥ä½œç©ºé—´è¾¹ç•Œå¸§ (æ¿€è¿›æ¨¡å¼: å•è½´è¾¹ç•Œ10%å³è¿‡æ»¤)")

        filtered_data, removed = self.filter_workspace_boundary(
            filtered_data,
            boundary_margin_ratio=0.10,  # 10%è¾¹ç•Œï¼ˆè¿‡æ»¤æé™ä½ç½®ï¼‰
            min_boundary_axes=1  # ä»»æ„1ä¸ªè½´åœ¨è¾¹ç•Œå³è¿‡æ»¤ï¼ˆæ¿€è¿›ç­–ç•¥ï¼Œæå‡ç²¾åº¦ï¼‰
        )

        if verbose and removed:
            print(f"  ç§»é™¤ {len(removed)} å¸§:")
            for r in removed[:5]:
                pos = r['position']
                print(f"    å¸§{r['frame_id']}: {r['reason']}, ä½ç½®=[{pos[0]:.1f}, {pos[1]:.1f}, {pos[2]:.1f}]mm")
            if len(removed) > 5:
                print(f"    ... è¿˜æœ‰ {len(removed)-5} å¸§")

        all_removed.extend(removed)

        # ç¬¬3å±‚ï¼šè¿‡æ»¤æç«¯å§¿æ€è§’çš„å¸§ï¼ˆåŸºäºç»å¯¹é˜ˆå€¼ï¼‰
        if verbose:
            print("\næ­¥éª¤3: è¿‡æ»¤æç«¯å§¿æ€è§’å¸§ (Pitch>70Â°)")

        filtered_data, removed = self.filter_extreme_poses(
            filtered_data,
            max_pitch_deg=70.0,  # Pitchè§’åº¦è¶…è¿‡70Â°åˆ™è¿‡æ»¤
            verbose=verbose
        )

        if verbose and removed:
            print(f"  ç§»é™¤ {len(removed)} å¸§:")
            for r in removed[:5]:
                print(f"    å¸§{r['frame_id']}: {r['reason']}")
                print(f"      å§¿æ€: Roll={r['roll_deg']:.1f}Â° Pitch={r['pitch_deg']:.1f}Â° Yaw={r['yaw_deg']:.1f}Â°")
            if len(removed) > 5:
                print(f"    ... è¿˜æœ‰ {len(removed)-5} å¸§")

        all_removed.extend(removed)

        # ç¬¬4å±‚ï¼šè¿‡æ»¤å·²çŸ¥çš„è¿ç»­å¼‚å¸¸å¸§ï¼ˆå¯é€‰ï¼‰
        # æ³¨æ„ï¼šè¿™ä¸ªéœ€è¦å…ˆè¿è¡Œä¸€æ¬¡æ ‡å®šæ‰çŸ¥é“å“ªäº›æ˜¯å¼‚å¸¸å¸§
        # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œå› ä¸ºç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¸çŸ¥é“å¼‚å¸¸å¸§
        """
        if verbose:
            print("\næ­¥éª¤4: è¿‡æ»¤è¿ç»­å¼‚å¸¸å¸§")

        known_outliers = [35, 36, 71, 76, 79, 81]  # ä»è¯Šæ–­ç»“æœè·å–
        filtered_data, removed = self.filter_consecutive_outliers(
            filtered_data,
            known_outliers=known_outliers
        )

        if verbose and removed:
            print(f"  ç§»é™¤ {len(removed)} å¸§:")
            for r in removed:
                print(f"    å¸§{r['frame_id']}: {r['reason']}")

        all_removed.extend(removed)
        """

        # ç»Ÿè®¡
        final_count = len(filtered_data)
        removed_count = original_count - final_count

        if verbose:
            print("\n" + "-"*60)
            print("ğŸ“Š è¿‡æ»¤ç»Ÿè®¡:")
            print(f"  åŸå§‹: {original_count} å¸§")
            print(f"  ä¿ç•™: {final_count} å¸§ ({final_count/original_count*100:.1f}%)")
            print(f"  ç§»é™¤: {removed_count} å¸§ ({removed_count/original_count*100:.1f}%)")

            # æŒ‰åŸå› åˆ†ç»„ç»Ÿè®¡
            reason_stats = {}
            for r in all_removed:
                reason = r['reason']
                reason_stats[reason] = reason_stats.get(reason, 0) + 1

            if reason_stats:
                print("\n  ç§»é™¤åŸå› ç»Ÿè®¡:")
                for reason, count in reason_stats.items():
                    print(f"    {reason}: {count} å¸§")

        filter_report = {
            'original_count': original_count,
            'final_count': final_count,
            'removed_count': removed_count,
            'removed_frames': all_removed,
            'removal_rate': removed_count / original_count if original_count > 0 else 0
        }

        return filtered_data, filter_report

    def ransac_filter_handeye(self, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, frame_ids, threshold=6.0):
        """æ”¹è¿›çš„RANSACè¿‡æ»¤å¼‚å¸¸å€¼ - ä½¿ç”¨AX=XBä¸€è‡´æ€§è¯¯å·®

        Args:
            threshold: å†…ç‚¹é˜ˆå€¼ï¼ˆmmï¼‰

        Returns:
            list: å†…ç‚¹ç´¢å¼•åˆ—è¡¨
        """
        n_samples = len(R_gripper2base)
        if n_samples < 5:
            return list(range(n_samples))

        best_inliers = []
        best_avg_error = float('inf')
        best_median_error = float('inf')

        # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜é²æ£’æ€§
        iterations = min(200, n_samples * 20)

        for iteration in range(iterations):
            # ä½¿ç”¨æ›´å¤šæ ·æœ¬ç‚¹ï¼ˆ8ä¸ªï¼‰æé«˜è¿åŠ¨å¤šæ ·æ€§ï¼Œå‡å°‘"è¿åŠ¨ä¸è¶³"é”™è¯¯
            # 8ä¸ªç‚¹é€šå¸¸è¶³ä»¥è¦†ç›–è¶³å¤Ÿçš„æ—‹è½¬å˜åŒ–
            sample_size = min(8, n_samples)
            sample_indices = np.random.choice(n_samples, sample_size, replace=False)

            try:
                # ä½¿ç”¨æ ·æœ¬è®¡ç®—æ ‡å®š
                R_sample, t_sample = cv2.calibrateHandEye(
                    [R_gripper2base[i] for i in sample_indices],
                    [t_gripper2base[i] for i in sample_indices],
                    [R_target2cam[i] for i in sample_indices],
                    [t_target2cam[i] for i in sample_indices],
                    method=cv2.CALIB_HAND_EYE_TSAI
                )

                # ä½¿ç”¨AX=XBæ–¹ç¨‹è®¡ç®—æ‰€æœ‰ç‚¹çš„ä¸€è‡´æ€§è¯¯å·®
                errors = []
                for i in range(n_samples):
                    # è®¡ç®—AX=XBè¯¯å·®ï¼Œè€Œéç›¸å¯¹è¯¯å·®
                    # A*Xåº”è¯¥ç­‰äºX*B
                    # Aæ˜¯gripperåˆ°baseçš„å˜æ¢ï¼ŒXæ˜¯ç›¸æœºåˆ°gripperçš„å˜æ¢ï¼ŒBæ˜¯targetåˆ°ç›¸æœºçš„å˜æ¢

                    # è®¡ç®—AX
                    AX_R = R_gripper2base[i] @ R_sample
                    AX_t = R_gripper2base[i] @ t_sample + t_gripper2base[i]

                    # è®¡ç®—XB
                    XB_R = R_sample @ R_target2cam[i]
                    XB_t = R_sample @ t_target2cam[i] + t_sample

                    # è®¡ç®—æ—‹è½¬è¯¯å·®ï¼ˆä½¿ç”¨è§’åº¦å·®ï¼‰
                    R_diff = AX_R @ XB_R.T
                    angle_error = np.arccos(np.clip((np.trace(R_diff) - 1) / 2, -1, 1))

                    # è®¡ç®—å¹³ç§»è¯¯å·®ï¼ˆæ¯«ç±³ï¼‰
                    t_error = np.linalg.norm(AX_t - XB_t) * 1000

                    # ç»¼åˆè¯¯å·®ï¼ˆæ—‹è½¬è¯¯å·®è½¬æ¢ä¸ºæ¯«ç±³å½“é‡ï¼‰
                    # å‡è®¾10åº¦æ—‹è½¬è¯¯å·®ç›¸å½“äº10mmå¹³ç§»è¯¯å·®
                    total_error = t_error + np.degrees(angle_error) * 1.0
                    errors.append(total_error)

                errors_array = np.array(errors)

                # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼ˆåŸºäºä¸­ä½æ•°å’ŒMADï¼‰
                median_error = np.median(errors_array)
                mad = np.median(np.abs(errors_array - median_error))

                # åŠ¨æ€é˜ˆå€¼ï¼šä¸­ä½æ•° + 2.5å€MAD
                dynamic_threshold = min(threshold, median_error + 2.5 * mad)

                # æ‰¾å‡ºå†…ç‚¹
                inliers = [i for i, e in enumerate(errors) if e < dynamic_threshold]

                # è¦æ±‚è‡³å°‘30%çš„æ•°æ®ä¸ºå†…ç‚¹
                min_inliers = max(4, int(n_samples * 0.3))

                if len(inliers) >= min_inliers:
                    # ä½¿ç”¨å†…ç‚¹é‡æ–°è®¡ç®—ä»¥è·å¾—æ›´å‡†ç¡®çš„ä¼°è®¡
                    try:
                        R_refined, t_refined = cv2.calibrateHandEye(
                            [R_gripper2base[i] for i in inliers],
                            [t_gripper2base[i] for i in inliers],
                            [R_target2cam[i] for i in inliers],
                            [t_target2cam[i] for i in inliers],
                            method=cv2.CALIB_HAND_EYE_TSAI
                        )

                        # é‡æ–°è®¡ç®—å†…ç‚¹çš„è¯¯å·®
                        refined_errors = []
                        for i in inliers:
                            AX_R = R_gripper2base[i] @ R_refined
                            AX_t = R_gripper2base[i] @ t_refined + t_gripper2base[i]
                            XB_R = R_refined @ R_target2cam[i]
                            XB_t = R_refined @ t_target2cam[i] + t_refined
                            t_error = np.linalg.norm(AX_t - XB_t) * 1000
                            refined_errors.append(t_error)

                        avg_error = np.mean(refined_errors)
                        median_error = np.median(refined_errors)

                        # ä¼˜å…ˆé€‰æ‹©æ›´å¤šå†…ç‚¹ï¼Œå…¶æ¬¡é€‰æ‹©æ›´å°çš„ä¸­ä½æ•°è¯¯å·®
                        if len(inliers) > len(best_inliers) or \
                           (len(inliers) == len(best_inliers) and median_error < best_median_error):
                            best_inliers = inliers
                            best_avg_error = avg_error
                            best_median_error = median_error
                    except:
                        pass

            except Exception as e:
                continue

        # å¦‚æœRANSACæœªæ‰¾åˆ°è¶³å¤Ÿå†…ç‚¹ï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ ‡å‡†
        if len(best_inliers) < max(4, int(n_samples * 0.3)):
            print(f"   âš ï¸ RANSACå†…ç‚¹ä¸è¶³ï¼Œä½¿ç”¨ä¿å®ˆçš„è¯¯å·®é˜ˆå€¼è¿‡æ»¤")
            # è®¡ç®—æ‰€æœ‰æ•°æ®ä½¿ç”¨Tsaiæ–¹æ³•çš„è¯¯å·®
            try:
                R_all, t_all = cv2.calibrateHandEye(
                    R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam,
                    method=cv2.CALIB_HAND_EYE_TSAI
                )
                errors = []
                for i in range(n_samples):
                    AX_t = R_gripper2base[i] @ t_all + t_gripper2base[i]
                    XB_t = R_all @ t_target2cam[i] + t_all
                    t_error = np.linalg.norm(AX_t - XB_t) * 1000
                    errors.append(t_error)

                # ä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥: ä¸­ä½æ•° + 3å€MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰
                # MADæ¯”æ ‡å‡†å·®æ›´é²æ£’ï¼Œä¸å—æç«¯å€¼å½±å“
                median = np.median(errors)
                mad = np.median(np.abs(np.array(errors) - median))
                # 3å€MADå¤§çº¦ç­‰ä»·äºæ­£æ€åˆ†å¸ƒçš„3Ïƒï¼ˆ99.7%ç½®ä¿¡åŒºé—´ï¼‰
                conservative_threshold = median + 3.0 * mad

                # åŒæ—¶è®¾ç½®ç»å¯¹ä¸Šé™ï¼š15mmï¼ˆè¿œå¤§äºæ­£å¸¸è¯¯å·®ï¼‰
                # è¿™é¿å…äº†åœ¨æ•°æ®æ•´ä½“è¾ƒå·®æ—¶è¿‡åº¦åˆ é™¤
                absolute_max = 15.0
                final_threshold = min(conservative_threshold, absolute_max)

                best_inliers = [i for i, e in enumerate(errors) if e < final_threshold]

                # å¦‚æœè¿˜æ˜¯è¿‡æ»¤å¤ªå¤šï¼ˆè¶…è¿‡20%ï¼‰ï¼Œåˆ™åªä¿ç•™æ‰€æœ‰æ•°æ®
                if len(best_inliers) < int(n_samples * 0.8):
                    print(f"   â„¹ï¸  ä¿å®ˆè¿‡æ»¤ä»ä¼šç§»é™¤è¿‡å¤šæ•°æ®ï¼Œä¿ç•™æ‰€æœ‰å¸§")
                    best_inliers = list(range(n_samples))
            except:
                best_inliers = list(range(n_samples))

        return best_inliers

    def filter_high_reprojection_error_frames(self, collected_data, max_error_px=2.0):
        """è¿‡æ»¤é‡æŠ•å½±è¯¯å·®è¿‡å¤§çš„å¸§

        Args:
            collected_data: é‡‡é›†çš„æ•°æ®åˆ—è¡¨
            max_error_px: æœ€å¤§å…è®¸é‡æŠ•å½±è¯¯å·®(åƒç´ ),é»˜è®¤2.0

        Returns:
            tuple: (filtered_data, removed_frames)
                - filtered_data: è¿‡æ»¤åçš„æ•°æ®åˆ—è¡¨
                - removed_frames: è¢«åˆ é™¤çš„å¸§IDåˆ—è¡¨
        """
        print(f"\nğŸ” è¿‡æ»¤é‡æŠ•å½±è¯¯å·® > {max_error_px}px çš„å¸§...")

        filtered_data = []
        removed_frames = []

        # å‡†å¤‡æ£‹ç›˜æ ¼3Dç‚¹
        objpoints = np.zeros((self.board_size[0] * self.board_size[1], 3), np.float32)
        objpoints[:, :2] = np.mgrid[0:self.board_size[0], 0:self.board_size[1]].T.reshape(-1, 2)
        objpoints *= self.chessboard_size_mm / 1000.0  # è½¬ä¸ºç±³

        for data in collected_data:
            frame_id = data['frame_id']
            imgpoints = data['corners']

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰rvec/tvec,å¦‚æœæ²¡æœ‰åˆ™è®¡ç®—
            if 'rvec' in data and 'tvec' in data:
                rvec = data['rvec']
                tvec = data['tvec']
            else:
                # ä½¿ç”¨PnPè®¡ç®—æ£‹ç›˜æ ¼ç›¸å¯¹äºç›¸æœºçš„ä½å§¿
                ret, rvec, tvec = cv2.solvePnP(
                    objpoints, imgpoints,
                    self.camera_matrix, self.dist_coeffs
                )
                if not ret:
                    # PnPå¤±è´¥,ç›´æ¥ç§»é™¤è¯¥å¸§
                    removed_frames.append(frame_id)
                    print(f"   âŒ å¸§ {frame_id}: PnPæ±‚è§£å¤±è´¥ (ç§»é™¤)")
                    continue

            # é‡æŠ•å½±
            projected_points, _ = cv2.projectPoints(
                objpoints, rvec, tvec,
                self.camera_matrix, self.dist_coeffs
            )

            # è®¡ç®—å¹³å‡é‡æŠ•å½±è¯¯å·®
            error = np.linalg.norm(
                imgpoints.reshape(-1, 2) - projected_points.reshape(-1, 2),
                axis=1
            ).mean()

            if error > max_error_px:
                removed_frames.append(frame_id)
                print(f"   âŒ å¸§ {frame_id}: é‡æŠ•å½±è¯¯å·® {error:.3f}px > {max_error_px}px (ç§»é™¤)")
            else:
                filtered_data.append(data)
                print(f"   âœ… å¸§ {frame_id}: é‡æŠ•å½±è¯¯å·® {error:.3f}px (ä¿ç•™)")

        print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
        print(f"   åŸå§‹å¸§æ•°: {len(collected_data)}")
        print(f"   ä¿ç•™å¸§æ•°: {len(filtered_data)}")
        print(f"   ç§»é™¤å¸§æ•°: {len(removed_frames)}")
        if removed_frames:
            print(f"   ç§»é™¤çš„å¸§: {removed_frames}")

        return filtered_data, removed_frames

    def multi_algorithm_fusion(self, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """å¤šç®—æ³•èåˆæ ‡å®š

        Returns:
            tuple: (æœ€ä¼˜R, æœ€ä¼˜t, ç®—æ³•åç§°)
        """
        methods = [
            (cv2.CALIB_HAND_EYE_TSAI, "Tsai"),
            (cv2.CALIB_HAND_EYE_PARK, "Park"),
            (cv2.CALIB_HAND_EYE_HORAUD, "Horaud"),
            (cv2.CALIB_HAND_EYE_DANIILIDIS, "Daniilidis"),
            (cv2.CALIB_HAND_EYE_ANDREFF, "Andreff")
        ]

        best_result = None
        best_score = float('inf')
        best_method = None

        for method_id, method_name in methods:
            try:
                R_test, t_test = cv2.calibrateHandEye(
                    R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam,
                    method=method_id
                )

                # è®¡ç®—å¹³ç§»å’Œæ—‹è½¬è¯¯å·®
                t_errors = []
                r_errors = []
                for i in range(len(R_gripper2base)):
                    R_pred = R_gripper2base[i] @ R_test @ R_target2cam[i]
                    t_pred = R_gripper2base[i] @ (R_test @ t_target2cam[i] + t_test) + t_gripper2base[i]

                    if i == 0:
                        R_ref = R_pred
                        t_ref = t_pred
                    else:
                        # å¹³ç§»è¯¯å·® (mm)
                        t_error = np.linalg.norm(t_pred - t_ref) * 1000
                        t_errors.append(t_error)

                        # æ—‹è½¬è¯¯å·® (degrees)
                        R_error = R_ref.T @ R_pred
                        r_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))
                        r_errors.append(r_error)

                avg_t_error = np.mean(t_errors) if t_errors else 0
                avg_r_error = np.mean(r_errors) if r_errors else 0

                # ç»¼åˆè¯„åˆ†: å½’ä¸€åŒ–ååŠ æƒæ±‚å’Œ
                # å¹³ç§»æƒé‡: 5mm = 1.0, æ—‹è½¬æƒé‡: 0.5Â° = 1.0
                # è¿™æ ·ä¸¤ä¸ªè¯¯å·®åœ¨ç›¸åŒé‡çº§ä¸‹æœ‰ç›¸åŒè´¡çŒ®
                score = (avg_t_error / 5.0) + (avg_r_error / 0.5)

                print(f"   {method_name}: å¹³ç§»={avg_t_error:.3f}mm, æ—‹è½¬={avg_r_error:.3f}Â°, ç»¼åˆ={score:.3f}")

                if score < best_score:
                    best_score = score
                    best_result = (R_test, t_test)
                    best_method = method_name

            except Exception as e:
                print(f"   {method_name}: å¤±è´¥")

        if best_result:
            return best_result[0], best_result[1], best_method
        return None, None, None

    def iterative_optimization(self, R_initial, t_initial, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """è¿­ä»£ä¼˜åŒ–æ ‡å®šç»“æœ

        Returns:
            tuple: (ä¼˜åŒ–åçš„R, ä¼˜åŒ–åçš„t)
        """
        from scipy.optimize import minimize

        def objective(params):
            # è§£æå‚æ•°
            rvec = params[:3]
            tvec = params[3:6]

            # è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            R_opt, _ = cv2.Rodrigues(rvec)
            t_opt = tvec.reshape(3, 1)

            # æ­£ç¡®çš„ç›®æ ‡å‡½æ•°: æœ€å°åŒ–ä½å§¿é‡å¤æ€§è¯¯å·®
            # è¿™æ˜¯è¯„ä¼°å‡½æ•°ä½¿ç”¨çš„ç›¸åŒæŒ‡æ ‡
            R_preds = []
            t_preds = []

            for i in range(len(R_gripper2base)):
                # è®¡ç®—åœ¨baseåæ ‡ç³»ä¸‹çš„targetä½å§¿
                R_pred = R_gripper2base[i] @ R_opt @ R_target2cam[i]
                t_pred = R_gripper2base[i] @ (R_opt @ t_target2cam[i] + t_opt) + t_gripper2base[i]
                R_preds.append(R_pred)
                t_preds.append(t_pred)

            # ä»¥ç¬¬ä¸€å¸§ä¸ºå‚è€ƒï¼Œè®¡ç®—æ‰€æœ‰å¸§çš„åå·®
            R_ref = R_preds[0]
            t_ref = t_preds[0]

            total_error = 0
            for i in range(1, len(R_preds)):
                # æ—‹è½¬è¯¯å·® (åº¦)
                R_error = R_ref.T @ R_preds[i]
                angle_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))

                # å¹³ç§»è¯¯å·® (mm)
                t_error = np.linalg.norm(t_preds[i] - t_ref) * 1000

                # ç»¼åˆè¯¯å·® (æƒé‡: 1Â° = 10mm)
                total_error += t_error + angle_error * 10.0

            return total_error

        # åˆå§‹å‚æ•°
        rvec_init, _ = cv2.Rodrigues(R_initial)
        tvec_init = t_initial.flatten()
        params_init = np.concatenate([rvec_init.flatten(), tvec_init])

        # è®¡ç®—åˆå§‹è¯¯å·®
        error_before = objective(params_init)

        # ä¼˜åŒ–: ä½¿ç”¨L-BFGS-Bæ–¹æ³• (æ¯”Powellæ›´ç¨³å®š)
        # è®¾ç½®ç´§çº¦æŸè¾¹ç•Œï¼Œåªå…è®¸å¾®è°ƒ (åˆå§‹å€¼å·²ç»å¾ˆå¥½äº†)
        bounds = []
        for i in range(3):
            # æ—‹è½¬å‘é‡: åˆå§‹å€¼ Â±5åº¦ (çº¦Â±0.087å¼§åº¦)
            bounds.append((rvec_init[i] - 0.087, rvec_init[i] + 0.087))
        for i in range(3):
            # å¹³ç§»å‘é‡: åˆå§‹å€¼ Â±10mm (Â±0.01m)
            bounds.append((tvec_init[i] - 0.01, tvec_init[i] + 0.01))

        result = minimize(
            objective,
            params_init,
            method='L-BFGS-B',
            bounds=bounds,
            options={
                'maxiter': 50,      # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                'ftol': 1e-6,       # å‡½æ•°å®¹å·®
                'gtol': 1e-5        # æ¢¯åº¦å®¹å·®
            }
        )

        # è§£æç»“æœ
        rvec_opt = result.x[:3]
        tvec_opt = result.x[3:6]
        R_opt, _ = cv2.Rodrigues(rvec_opt)
        t_opt = tvec_opt.reshape(3, 1)

        # è®¡ç®—æ”¹è¿›
        error_after = objective(result.x)

        # å¦‚æœä¼˜åŒ–ååè€Œæ›´å·®ï¼Œé€€å›åˆ°åˆå§‹å€¼
        if error_after > error_before:
            print(f"   âš ï¸  ä¼˜åŒ–æœªæ”¹å–„ç»“æœï¼Œä½¿ç”¨åˆå§‹å€¼")
            print(f"   ä¼˜åŒ–å‰è¯¯å·®: {error_before:.3f}")
            print(f"   ä¼˜åŒ–åè¯¯å·®: {error_after:.3f} (æ›´å·®)")
            return R_initial, t_initial

        print(f"   ä¼˜åŒ–å‰è¯¯å·®: {error_before:.3f}")
        print(f"   ä¼˜åŒ–åè¯¯å·®: {error_after:.3f}")
        print(f"   æ”¹è¿›: {(1 - error_after/error_before)*100:.1f}%")

        return R_opt, t_opt

    def evaluate_calibration(self, R_cam2gripper, t_cam2gripper, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, frame_ids, perform_analysis=True, original_poses=None):
        """è¯„ä¼°æ ‡å®šç»“æœå¹¶è¿›è¡Œæ·±åº¦è¯¯å·®åˆ†æ

        Args:
            perform_analysis: æ˜¯å¦æ‰§è¡Œè¯¯å·®æ¨¡å¼åˆ†æ
            original_poses: åŸå§‹æœºæ¢°è‡‚ä½å§¿åˆ—è¡¨ (å¯é€‰) [x, y, z, roll, pitch, yaw] in meters and radians

        Returns:
            tuple: (avg_error, detailed_errors_dict) å¹³å‡è¯¯å·®å’Œè¯¦ç»†è¯¯å·®å­—å…¸
        """
        errors = []
        angle_errors = []
        print("\n" + "="*60)
        print("ğŸ“Š æœ€ç»ˆæ ‡å®šè´¨é‡åˆ†æ")
        print("="*60)

        # initial_position: [x, y, z, roll, pitch, yaw] in mm and degrees
        ref_pos_mm = np.array(self.initial_position[:3])  # mm
        ref_rpy_deg = np.array(self.initial_position[3:])  # degrees

        for i in range(len(R_gripper2base)):
            R_pred = R_gripper2base[i] @ R_cam2gripper @ R_target2cam[i]
            t_pred = R_gripper2base[i] @ (R_cam2gripper @ t_target2cam[i] + t_cam2gripper) + t_gripper2base[i]

            # è®¡ç®—ä½å§¿åå·®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰åŸå§‹ä½å§¿ï¼‰
            pose_deviation_str = ""
            if original_poses is not None and i < len(original_poses):
                pose = original_poses[i]  # [x, y, z, roll, pitch, yaw] in meters and radians

                # è½¬æ¢ä¸ºmmå’Œåº¦
                pos_mm = np.array(pose[:3]) * 1000  # m â†’ mm
                rpy_deg = np.rad2deg(pose[3:])      # rad â†’ deg

                # è®¡ç®—åå·®
                pos_dev = pos_mm - ref_pos_mm
                rpy_dev = np.array([
                    self.angle_difference_deg(rpy_deg[0], ref_rpy_deg[0]),  # roll
                    self.angle_difference_deg(rpy_deg[1], ref_rpy_deg[1]),  # pitch
                    self.angle_difference_deg(rpy_deg[2], ref_rpy_deg[2])   # yaw
                ])

                pose_deviation_str = (
                    f" | ä½å§¿åå·®: "
                    f"Î”X={pos_dev[0]:+6.1f} Î”Y={pos_dev[1]:+6.1f} Î”Z={pos_dev[2]:+6.1f}mm, "
                    f"Î”R={rpy_dev[0]:+5.1f} Î”P={rpy_dev[1]:+5.1f} Î”Y={rpy_dev[2]:+5.1f}Â°"
                )

            if i == 0:
                R_ref = R_pred
                t_ref = t_pred
                print(f"âœ… å¸§ {frame_ids[i]:2d}: æ—‹è½¬è¯¯å·®  0.000Â°  å¹³ç§»è¯¯å·®   0.000mm{pose_deviation_str}")
                # ç¬¬ä¸€å¸§ä¹ŸåŠ å…¥ï¼Œä½†è¯¯å·®ä¸º0
                errors.append(0.0)
                angle_errors.append(0.0)
            else:
                R_error = R_ref.T @ R_pred
                angle_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))
                t_error = np.linalg.norm(t_pred - t_ref) * 1000
                errors.append(t_error)
                angle_errors.append(angle_error)

                status = "âœ…" if t_error < 3.0 else "âš ï¸" if t_error < 5.0 else "âŒ"
                print(f"{status} å¸§ {frame_ids[i]:2d}: æ—‹è½¬è¯¯å·® {angle_error:6.3f}Â°  å¹³ç§»è¯¯å·® {t_error:7.3f}mm{pose_deviation_str}")

        avg_error = np.mean(errors) if errors else 0
        avg_angle_error = np.mean(angle_errors) if angle_errors else 0

        print(f"\nğŸ“ˆ ç»¼åˆè¯¯å·®:")
        print(f"   å¹³å‡å¹³ç§»è¯¯å·®: {avg_error:.3f}mm")
        print(f"   å¹³å‡æ—‹è½¬è¯¯å·®: {avg_angle_error:.3f}Â°")

        # æ‰§è¡Œæ·±åº¦è¯¯å·®æ¨¡å¼åˆ†æ
        if perform_analysis and len(errors) > 5:
            print("\n" + "="*60)
            print("ğŸ” æ·±åº¦è¯¯å·®æ¨¡å¼åˆ†æ")
            print("="*60)

            analysis = self.analyze_error_patterns(errors[1:], frame_ids[1:])  # è·³è¿‡ç¬¬ä¸€å¸§ï¼ˆå‚è€ƒå¸§ï¼‰

            # æ˜¾ç¤ºåˆ†æç»“æœ
            if analysis['temporal_drift']:
                print("âš ï¸  æ£€æµ‹åˆ°çƒ­æ¼‚ç§»ç°è±¡")
            if analysis['periodic_pattern']:
                print("âš ï¸  æ£€æµ‹åˆ°å‘¨æœŸæ€§è¯¯å·®æ¨¡å¼")
            if analysis['spatial_clustering']:
                print("âš ï¸  æ£€æµ‹åˆ°ç©ºé—´èšç±»è¯¯å·®")

            if analysis['stable_frames']:
                print(f"\nâœ… ç¨³å®šå¸§ï¼ˆè¯¯å·®è¾ƒå°ï¼‰: {analysis['stable_frames']}")
            if analysis['outlier_frames']:
                print(f"âŒ å¼‚å¸¸å¸§ï¼ˆè¯¯å·®è¾ƒå¤§ï¼‰: {analysis['outlier_frames']}")

            if analysis['recommendations']:
                print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
                for rec in analysis['recommendations']:
                    print(f"   â€¢ {rec}")

            # ç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   å¹³ç§»è¯¯å·®æ ‡å‡†å·®: {np.std(errors):.3f}mm")
            print(f"   å¹³ç§»è¯¯å·®ä¸­ä½æ•°: {np.median(errors):.3f}mm")
            print(f"   å¹³ç§»è¯¯å·®èŒƒå›´: {np.min(errors):.3f}mm - {np.max(errors):.3f}mm")
            print(f"   æ—‹è½¬è¯¯å·®æ ‡å‡†å·®: {np.std(angle_errors):.3f}Â°")
            print(f"   æ—‹è½¬è¯¯å·®èŒƒå›´: {np.min(angle_errors):.3f}Â° - {np.max(angle_errors):.3f}Â°")

            # è´¨é‡åˆ†çº§å»ºè®®
            if avg_error > 5.0 and len(analysis['outlier_frames']) > len(errors) * 0.3:
                print("\nğŸ¯ ä¸»è¦é—®é¢˜: å­˜åœ¨å¤§é‡å¼‚å¸¸æ•°æ®ç‚¹")
                print("   å»ºè®®: ")
                print("   1. é‡æ–°é‡‡é›†è¿™äº›ä½ç½®çš„æ•°æ®: ", analysis['outlier_frames'])
                print("   2. æ£€æŸ¥è¯¥åŒºåŸŸçš„å…‰ç…§æ¡ä»¶å’Œæ£‹ç›˜æ ¼å¯è§æ€§")
                print("   3. è€ƒè™‘è°ƒæ•´æœºå™¨è‡‚è¿åŠ¨é€Ÿåº¦ï¼Œç­‰å¾…ç¨³å®šåå†é‡‡é›†")

        # è¿”å›è¯¦ç»†è¯¯å·®ä¿¡æ¯
        detailed_errors = {
            'translation_errors': errors,
            'rotation_errors': angle_errors,
            'avg_translation_error': avg_error,
            'avg_rotation_error': avg_angle_error
        }

        return avg_error, detailed_errors

    def save_optimized_result(self, R_cam2gripper, t_cam2gripper, avg_error, method_name, n_inliers, n_total, save_dir, detailed_errors=None):
        """ä¿å­˜ä¼˜åŒ–åçš„æ ‡å®šç»“æœ

        Args:
            detailed_errors: dict, å¯é€‰çš„è¯¦ç»†è¯¯å·®ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
                - rotation_errors: æ—‹è½¬è¯¯å·®åˆ—è¡¨ï¼ˆåº¦ï¼‰
                - translation_errors: å¹³ç§»è¯¯å·®åˆ—è¡¨ï¼ˆæ¯«ç±³ï¼‰
                - avg_rotation_error: å¹³å‡æ—‹è½¬è¯¯å·®ï¼ˆåº¦ï¼‰
                - avg_translation_error: å¹³å‡å¹³ç§»è¯¯å·®ï¼ˆæ¯«ç±³ï¼‰
        """
        rotation = R.from_matrix(R_cam2gripper)
        quaternion = rotation.as_quat()
        euler_angles = rotation.as_euler('xyz', degrees=True)
        t_mm = t_cam2gripper.flatten() * 1000.0

        print("\n" + "="*60)
        print("âœ… ä¼˜åŒ–ç‰ˆæ ‡å®šå®Œæˆï¼")
        print("="*60)
        print(f"\nğŸ“Š æœ€ç»ˆæ ‡å®šç»“æœ:")
        print(f"   å¹³ç§»å‘é‡ (mm): X={t_mm[0]:.2f}, Y={t_mm[1]:.2f}, Z={t_mm[2]:.2f}")
        print(f"   æ¬§æ‹‰è§’ (åº¦):    Rx={euler_angles[0]:.2f}Â°, Ry={euler_angles[1]:.2f}Â°, Rz={euler_angles[2]:.2f}Â°")
        print(f"   å››å…ƒæ•° (xyzw):  x={quaternion[0]:.4f}, y={quaternion[1]:.4f}, z={quaternion[2]:.4f}, w={quaternion[3]:.4f}")

        print(f"\nğŸ“ˆ æ ‡å®šè´¨é‡:")

        # æ˜¾ç¤ºè¯¦ç»†è¯¯å·®ä¿¡æ¯
        if detailed_errors:
            avg_rot_err = detailed_errors.get('avg_rotation_error', 0)
            avg_trans_err = detailed_errors.get('avg_translation_error', avg_error)
            rot_errors = detailed_errors.get('rotation_errors', [])
            trans_errors = detailed_errors.get('translation_errors', [])

            print(f"   å¹³å‡å¹³ç§»è¯¯å·®: {avg_trans_err:.3f}mm")
            print(f"   å¹³å‡æ—‹è½¬è¯¯å·®: {avg_rot_err:.3f}Â°")

            if trans_errors:
                print(f"   å¹³ç§»è¯¯å·®èŒƒå›´: {np.min(trans_errors):.3f}mm - {np.max(trans_errors):.3f}mm")
                print(f"   å¹³ç§»è¯¯å·®æ ‡å‡†å·®: {np.std(trans_errors):.3f}mm")

            if rot_errors:
                print(f"   æ—‹è½¬è¯¯å·®èŒƒå›´: {np.min(rot_errors):.3f}Â° - {np.max(rot_errors):.3f}Â°")
                print(f"   æ—‹è½¬è¯¯å·®æ ‡å‡†å·®: {np.std(rot_errors):.3f}Â°")
        else:
            print(f"   å¹³å‡å¹³ç§»è¯¯å·®: {avg_error:.3f}mm")

        print(f"   ä½¿ç”¨æ•°æ®: {n_inliers}/{n_total} (è´¨é‡é¢„ç­›é€‰+RANSAC)")
        print(f"   æœ€ä¼˜ç®—æ³•: {method_name}")

        # ç»¼åˆè´¨é‡è¯„ä¼°
        if detailed_errors:
            avg_rot = detailed_errors.get('avg_rotation_error', 0)
            avg_trans = detailed_errors.get('avg_translation_error', avg_error)
        else:
            avg_rot = 0
            avg_trans = avg_error

        # æ›´ä¸¥æ ¼çš„è´¨é‡æ ‡å‡†
        if avg_trans < 3.0 and avg_rot < 1.0:
            quality = "ğŸŒŸ ä¼˜ç§€ (å·¥ä¸šçº§ç²¾åº¦)"
            quality_tips = ""
        elif avg_trans < 5.0 and avg_rot < 2.0:
            quality = "ğŸ‘ è‰¯å¥½"
            quality_tips = "\n   ğŸ’¡ æç¤º: å¯é€šè¿‡å¢åŠ é«˜è´¨é‡æ•°æ®è¿›ä¸€æ­¥æå‡ç²¾åº¦"
        elif avg_trans < 8.0 and avg_rot < 3.0:
            quality = "âš ï¸  å¯æ¥å—"
            quality_tips = "\n   ğŸ’¡ å»ºè®®: é‡æ–°é‡‡é›†æ•°æ®ï¼Œæ³¨æ„æœºæ¢°è‡‚é¢„çƒ­å’Œä½å§¿ç¨³å®š"
        else:
            quality = "âŒ éœ€è¦æ”¹è¿›"
            quality_tips = "\n   â— è­¦å‘Š: ç²¾åº¦ä¸è¶³ï¼Œå¿…é¡»é‡æ–°æ ‡å®šï¼"
            quality_tips += "\n   ä¸»è¦é—®é¢˜åˆ†æ:"
            if avg_trans > 8.0:
                quality_tips += f"\n     â€¢ å¹³ç§»è¯¯å·®è¿‡å¤§({avg_trans:.1f}mm) - æ£€æŸ¥ç›¸æœºå†…å‚å’Œæœºæ¢°è‡‚ç²¾åº¦"
            if avg_rot > 3.0:
                quality_tips += f"\n     â€¢ æ—‹è½¬è¯¯å·®è¿‡å¤§({avg_rot:.1f}Â°) - å¢åŠ æ—‹è½¬è§’åº¦å˜åŒ–èŒƒå›´"
            if detailed_errors and detailed_errors.get('translation_errors'):
                trans_std = np.std(detailed_errors['translation_errors'])
                if trans_std > 3.0:
                    quality_tips += f"\n     â€¢ è¯¯å·®åˆ†å¸ƒä¸å‡(Ïƒ={trans_std:.1f}mm) - æ•°æ®è´¨é‡ä¸ä¸€è‡´"

        print(f"   è´¨é‡ç­‰çº§: {quality}{quality_tips}")

        # ä¿å­˜ç»“æœ
        result_data = {
            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'method': f'Optimized_{method_name}_with_RANSAC',
            'optimization': {
                'ransac_enabled': True,
                'multi_algorithm': True,
                'iterative_refinement': True
            },
            'data_points': {
                'total': n_total,
                'used': n_inliers,
                'filtered': n_total - n_inliers
            },
            'transformation': {
                'rotation_matrix': R_cam2gripper.tolist(),
                'translation_mm': t_mm.tolist(),
                'quaternion_xyzw': quaternion.tolist(),
                'euler_xyz_deg': euler_angles.tolist()
            },
            'quality': {
                'avg_error_mm': float(avg_error),
                'quality_level': quality
            },
            'camera_intrinsics': {
                'camera_matrix': self.camera_matrix.tolist() if self.camera_matrix is not None else None,
                'distortion_coefficients': self.dist_coeffs.tolist() if self.dist_coeffs is not None else None
            }
        }

        result_file = os.path.join(save_dir, "optimized_calibration_result.json")
        with open(result_file, 'w') as f:
            json.dump(result_data, f, indent=2)

        print(f"\nğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

    def save_intrinsics_to_file(self, save_path, camera_matrix, dist_coeffs, source="Unknown"):
        """ä¿å­˜ç›¸æœºå†…å‚åˆ°æ–‡ä»¶

        Args:
            save_path: ä¿å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
            camera_matrix: ç›¸æœºå†…å‚çŸ©é˜µ
            dist_coeffs: ç•¸å˜ç³»æ•°
            source: å†…å‚æ¥æºè¯´æ˜
        """
        try:
            intrinsics_data = {
                'camera_matrix': camera_matrix.tolist() if hasattr(camera_matrix, 'tolist') else camera_matrix,
                'distortion_coefficients': dist_coeffs.tolist() if hasattr(dist_coeffs, 'tolist') else dist_coeffs,
                'source': source,
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'image_size': [1280, 720],  # é»˜è®¤RealSenseåˆ†è¾¨ç‡
                'calibration_info': {
                    'board_size': list(self.board_size),
                    'square_size_mm': self.chessboard_size_mm
                }
            }

            with open(save_path, 'w') as f:
                yaml.dump(intrinsics_data, f, default_flow_style=False)

            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å†…å‚å¤±è´¥: {e}")
            return False

    def get_camera_intrinsics(self, data_dir=None, save_to_dir=None):
        """æ™ºèƒ½è·å–ç›¸æœºå†…å‚

        Args:
            data_dir: å¯é€‰ï¼Œæ•°æ®ç›®å½•è·¯å¾„ï¼Œç”¨äºæŸ¥æ‰¾å·²ä¿å­˜çš„å†…å‚æ–‡ä»¶
            save_to_dir: å¯é€‰ï¼Œä¿å­˜å†…å‚æ–‡ä»¶çš„ç›®å½•

        Returns:
            tuple: (camera_matrix, dist_coeffs, source) ç›¸æœºå†…å‚çŸ©é˜µã€ç•¸å˜ç³»æ•°ã€æ¥æºè¯´æ˜
        """
        intrinsics_loaded = False
        camera_matrix = None
        dist_coeffs = None
        source = ""


        data_dir = None
        
        # ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨data_dirä¸­çš„realsense_intrinsics.yamlï¼ˆå¦‚æœæä¾›ï¼‰
        if data_dir:
            intrinsics_file = os.path.join(data_dir, "realsense_intrinsics.yaml")
            if os.path.exists(intrinsics_file):
                try:
                    with open(intrinsics_file, 'r') as f:
                        intrinsics_data = yaml.load(f, Loader=yaml.FullLoader)
                        camera_matrix = np.array(intrinsics_data['camera_matrix'], dtype=np.float32)
                        dist_coeffs = np.array(intrinsics_data['distortion_coefficients'], dtype=np.float32)
                        source = f"æ–‡ä»¶: {intrinsics_file}"
                        print(f"âœ… ä»æ•°æ®ç›®å½•åŠ è½½ç›¸æœºå†…å‚")
                        intrinsics_loaded = True
                except Exception as e:
                    print(f"âš ï¸  è¯»å–å†…å‚æ–‡ä»¶å¤±è´¥: {e}")

        # ç­–ç•¥2ï¼šä½¿ç”¨é«˜è´¨é‡æ ‡å®šæ–‡ä»¶ hand_camera_intrinsics.yaml
        if not intrinsics_loaded:
            default_intrinsics_file = os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                "hand_camera_intrinsics.yaml"
            )
            if os.path.exists(default_intrinsics_file):
                try:
                    with open(default_intrinsics_file, 'r') as f:
                        intrinsics_data = yaml.load(f, Loader=yaml.FullLoader)
                        # è§£ææ ¼å¼: Kæ˜¯3x3çŸ©é˜µå±•å¹³æˆ9å…ƒç´ , Dæ˜¯ç•¸å˜ç³»æ•°
                        K = np.array(intrinsics_data['K'], dtype=np.float32)
                        camera_matrix = K.reshape(3, 3)
                        dist_coeffs = np.array(intrinsics_data['D'][:5], dtype=np.float32)
                        source = f"æ–‡ä»¶: hand_camera_intrinsics.yaml"
                        print(f"âœ… ä»é»˜è®¤æ ‡å®šæ–‡ä»¶åŠ è½½ç›¸æœºå†…å‚")
                        intrinsics_loaded = True
                except Exception as e:
                    print(f"âš ï¸  è¯»å–é»˜è®¤æ ‡å®šæ–‡ä»¶å¤±è´¥: {e}")

        # ç­–ç•¥3ï¼šå°è¯•è¿æ¥RealSenseç›¸æœºè·å–å®é™…å†…å‚
        if not intrinsics_loaded:
            try:
                import pyrealsense2 as rs
                print("ğŸ“· å°è¯•ä»RealSenseç›¸æœºè·å–å†…å‚...")

                # åˆ›å»ºä¸´æ—¶ç®¡é“è·å–å†…å‚
                pipeline = rs.pipeline()
                config = rs.config()

                # ä¼˜å…ˆå°è¯•è¿æ¥æ‰‹çœ¼ç›¸æœº
                hand_camera_id = "337122071190"
                try:
                    config.enable_device(hand_camera_id)
                    print("   æ‰¾åˆ°æ‰‹çœ¼ç›¸æœº")
                except:
                    print("   ä½¿ç”¨é»˜è®¤ç›¸æœº")

                config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 15)

                # å¯åŠ¨ç®¡é“è·å–å†…å‚
                profile = pipeline.start(config)
                color_stream = profile.get_stream(rs.stream.color)
                intrinsics = color_stream.as_video_stream_profile().get_intrinsics()

                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                camera_matrix = np.array([
                    [intrinsics.fx, 0, intrinsics.ppx],
                    [0, intrinsics.fy, intrinsics.ppy],
                    [0, 0, 1]
                ], dtype=np.float32)

                dist_coeffs = np.array(intrinsics.coeffs[:5], dtype=np.float32)

                pipeline.stop()

                source = "RealSenseç›¸æœºï¼ˆå‡ºå‚æ ‡å®šï¼‰"
                print(f"âœ… ä»RealSenseè·å–å†…å‚æˆåŠŸ")
                print(f"   ç„¦è·: fx={intrinsics.fx:.2f}, fy={intrinsics.fy:.2f}")
                print(f"   å…‰å¿ƒ: cx={intrinsics.ppx:.2f}, cy={intrinsics.ppy:.2f}")
                print(f"   ç•¸å˜: [{dist_coeffs[0]:.4f}, {dist_coeffs[1]:.4f}, {dist_coeffs[2]:.4f}, {dist_coeffs[3]:.4f}, {dist_coeffs[4]:.4f}]")

                # ä¿å­˜å†…å‚
                if save_to_dir or data_dir:
                    save_dir = save_to_dir or data_dir
                    try:
                        intrinsics_save_data = {
                            'camera_matrix': camera_matrix.tolist(),
                            'distortion_coefficients': dist_coeffs.tolist(),
                            'width': intrinsics.width,
                            'height': intrinsics.height,
                            'model': str(intrinsics.model),
                            'source': 'RealSense Factory Calibration',
                            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
                        }
                        save_path = os.path.join(save_dir, "realsense_intrinsics.yaml")
                        with open(save_path, 'w') as f:
                            yaml.dump(intrinsics_save_data, f)
                        print(f"   ğŸ’¾ å†…å‚å·²ä¿å­˜åˆ°: {save_path}")
                    except Exception as e:
                        print(f"   âš ï¸ ä¿å­˜å†…å‚å¤±è´¥: {e}")

                intrinsics_loaded = True

            except Exception as e:
                print(f"âš ï¸  æ— æ³•ä»ç›¸æœºè·å–å†…å‚: {e}")
                print(f"âš ï¸  æ‰€æœ‰å†…å‚åŠ è½½ç­–ç•¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                return None, None, "å¤±è´¥"

        return camera_matrix, dist_coeffs, source

    def load_camera_intrinsics(self):
        """ä»RealSenseç›¸æœºè·å–å‡ºå‚å†…å‚ï¼ˆå‘åå…¼å®¹ï¼‰"""
        print("ğŸ“· è·å–ç›¸æœºå†…å‚...")

        # ä½¿ç”¨æ–°çš„é€šç”¨å‡½æ•°ï¼Œä¿å­˜åˆ°å½“å‰ç›®å½•
        self.camera_matrix, self.dist_coeffs, source = self.get_camera_intrinsics(save_to_dir=".")

        if self.camera_matrix is not None:
            print(f"âœ… å†…å‚è·å–æˆåŠŸï¼ˆ{source}ï¼‰")
            return True
        else:
            print("âŒ å†…å‚è·å–å¤±è´¥")
            return False

    def warm_up_robot(self, duration_minutes=15):
        """æœºæ¢°è‡‚é¢„çƒ­ç¨‹åºï¼Œæ¶ˆé™¤æ¸©åº¦å½±å“

        Args:
            duration_minutes: é¢„çƒ­æ—¶é•¿(åˆ†é’Ÿ)ï¼Œé»˜è®¤15åˆ†é’Ÿ
        """
        print(f"\nğŸ”¥ æ‰§è¡Œæœºæ¢°è‡‚é¢„çƒ­ç¨‹åº ({duration_minutes}åˆ†é’Ÿ)...")
        print("   é¢„çƒ­å¯ä»¥æ¶ˆé™¤æ¸©åº¦å˜åŒ–å¯¹ç²¾åº¦çš„å½±å“")
        print("   è¾¾åˆ°çƒ­ç¨³å®šçŠ¶æ€éœ€è¦15-20åˆ†é’Ÿ")

        # æ›´å¤§èŒƒå›´çš„é¢„çƒ­ä½ç½®ï¼Œè¦†ç›–å·¥ä½œç©ºé—´
        warm_up_positions = [
            [400, 0, 350, 180, 60, 180],    # Xæ­£æ–¹å‘æé™
            [200, 0, 250, 180, 60, 180],    # Xè´Ÿæ–¹å‘æé™
            [300, 100, 300, 180, 60, 180],  # Yæ­£æ–¹å‘æé™
            [300, -100, 300, 180, 60, 180], # Yè´Ÿæ–¹å‘æé™
            [300, 0, 400, 180, 60, 180],    # Zæ­£æ–¹å‘æé™
            [300, 0, 200, 180, 60, 180],    # Zè´Ÿæ–¹å‘æé™
            [300, 0, 300, 180, 80, 180],    # Pitchå˜åŒ–
            [300, 0, 300, 180, 40, 180],    # Pitchå˜åŒ–
        ]

        # åŸºäºå®é™…æ—¶é—´çš„é¢„çƒ­å¾ªç¯ (ç²¾ç¡®æ§åˆ¶)
        target_seconds = duration_minutes * 60
        print(f"   é¢„çƒ­ç­–ç•¥: å¾ªç¯{len(warm_up_positions)}ä¸ªä½ç½®ï¼Œç›´åˆ°è¾¾åˆ°{duration_minutes}åˆ†é’Ÿ")
        print(f"   ç›®æ ‡æ—¶é•¿: {duration_minutes}åˆ†é’Ÿ ({target_seconds}ç§’)")

        start_time = time.time()
        cycle_count = 0

        # ä½¿ç”¨whileå¾ªç¯,æ ¹æ®å®é™…å·²ç”¨æ—¶é—´é€€å‡º
        while (time.time() - start_time) < target_seconds:
            cycle_count += 1
            elapsed = (time.time() - start_time) / 60
            remaining = duration_minutes - elapsed
            progress = min(100, (elapsed / duration_minutes) * 100)

            print(f"\né¢„çƒ­è¿›åº¦: {progress:.0f}% (å¾ªç¯{cycle_count}, {elapsed:.1f}/{duration_minutes}åˆ†é’Ÿ, å‰©ä½™{remaining:.1f}åˆ†é’Ÿ)")

            for j, pos in enumerate(warm_up_positions):
                # æ¯ä¸ªä½ç½®å‰æ£€æŸ¥æ—¶é—´,é¿å…è¶…æ—¶
                if (time.time() - start_time) >= target_seconds:
                    print(f"   â±ï¸  å·²è¾¾åˆ°ç›®æ ‡æ—¶é•¿,åœæ­¢é¢„çƒ­")
                    break

                self.robot.arm.set_position(
                    x=pos[0], y=pos[1], z=pos[2],
                    roll=pos[3], pitch=pos[4], yaw=pos[5],
                    wait=True, speed=self.motion_config['warmup_speed'],
                    use_gripper_center=False
                )
                time.sleep(2)

        total_time = (time.time() - start_time) / 60
        print(f"\nâœ… é¢„çƒ­å®Œæˆ! å®é™…è€—æ—¶: {total_time:.1f}åˆ†é’Ÿ")
        print("   æœºæ¢°è‡‚å·²è¾¾åˆ°çƒ­ç¨³å®šçŠ¶æ€")

    def connect_devices(self):
        """è¿æ¥æœºå™¨è‡‚å’Œç›¸æœº"""
        try:
            import pyrealsense2 as rs

            # åˆå§‹åŒ–æœºå™¨è‡‚
            print("ğŸ”§ åˆå§‹åŒ–æœºå™¨è‡‚...")
            cfg = create_config()
            self.robot = PiperRobot(cfg)
            self.robot.connect()

            # è¯¢é—®æ˜¯å¦éœ€è¦é¢„çƒ­
            warm_up_choice = input("\næ˜¯å¦æ‰§è¡Œæœºæ¢°è‡‚é¢„çƒ­15åˆ†é’Ÿ? (y/n) [æ¨èy]: ").strip().lower()
            if warm_up_choice == 'y' or warm_up_choice == '':
                self.warm_up_robot(duration_minutes=15)
            else:
                print("âš ï¸  è·³è¿‡é¢„çƒ­å¯èƒ½å¯¼è‡´ç²¾åº¦ä¸‹é™")
                print("   å»ºè®®æ‰‹åŠ¨è¿è¡Œæœºæ¢°è‡‚15åˆ†é’Ÿåå†å¼€å§‹æ ‡å®š")

            # ç§»åŠ¨åˆ°åˆå§‹ä½ç½®
            print(f"\nğŸ“ ç§»åŠ¨åˆ°åˆå§‹ä½ç½®: {self.initial_position[:3]}")
            self.robot.arm.set_position(
                x=self.initial_position[0], y=self.initial_position[1], z=self.initial_position[2],
                roll=self.initial_position[3], pitch=self.initial_position[4], yaw=self.initial_position[5],
                wait=True, speed=self.motion_config['normal_speed'], use_gripper_center=False
            )

            # å…³é—­çˆªå­
            print("ğŸ¤ å…³é—­çˆªå­...")
            self.robot.arm.set_gripper_position(0, wait=True, speed=1000)

            # åˆå§‹åŒ–ç›¸æœº
            print("ğŸ“¹ åˆå§‹åŒ–æ‰‹çœ¼ç›¸æœº...")
            self.pipeline = rs.pipeline()
            config = rs.config()

            hand_camera_id = "337122071190"
            config.enable_device(hand_camera_id)
            config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 15)

            profile = self.pipeline.start(config)
            self.pipeline_started = True

            # ä½¿ç”¨é€šç”¨å‡½æ•°è·å–å†…å‚ï¼ˆä¼šè‡ªåŠ¨ä»ç›¸æœºè·å–ï¼‰
            self.camera_matrix, self.dist_coeffs, source = self.get_camera_intrinsics()
            print(f"âœ… å†…å‚æ¥æº: {source}")

            # ç­‰å¾…ç›¸æœºç¨³å®š
            for _ in range(5):
                self.pipeline.wait_for_frames()

            print("âœ… è®¾å¤‡è¿æ¥æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ è®¾å¤‡è¿æ¥å¤±è´¥: {e}")
            return False

    def disconnect_devices(self):
        """æ–­å¼€è®¾å¤‡è¿æ¥"""
        if self.pipeline and self.pipeline_started:
            try:
                self.pipeline.stop()
                self.pipeline_started = False
                print("ğŸ“¹ ç›¸æœºå·²æ–­å¼€")
            except:
                pass

        if self.robot:
            try:
                self.robot.disconnect()
                print("ğŸ¤– æœºå™¨è‡‚å·²æ–­å¼€")
            except:
                pass

    def wait_for_pose_stability(self, target_pose, tolerance_mm=0.5, tolerance_deg=0.1, max_wait=None):
        """ç­‰å¾…æœºå™¨è‡‚ä½å§¿ç¨³å®š

        Args:
            target_pose: ç›®æ ‡ä½å§¿
            tolerance_mm: ä½ç½®å®¹å·®ï¼ˆmmï¼‰
            tolerance_deg: è§’åº¦å®¹å·®ï¼ˆåº¦ï¼‰
            max_wait: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®å€¼

        Returns:
            bool: æ˜¯å¦è¾¾åˆ°ç¨³å®šçŠ¶æ€
        """
        if max_wait is None:
            max_wait = self.motion_config['stability_wait']

        start_time = time.time()
        stable_count = 0
        required_stable_readings = 5

        print("   ç­‰å¾…ä½å§¿ç¨³å®š...", end="", flush=True)

        while time.time() - start_time < max_wait:
            # è·å–å½“å‰ä½å§¿
            _, current_pose = self.robot.arm.get_position(return_gripper_center=False)
            current_pose = current_pose if isinstance(current_pose, list) else current_pose.tolist()

            # è®¡ç®—åå·®
            pos_diff = np.linalg.norm([
                current_pose[0] - target_pose[0],
                current_pose[1] - target_pose[1],
                current_pose[2] - target_pose[2]
            ]) * 1000  # è½¬æ¢ä¸ºmm

            rot_diff = max([
                abs(current_pose[3] - target_pose[3]),
                abs(current_pose[4] - target_pose[4]),
                abs(current_pose[5] - target_pose[5])
            ])

            # æ£€æŸ¥æ˜¯å¦ç¨³å®š
            if pos_diff < tolerance_mm and rot_diff < tolerance_deg:
                stable_count += 1
                if stable_count >= required_stable_readings:
                    print(" âœ… ç¨³å®š")
                    # é¢å¤–é™æ­¢æ—¶é—´ï¼Œè®©å¾®å°æŒ¯åŠ¨å®Œå…¨æ¶ˆå¤±
                    if self.motion_config['extra_settle_time'] > 0:
                        print(f"   é¢å¤–é™æ­¢ {self.motion_config['extra_settle_time']:.1f}ç§’...", end="", flush=True)
                        time.sleep(self.motion_config['extra_settle_time'])
                        print(" âœ…")
                    return True
            else:
                stable_count = 0

            time.sleep(0.1)

        print(" âš ï¸ æœªè¾¾åˆ°ç¨³å®š")
        return False

    def get_historical_data_dirs(self):
        """è·å–å†å²æ ‡å®šæ•°æ®ç›®å½•ï¼ˆä»…åŸå§‹é‡‡é›†æ•°æ®ï¼šmanual å’Œ replayï¼‰

        Returns:
            list: æ’åºåçš„ç›®å½•è·¯å¾„åˆ—è¡¨
        """
        data_dirs = []
        valid_prefixes = ("manual_calibration_", "replay_calibration_")

        # æœç´¢ calibration_data ç›®å½•
        if os.path.exists(self.calibration_data_dir):
            for item in os.listdir(self.calibration_data_dir):
                if item.startswith(valid_prefixes):
                    full_path = os.path.join(self.calibration_data_dir, item)
                    if os.path.isdir(full_path):
                        data_dirs.append(full_path)

        # æœç´¢ verified_data ç›®å½•
        if os.path.exists(self.verified_data_dir):
            for item in os.listdir(self.verified_data_dir):
                if item.startswith(valid_prefixes):
                    full_path = os.path.join(self.verified_data_dir, item)
                    if os.path.isdir(full_path):
                        data_dirs.append(full_path)

        return sorted(data_dirs)

    def manual_calibration_mode(self):
        """æ‰‹åŠ¨æ ‡å®šæ¨¡å¼ï¼šæŒ‰ç©ºæ ¼ä¿å­˜å›¾åƒå’Œä½å§¿ï¼ŒæŒ‰ESCç»“æŸ"""
        print("\n" + "="*60)
        print("ğŸ¯ æ‰‹åŠ¨æ‰‹çœ¼æ ‡å®šæ¨¡å¼ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print("="*60)
        print("æ“ä½œè¯´æ˜:")
        print("  [ç©ºæ ¼] - ä¿å­˜å½“å‰å›¾åƒå’Œæœºæ¢°è‡‚ä½å§¿")
        print("  [ESC]  - ç»“æŸé‡‡é›†å¹¶å¼€å§‹æ ‡å®š")
        print("  [q]    - é€€å‡ºä¸æ ‡å®š")
        print("  [w]    - æŸ¥çœ‹é¢„çƒ­çŠ¶æ€")
        print("")
        print("âš ï¸  æ³¨æ„äº‹é¡¹:")
        print("  â€¢ å»ºè®®æœºå™¨è‡‚é¢„çƒ­5-10åˆ†é’Ÿä»¥å‡å°‘çƒ­æ¼‚ç§»")
        print("  â€¢ å»ºè®®Yawè§’åº¦åå·®ä¿æŒåœ¨ Â±30Â° ä»¥å†…ï¼ˆç›¸å¯¹äºåˆå§‹ä½ç½®ï¼‰")
        print("  â€¢ ç³»ç»Ÿå°†è‡ªåŠ¨ç­‰å¾…ä½å§¿ç¨³å®šåå†é‡‡é›†")
        print("="*60)
        print("\nâœ… æœºå™¨è‡‚å·²å®Œæˆ15åˆ†é’Ÿé¢„çƒ­ï¼Œå¯ä»¥å¼€å§‹é‡‡é›†")

        # åˆ›å»ºä¿å­˜ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(self.calibration_data_dir, f"manual_calibration_{timestamp}")
        os.makedirs(save_dir, exist_ok=True)
        print(f"\nğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {save_dir}/")

        # æ•°æ®è®°å½•
        collected_data = []
        frame_count = 0
        poses_txt_path = os.path.join(save_dir, "poses.txt")

        # å†™å…¥txtæ–‡ä»¶å¤´
        with open(poses_txt_path, 'w') as f:
            f.write("# æ‰‹çœ¼æ ‡å®šæ•°æ® - æœºæ¢°è‡‚æœ«ç«¯ä½å§¿\n")
            f.write("# æ ¼å¼: frame_id roll(rad) pitch(rad) yaw(rad) x(m) y(m) z(m)\n")
            f.write(f"# é‡‡é›†æ—¶é—´: {timestamp}\n")
            f.write("# " + "-"*70 + "\n")

        # ä¿å­˜ç›¸æœºå†…å‚åˆ°æ•°æ®ç›®å½•
        if self.camera_matrix is not None and self.dist_coeffs is not None:
            intrinsics_path = os.path.join(save_dir, "camera_intrinsics.yaml")
            # è·å–å†…å‚æ¥æºä¿¡æ¯
            _, _, source = self.get_camera_intrinsics(save_to_dir=save_dir)
            if self.save_intrinsics_to_file(intrinsics_path, self.camera_matrix, self.dist_coeffs, source):
                print(f"ğŸ’¾ ç›¸æœºå†…å‚å·²ä¿å­˜åˆ°: {intrinsics_path}")

        print("\nğŸ“¸ å¼€å§‹å®æ—¶é¢„è§ˆï¼Œç§»åŠ¨æœºæ¢°è‡‚åˆ°åˆé€‚ä½ç½®åæŒ‰ç©ºæ ¼ä¿å­˜...")

        # è·å–åˆå§‹yawå€¼ä½œä¸ºå‚è€ƒ
        _, initial_pose = self.robot.arm.get_position(return_gripper_center=False)
        initial_pose = initial_pose if isinstance(initial_pose, list) else initial_pose.tolist()
        initial_yaw = initial_pose[5]  # åˆå§‹yawè§’åº¦ï¼ˆåº¦ï¼‰
        print(f"ğŸ“ åˆå§‹Yawè§’åº¦: {initial_yaw:.1f}Â°")
        print(f"   å»ºè®®ä¿æŒYawåå·®åœ¨ Â±30Â° ä»¥å†…ï¼ˆç›¸å¯¹äºåˆå§‹å€¼ï¼‰")

        try:
            while True:
                # è·å–ç›¸æœºå›¾åƒ
                frames = self.pipeline.wait_for_frames()
                color_frame = frames.get_color_frame()

                if not color_frame:
                    continue

                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                color_image = np.asanyarray(color_frame.get_data())
                display_image = color_image.copy()

                # å°è¯•æ£€æµ‹æ£‹ç›˜æ ¼
                gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
                #ret, corners = cv2.findChessboardCorners(gray, self.board_size, None)
                ret, corners = cv2.findChessboardCorners(gray, self.board_size, flags=cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + 
          cv2.CALIB_CB_ACCURACY + cv2.CALIB_CB_FILTER_QUADS)

                if ret:
                    # ç²¾ç»†åŒ–è§’ç‚¹
                    corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), self.criteria)
                    cv2.drawChessboardCorners(display_image, self.board_size, corners_refined, ret)
                    cv2.putText(display_image, "Chessboard: DETECTED", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
                else:
                    cv2.putText(display_image, "Chessboard: NOT DETECTED", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)

                # æ˜¾ç¤ºé‡‡é›†æ•°é‡
                cv2.putText(display_image, f"Collected: {frame_count}", (10, 70),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)

                # æ˜¾ç¤ºé¢„çƒ­å®ŒæˆçŠ¶æ€
                cv2.putText(display_image, "Warmup: READY", (10, 110),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                cv2.putText(display_image, "[SPACE] Save  [ESC] Calibrate  [Q] Quit",
                           (10, display_image.shape[0]-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                # æ˜¾ç¤ºå›¾åƒ
                cv2.imshow("Manual Calibration", display_image)

                # æŒ‰é”®å¤„ç†
                key = cv2.waitKey(1) & 0xFF

                if key == ord(' '):  # ç©ºæ ¼ï¼šä¿å­˜
                    if not ret:
                        print("\nâš ï¸  æœªæ£€æµ‹åˆ°æ£‹ç›˜æ ¼ï¼Œå»ºè®®è°ƒæ•´ä½ç½®åå†ä¿å­˜")
                        user_confirm = input("æ˜¯å¦ä»è¦ä¿å­˜æ­¤å¸§? (y/n): ")
                        if user_confirm.lower() != 'y':
                            continue

                    # è·å–å½“å‰æœºæ¢°è‡‚ä½å§¿
                    _, current_pose = self.robot.arm.get_position(return_gripper_center=False)
                    current_pose = current_pose if isinstance(current_pose, list) else current_pose.tolist()

                    # ç­‰å¾…ä½å§¿ç¨³å®š
                    is_stable = self.wait_for_pose_stability(current_pose)
                    if not is_stable:
                        print("   âš ï¸ ä½å§¿æœªå®Œå…¨ç¨³å®šï¼Œå¯èƒ½å½±å“ç²¾åº¦")
                        user_confirm = input("   æ˜¯å¦ä»è¦ä¿å­˜? (y/n): ")
                        if user_confirm.lower() != 'y':
                            continue

                    # æ£€æŸ¥ yaw è§’åº¦ç›¸å¯¹äºåˆå§‹å€¼çš„åå·®
                    yaw_deg = current_pose[5]
                    yaw_deviation = yaw_deg - initial_yaw

                    # å¤„ç†è§’åº¦ç¯ç»•ï¼ˆ-180Â° åˆ° 180Â°ï¼‰
                    if yaw_deviation > 180:
                        yaw_deviation -= 360
                    elif yaw_deviation < -180:
                        yaw_deviation += 360

                    '''
                    if abs(yaw_deviation) > 30:
                        print(f"\nâš ï¸  è­¦å‘Šï¼šYawåå·® {yaw_deviation:.1f}Â° è¶…å‡ºæ¨èèŒƒå›´ [-30Â°, 30Â°]")
                        print(f"   å½“å‰Yaw: {yaw_deg:.1f}Â°, åˆå§‹Yaw: {initial_yaw:.1f}Â°")
                        print(f"   å¤§å¹…åº¦çš„Yawæ—‹è½¬å¯èƒ½å½±å“æ ‡å®šç²¾åº¦")
                        user_confirm = input("æ˜¯å¦ä»è¦ä¿å­˜æ­¤æ•°æ®? (y/n): ")
                        if user_confirm.lower() != 'y':
                            print("   å·²è·³è¿‡æ­¤å¸§")
                            continue
                    '''
                    
                    # å•ä½è½¬æ¢ï¼šæ¯«ç±³â†’ç±³ï¼Œåº¦â†’å¼§åº¦
                    x_m = current_pose[0] / 1000.0
                    y_m = current_pose[1] / 1000.0
                    z_m = current_pose[2] / 1000.0
                    roll_rad = np.deg2rad(current_pose[3])
                    pitch_rad = np.deg2rad(current_pose[4])
                    yaw_rad = np.deg2rad(current_pose[5])

                    frame_count += 1

                    # ä¿å­˜å›¾åƒ
                    image_filename = f"{frame_count}_Color.png"
                    image_path = os.path.join(save_dir, image_filename)
                    cv2.imwrite(image_path, color_image)

                    # ä¿å­˜ä½å§¿åˆ°txt
                    with open(poses_txt_path, 'a') as f:
                        f.write(f"{frame_count} {roll_rad:.6f} {pitch_rad:.6f} {yaw_rad:.6f} "
                               f"{x_m:.6f} {y_m:.6f} {z_m:.6f}\n")

                    # è®°å½•æ•°æ®
                    collected_data.append({
                        'frame_id': frame_count,
                        'image_path': image_path,
                        'pose': [x_m, y_m, z_m, roll_rad, pitch_rad, yaw_rad],
                        'pose_original': current_pose,
                        'has_chessboard': ret,
                        'corners': corners_refined if ret else None
                    })

                    print(f"âœ… å·²ä¿å­˜ç¬¬ {frame_count} å¸§:")
                    print(f"   ä½ç½®: X={current_pose[0]:.1f}mm Y={current_pose[1]:.1f}mm Z={current_pose[2]:.1f}mm")

                    # æ ¹æ® yaw åå·®æ˜¾ç¤ºä¸åŒçš„çŠ¶æ€
                    yaw_status = ""
                    if abs(yaw_deviation) > 30:
                        yaw_status = f" âš ï¸ (åå·®{yaw_deviation:+.1f}Â°, è¶…èŒƒå›´)"
                    elif abs(yaw_deviation) > 20:
                        yaw_status = f" âš¡ (åå·®{yaw_deviation:+.1f}Â°, æ¥è¿‘è¾¹ç•Œ)"
                    else:
                        yaw_status = f" (åå·®{yaw_deviation:+.1f}Â°)"

                    print(f"   å§¿æ€: R={current_pose[3]:.1f}Â° P={current_pose[4]:.1f}Â° Y={current_pose[5]:.1f}Â°{yaw_status}")
                    print(f"   æ£‹ç›˜æ ¼: {'âœ“ æ£€æµ‹åˆ°' if ret else 'âœ— æœªæ£€æµ‹åˆ°'}")

                elif key == 27:  # ESCï¼šç»“æŸé‡‡é›†
                    if frame_count == 0:
                        print("\nâš ï¸  æœªé‡‡é›†ä»»ä½•æ•°æ®")
                        break
                    print(f"\nğŸ“Š é‡‡é›†å®Œæˆï¼Œå…± {frame_count} å¸§")
                    print("ğŸ”§ å¼€å§‹æ‰‹çœ¼æ ‡å®š...")
                    calibration_success = self.perform_manual_calibration(collected_data, save_dir)

                    # æ ‡å®šæˆåŠŸåï¼Œè¯¢é—®æ˜¯å¦é‡æ’­
                    if calibration_success:
                        self.save_dir_for_replay = save_dir  # ä¿å­˜ç›®å½•ä¾›åç»­é‡æ’­ä½¿ç”¨
                        print("\n" + "="*60)
                        print("ğŸ“ æ˜¯å¦è¿›è¡Œè½¨è¿¹é‡æ’­éªŒè¯?")
                        print("   é‡æ’­å¯ä»¥éªŒè¯æ ‡å®šç»“æœçš„é‡å¤æ€§")
                        replay_choice = input("æ˜¯å¦é‡æ’­è½¨è¿¹? (y/n): ")
                        if replay_choice.lower() == 'y':
                            cv2.destroyAllWindows()  # å…ˆå…³é—­å½“å‰çª—å£
                            print("\nğŸ”„ å‡†å¤‡é‡æ’­è½¨è¿¹...")
                            

                            mode_choice = '1'
                            '''
                            # è¯¢é—®é‡æ’­æ¨¡å¼
                            print("\né€‰æ‹©é‡æ’­æ¨¡å¼:")
                            print("  1. é«˜ç²¾åº¦æ¨¡å¼ - æ¯æ¬¡ï¼šåˆå§‹ä½ç½®â†’é›¶ç‚¹â†’ç›®æ ‡ï¼ˆæ¨èï¼Œæ¶ˆé™¤ç´¯ç§¯è¯¯å·®ï¼‰")
                            print("  2. å¿«é€Ÿæ¨¡å¼ - ç›´æ¥ç§»åŠ¨åˆ°ç›®æ ‡ï¼ˆæ›´å¿«ä½†å¯èƒ½æœ‰ç´¯ç§¯è¯¯å·®ï¼‰")
                            mode_choice = input("é€‰æ‹©æ¨¡å¼ (1/2) [é»˜è®¤1]: ").strip()
                            '''
                            
                            return_to_zero = True  # é»˜è®¤é«˜ç²¾åº¦æ¨¡å¼
                            if mode_choice == "2":
                                return_to_zero = False
                                print("âš¡ å·²é€‰æ‹©å¿«é€Ÿæ¨¡å¼")
                            else:
                                print("âœ… å·²é€‰æ‹©é«˜ç²¾åº¦æ¨¡å¼")

                            time.sleep(1)
                            self.replay_trajectory(save_dir, return_to_zero=return_to_zero)
                    break

                elif key == ord('q') or key == ord('Q'):  # Qï¼šé€€å‡º
                    print("\nâŒ ç”¨æˆ·å–æ¶ˆ")
                    break

        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")

        finally:
            cv2.destroyAllWindows()

            # ä¿å­˜Excelæ–‡ä»¶
            if frame_count > 0:
                try:
                    import pandas as pd

                    # è¯»å–poses.txtæ•°æ®
                    data_rows = []
                    with open(poses_txt_path, 'r') as f:
                        for line in f:
                            if not line.startswith('#'):
                                data_rows.append(line.strip().split())

                    # è½¬æ¢ä¸ºDataFrame
                    df = pd.DataFrame(data_rows, columns=['frame_id', 'roll_rad', 'pitch_rad',
                                                          'yaw_rad', 'x_m', 'y_m', 'z_m'])
                    df = df.astype(float)
                    df['frame_id'] = df['frame_id'].astype(int)

                    # ä¿å­˜ä¸ºExcel
                    excel_path = os.path.join(save_dir, "poses.xlsx")
                    df.to_excel(excel_path, index=False, sheet_name='Poses')

                    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜")
                except ImportError:
                    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜ (æ— æ³•ç”ŸæˆExcelï¼Œéœ€è¦å®‰è£…pandas)")

    def analyze_error_patterns(self, errors, frame_ids):
        """åˆ†æè¯¯å·®æ¨¡å¼ä»¥è¯†åˆ«ç³»ç»Ÿæ€§é—®é¢˜

        Args:
            errors: è¯¯å·®åˆ—è¡¨
            frame_ids: å¸§IDåˆ—è¡¨

        Returns:
            dict: è¯¯å·®åˆ†æç»“æœ
        """
        analysis = {
            'temporal_drift': False,
            'spatial_clustering': False,
            'periodic_pattern': False,
            'outlier_frames': [],
            'stable_frames': [],
            'recommendations': []
        }

        # æ—¶é—´åºåˆ—åˆ†æ - æ£€æµ‹çƒ­æ¼‚ç§»
        if len(errors) > 10:
            # å°†æ•°æ®åˆ†æˆå‰åŠå’ŒååŠ
            mid_point = len(errors) // 2
            first_half_avg = np.mean(errors[:mid_point])
            second_half_avg = np.mean(errors[mid_point:])

            # å¦‚æœååŠéƒ¨åˆ†è¯¯å·®æ˜æ˜¾å¢å¤§ï¼Œå¯èƒ½å­˜åœ¨çƒ­æ¼‚ç§»
            if second_half_avg > first_half_avg * 1.3:
                analysis['temporal_drift'] = True
                analysis['recommendations'].append("æ£€æµ‹åˆ°æ—¶é—´ç›¸å…³è¯¯å·®å¢é•¿ï¼Œå»ºè®®ï¼šæœºå™¨è‡‚é¢„çƒ­15-20åˆ†é’Ÿ")

            # æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼
            try:
                from scipy import signal
                # ä½¿ç”¨è‡ªç›¸å…³æ£€æµ‹å‘¨æœŸæ€§
                autocorr = signal.correlate(errors - np.mean(errors), errors - np.mean(errors), mode='full')
                autocorr = autocorr[len(autocorr)//2:]
                autocorr = autocorr / autocorr[0]  # å½’ä¸€åŒ–

                # å¯»æ‰¾å±€éƒ¨æœ€å¤§å€¼
                peaks, _ = signal.find_peaks(autocorr, height=0.3, distance=3)
                if len(peaks) > 0:
                    analysis['periodic_pattern'] = True
                    period = peaks[0]
                    analysis['recommendations'].append(f"æ£€æµ‹åˆ°å‘¨æœŸæ€§è¯¯å·®ï¼ˆå‘¨æœŸçº¦{period}å¸§ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æœºæ¢°æŒ¯åŠ¨æˆ–é½¿è½®é—´éš™")
            except:
                pass

        # è¯†åˆ«ç¨³å®šå¸§å’Œå¼‚å¸¸å¸§
        mean_error = np.mean(errors)
        std_error = np.std(errors)

        for i, (err, fid) in enumerate(zip(errors, frame_ids)):
            if err < mean_error - std_error:
                analysis['stable_frames'].append(fid)
            elif err > mean_error + 1.5 * std_error:
                analysis['outlier_frames'].append(fid)

        # ç©ºé—´èšç±»åˆ†æ
        if len(analysis['outlier_frames']) > 3:
            # æ£€æŸ¥å¼‚å¸¸å¸§æ˜¯å¦è¿ç»­
            outlier_diffs = np.diff(analysis['outlier_frames'])
            if np.all(outlier_diffs <= 2):
                analysis['spatial_clustering'] = True
                analysis['recommendations'].append(f"å¼‚å¸¸å¸§é›†ä¸­åœ¨{analysis['outlier_frames'][0]}-{analysis['outlier_frames'][-1]}ï¼Œå¯èƒ½è¯¥åŒºåŸŸå­˜åœ¨é®æŒ¡æˆ–å…‰ç…§é—®é¢˜")

        return analysis

    def detect_pose_stability(self, pose_sequence, timestamps=None):
        """æ£€æµ‹ä½å§¿åºåˆ—çš„ç¨³å®šæ€§

        Args:
            pose_sequence: ä½å§¿åºåˆ—
            timestamps: å¯é€‰çš„æ—¶é—´æˆ³

        Returns:
            dict: ç¨³å®šæ€§åˆ†æç»“æœ
        """
        stability = {
            'position_jitter_mm': 0,
            'rotation_jitter_deg': 0,
            'settling_time_s': 0,
            'is_stable': True
        }

        if len(pose_sequence) < 2:
            return stability

        # è®¡ç®—ç›¸é‚»ä½å§¿ä¹‹é—´çš„å˜åŒ–
        position_changes = []
        rotation_changes = []

        for i in range(1, len(pose_sequence)):
            # ä½ç½®å˜åŒ–
            pos_diff = np.linalg.norm(pose_sequence[i][:3] - pose_sequence[i-1][:3]) * 1000  # è½¬æ¢ä¸ºmm
            position_changes.append(pos_diff)

            # æ—‹è½¬å˜åŒ–
            rot_diff = np.abs(pose_sequence[i][3:] - pose_sequence[i-1][3:])
            rot_diff_deg = np.degrees(np.max(rot_diff))
            rotation_changes.append(rot_diff_deg)

        # è®¡ç®—æŠ–åŠ¨æŒ‡æ ‡
        if position_changes:
            stability['position_jitter_mm'] = np.std(position_changes)
            stability['rotation_jitter_deg'] = np.std(rotation_changes)

            # åˆ¤æ–­æ˜¯å¦ç¨³å®š
            if stability['position_jitter_mm'] > 0.5 or stability['rotation_jitter_deg'] > 0.1:
                stability['is_stable'] = False

        return stability

    def _prepare_calibration_data(self, collected_data, compute_reprojection_errors=False):
        """æå–å…¬å…±çš„æ ‡å®šæ•°æ®å‡†å¤‡é€»è¾‘

        Args:
            collected_data: å·²åŒ…å« corners çš„æ•°æ®ï¼ˆæ¥è‡ª apply_quality_filters æˆ– æ£‹ç›˜æ ¼æ£€æµ‹ï¼‰
            compute_reprojection_errors: æ˜¯å¦è®¡ç®—é‡æŠ•å½±è¯¯å·®ï¼ˆä¼˜åŒ–æ¨¡å¼éœ€è¦ï¼‰

        Returns:
            tuple: (R_gripper2base_list, t_gripper2base_list, R_target2cam_list, t_target2cam_list,
                    frame_ids, reprojection_errors)
                    å¦‚æœ compute_reprojection_errors=Falseï¼Œreprojection_errors ä¸º None
        """
        # å‡†å¤‡æ£‹ç›˜æ ¼ä¸–ç•Œåæ ‡
        square_size = self.chessboard_size_mm / 1000.0
        objp = np.zeros((self.board_size[0] * self.board_size[1], 3), np.float32)
        objp[:, :2] = np.mgrid[0:self.board_size[0], 0:self.board_size[1]].T.reshape(-1, 2)
        objp *= square_size

        # åˆå§‹åŒ–æ•°æ®åˆ—è¡¨
        R_gripper2base_list = []
        t_gripper2base_list = []
        R_target2cam_list = []
        t_target2cam_list = []
        frame_ids = []
        reprojection_errors = [] if compute_reprojection_errors else None

        for data in collected_data:
            # æœºå™¨äººä½å§¿ï¼ˆæœ«ç«¯åˆ°åŸºåº§ï¼‰
            x, y, z, roll, pitch, yaw = data['pose']
            t_gripper2base_list.append(np.array([[x], [y], [z]]))
            R_robot = R.from_euler('xyz', [roll, pitch, yaw]).as_matrix()
            R_gripper2base_list.append(R_robot)
            frame_ids.append(data['frame_id'])

            # æ±‚è§£æ£‹ç›˜æ ¼ç›¸å¯¹äºç›¸æœºçš„ä½å§¿
            ret, rvec, tvec = cv2.solvePnP(objp, data['corners'], self.camera_matrix, self.dist_coeffs)
            if ret:
                R_target2cam_mat, _ = cv2.Rodrigues(rvec)
                R_target2cam_list.append(R_target2cam_mat)
                t_target2cam_list.append(tvec)

                # å¯é€‰ï¼šè®¡ç®—é‡æŠ•å½±è¯¯å·®
                if compute_reprojection_errors:
                    projected_points, _ = cv2.projectPoints(objp, rvec, tvec,
                                                           self.camera_matrix, self.dist_coeffs)
                    projected_points = projected_points.reshape(-1, 2)
                    detected_points = data['corners'].reshape(-1, 2)
                    error = np.sqrt(np.mean(np.sum((projected_points - detected_points)**2, axis=1)))
                    reprojection_errors.append(error)
            else:
                print(f"âš ï¸  å¸§ {data['frame_id']}: solvePnP å¤±è´¥")

        return R_gripper2base_list, t_gripper2base_list, R_target2cam_list, t_target2cam_list, frame_ids, reprojection_errors

    def perform_optimized_calibration(self, collected_data, save_dir):
        """ä¼˜åŒ–ç‰ˆæ‰‹çœ¼æ ‡å®š - åŒ…å«è´¨é‡é¢„ç­›é€‰ã€RANSACè¿‡æ»¤ã€å¤šç®—æ³•èåˆã€è¿­ä»£ä¼˜åŒ–å’Œé«˜çº§è¯Šæ–­

        Args:
            collected_data: é‡‡é›†çš„æ ‡å®šæ•°æ®
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            bool: æ ‡å®šæ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("ğŸ”§ æ‰§è¡Œä¼˜åŒ–ç‰ˆæ‰‹çœ¼æ ‡å®šï¼ˆå«è´¨é‡é¢„ç­›é€‰+é«˜çº§è¯Šæ–­ï¼‰")
        print("="*60)

        if len(collected_data) < 3:
            print(f"âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ç»„æ•°æ®ï¼Œå½“å‰åªæœ‰ {len(collected_data)} ç»„")
            return False

        # ========================================
        # æ­¥éª¤0: æ•°æ®è´¨é‡é¢„ç­›é€‰ï¼ˆåœ¨RANSACä¹‹å‰ï¼‰
        # ========================================
        collected_data, filter_report = self.apply_quality_filters(collected_data, verbose=True)

        if len(collected_data) < 10:
            print(f"\nâŒ è´¨é‡è¿‡æ»¤åæ•°æ®ä¸è¶³ï¼ˆä»… {len(collected_data)} å¸§ï¼‰ï¼Œè‡³å°‘éœ€è¦10å¸§")
            print("   å»ºè®®:")
            print("   1. é‡æ–°é‡‡é›†æ•°æ®ï¼Œç¡®ä¿è¿åŠ¨å……åˆ†")
            print("   2. é¿å…åœ¨å·¥ä½œç©ºé—´è¾¹ç•Œé‡‡é›†")
            print("   3. å¢åŠ æ•°æ®é‡‡é›†æ€»é‡")
            return False

        # ä½¿ç”¨å…¬å…±æ–¹æ³•å‡†å¤‡æ ‡å®šæ•°æ®ï¼ˆåŒ…å«é‡æŠ•å½±è¯¯å·®è®¡ç®—ï¼‰
        R_gripper2base_all, t_gripper2base_all, R_target2cam_all, t_target2cam_all, frame_ids, reprojection_errors = \
            self._prepare_calibration_data(collected_data, compute_reprojection_errors=True)

        print(f"ğŸ“Š è´¨é‡é¢„ç­›é€‰åå‡†å¤‡äº† {len(R_gripper2base_all)} ç»„æ•°æ®")

        # æ˜¾ç¤ºè§’ç‚¹æ£€æµ‹è´¨é‡ç»Ÿè®¡
        if reprojection_errors:
            avg_reproj_error = np.mean(reprojection_errors)
            max_reproj_error = np.max(reprojection_errors)
            min_reproj_error = np.min(reprojection_errors)

            print(f"\nğŸ¯ è§’ç‚¹æ£€æµ‹è´¨é‡:")
            print(f"   å¹³å‡é‡æŠ•å½±è¯¯å·®: {avg_reproj_error:.3f} åƒç´ ")
            print(f"   è¯¯å·®èŒƒå›´: {min_reproj_error:.3f} - {max_reproj_error:.3f} åƒç´ ")

            if avg_reproj_error < 0.5:
                print(f"   è´¨é‡è¯„çº§: ğŸŒŸ ä¼˜ç§€ (< 0.5px)")
            elif avg_reproj_error < 1.0:
                print(f"   è´¨é‡è¯„çº§: ğŸ‘ è‰¯å¥½ (< 1.0px)")
            elif avg_reproj_error < 2.0:
                print(f"   è´¨é‡è¯„çº§: âš ï¸  å¯æ¥å— (< 2.0px)")
            else:
                print(f"   è´¨é‡è¯„çº§: âŒ è¾ƒå·® (>= 2.0px)")
                print(f"   å»ºè®®: é‡æ–°æ ‡å®šç›¸æœºå†…å‚æˆ–è°ƒæ•´æ£‹ç›˜æ ¼æ£€æµ‹å‚æ•°")

            # æ ‡è®°å¹¶è¿‡æ»¤é‡æŠ•å½±è¯¯å·®è¿‡å¤§çš„å¸§
            bad_reproj_frames = [frame_ids[i] for i, err in enumerate(reprojection_errors) if err > 2.0]
            if bad_reproj_frames:
                print(f"   âš ï¸  é‡æŠ•å½±è¯¯å·®>2.0pxçš„å¸§: {bad_reproj_frames[:10]}")
                if len(bad_reproj_frames) > 10:
                    print(f"      ... è¿˜æœ‰ {len(bad_reproj_frames)-10} å¸§")

        # æ­¥éª¤0: è¿‡æ»¤é«˜é‡æŠ•å½±è¯¯å·®å¸§ï¼ˆå¼‚å¸¸æ•°æ®æ¸…ç†ï¼‰
        if reprojection_errors:
            print("\n" + "-"*40)
            print("æ­¥éª¤0: è¿‡æ»¤å¼‚å¸¸å¸§ï¼ˆé‡æŠ•å½±è¯¯å·® > 2.0pxï¼‰")

            # åˆ›å»ºä¿ç•™ç´¢å¼•åˆ—è¡¨
            keep_indices = [i for i, err in enumerate(reprojection_errors) if err <= 2.0]
            removed_indices = [i for i, err in enumerate(reprojection_errors) if err > 2.0]

            if removed_indices:
                print(f"   ç§»é™¤ {len(removed_indices)} å¸§:")
                for idx in removed_indices:
                    print(f"   âŒ å¸§ {frame_ids[idx]}: é‡æŠ•å½±è¯¯å·® {reprojection_errors[idx]:.3f}px (ç§»é™¤)")

                # è¿‡æ»¤æ•°æ®æ•°ç»„
                R_gripper2base_all = [R_gripper2base_all[i] for i in keep_indices]
                t_gripper2base_all = [t_gripper2base_all[i] for i in keep_indices]
                R_target2cam_all = [R_target2cam_all[i] for i in keep_indices]
                t_target2cam_all = [t_target2cam_all[i] for i in keep_indices]
                frame_ids = [frame_ids[i] for i in keep_indices]
                reprojection_errors = [reprojection_errors[i] for i in keep_indices]

                print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
                print(f"   ä¿ç•™å¸§æ•°: {len(keep_indices)}")
                print(f"   ç§»é™¤å¸§æ•°: {len(removed_indices)}")
            else:
                print(f"   âœ… æ‰€æœ‰å¸§çš„é‡æŠ•å½±è¯¯å·®éƒ½ â‰¤ 2.0pxï¼Œæ— éœ€è¿‡æ»¤")

            if len(R_gripper2base_all) < 10:
                print("âŒ è¿‡æ»¤åæœ‰æ•ˆæ•°æ®ä¸è¶³10å¸§ï¼Œæ— æ³•è¿›è¡Œæ ‡å®š")
                return False

        # æ­¥éª¤1: RANSACå‡ ä½•ä¸€è‡´æ€§è¿‡æ»¤ï¼ˆç¬¬äºŒå±‚è¿‡æ»¤ï¼‰
        print("\n" + "-"*40)
        print("æ­¥éª¤1: RANSACå‡ ä½•ä¸€è‡´æ€§è¿‡æ»¤")

        # æ”¹è¿›çš„é˜ˆå€¼ç­–ç•¥ï¼šåŸºäºæ•°æ®è´¨é‡åŠ¨æ€è°ƒæ•´
        initial_threshold = 10.0  # åˆç†çš„åˆå§‹é˜ˆå€¼
        min_inliers = max(10, int(len(R_gripper2base_all) * 0.7))  # è‡³å°‘ä¿ç•™70%çš„æ•°æ®ï¼ˆæ›´ä¿å®ˆï¼‰

        best_inliers = self.ransac_filter_handeye(
            R_gripper2base_all, t_gripper2base_all,
            R_target2cam_all, t_target2cam_all,
            frame_ids,
            threshold=initial_threshold
        )

        # å¦‚æœå†…ç‚¹å¤ªå°‘ï¼Œé€æ­¥æ”¾å®½é˜ˆå€¼ï¼ˆä½†ä¸è¦å¤ªæ¿€è¿›ï¼‰
        attempts = 0
        while len(best_inliers) < min_inliers and initial_threshold < 20.0 and attempts < 3:
            initial_threshold += 3.0
            attempts += 1
            print(f"   è°ƒæ•´RANSACé˜ˆå€¼åˆ° {initial_threshold}mm (å°è¯• {attempts}/3)")
            best_inliers = self.ransac_filter_handeye(
                R_gripper2base_all, t_gripper2base_all,
                R_target2cam_all, t_target2cam_all,
                frame_ids,
                threshold=initial_threshold
            )

        if len(best_inliers) < 3:
            print("âŒ RANSACåæœ‰æ•ˆæ•°æ®ä¸è¶³")
            return self.perform_manual_calibration(collected_data, save_dir)

        # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®
        R_gripper2base = [R_gripper2base_all[i] for i in best_inliers]
        t_gripper2base = [t_gripper2base_all[i] for i in best_inliers]
        R_target2cam = [R_target2cam_all[i] for i in best_inliers]
        t_target2cam = [t_target2cam_all[i] for i in best_inliers]

        print(f"âœ… RANSACè¿‡æ»¤: {len(best_inliers)}/{len(R_gripper2base_all)} ä¸ªå†…ç‚¹")
        print(f"   è¢«è¿‡æ»¤çš„å¸§: {[frame_ids[i] for i in range(len(frame_ids)) if i not in best_inliers]}")

        # æ­¥éª¤2: å¤šç®—æ³•èåˆ
        print("\n" + "-"*40)
        print("æ­¥éª¤2: å¤šç®—æ³•èåˆæ ‡å®š")

        best_R, best_t, best_method = self.multi_algorithm_fusion(
            R_gripper2base, t_gripper2base,
            R_target2cam, t_target2cam
        )

        if best_R is None:
            print("âŒ å¤šç®—æ³•èåˆå¤±è´¥")
            return self.perform_manual_calibration(collected_data, save_dir)

        print(f"âœ… æœ€ä¼˜ç®—æ³•: {best_method}")

        # æ­¥éª¤3: è¿­ä»£ä¼˜åŒ–
        print("\n" + "-"*40)
        print("æ­¥éª¤3: è¿­ä»£ä¼˜åŒ–")

        R_optimized, t_optimized = self.iterative_optimization(
            best_R, best_t,
            R_gripper2base, t_gripper2base,
            R_target2cam, t_target2cam
        )

        # æå–åŸå§‹ä½å§¿ç”¨äºåå·®æ˜¾ç¤º
        original_poses = [data['pose'] for data in collected_data]

        # è¯„ä¼°æœ€ç»ˆç»“æœï¼ˆè¿”å›è¯¦ç»†è¯¯å·®ï¼ŒåŒ…å«ä½å§¿åå·®ä¿¡æ¯ï¼‰
        final_avg_error, detailed_errors = self.evaluate_calibration(
            R_optimized, t_optimized,
            R_gripper2base_all, t_gripper2base_all,
            R_target2cam_all, t_target2cam_all,
            frame_ids,
            original_poses=original_poses
        )

        # ä¿å­˜ä¼˜åŒ–ç»“æœï¼ˆä¼ å…¥è¯¦ç»†è¯¯å·®ï¼‰
        self.save_optimized_result(
            R_optimized, t_optimized,
            final_avg_error,
            best_method,
            len(best_inliers),
            len(R_gripper2base_all),
            save_dir,
            detailed_errors=detailed_errors
        )

        return True

    def perform_manual_calibration(self, collected_data, save_dir):
        """ä½¿ç”¨æ‰‹åŠ¨é‡‡é›†çš„æ•°æ®è¿›è¡Œæ‰‹çœ¼æ ‡å®š"""
        print("\n" + "="*60)
        print("ğŸ”§ å¼€å§‹æ‰‹çœ¼æ ‡å®šè®¡ç®—")
        print("="*60)

        if len(collected_data) < 3:
            print(f"âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ç»„æ•°æ®ï¼Œå½“å‰åªæœ‰ {len(collected_data)} ç»„")
            return False

        # æ£€æµ‹å¹¶è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_data = []
        for data in collected_data:
            image = cv2.imread(data['image_path'])
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            ret, corners = cv2.findChessboardCorners(gray, self.board_size, None)

            if ret:
                corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), self.criteria)
                valid_data.append({
                    'frame_id': data['frame_id'],
                    'pose': data['pose'],
                    'corners': corners_refined,
                    'image': image
                })
                print(f"âœ… å¸§ {data['frame_id']}: æ£‹ç›˜æ ¼æ£€æµ‹æˆåŠŸ")
            else:
                print(f"âš ï¸  å¸§ {data['frame_id']}: æ£‹ç›˜æ ¼æ£€æµ‹å¤±è´¥ï¼Œè·³è¿‡")

        if len(valid_data) < 3:
            print(f"\nâŒ æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ç»„ï¼Œå½“å‰åªæœ‰ {len(valid_data)} ç»„")
            return False

        print(f"\nğŸ“Š æœ‰æ•ˆæ•°æ®: {len(valid_data)}/{len(collected_data)} ç»„")

        # ä½¿ç”¨å…¬å…±æ–¹æ³•å‡†å¤‡æ ‡å®šæ•°æ®ï¼ˆä¸è®¡ç®—é‡æŠ•å½±è¯¯å·®ï¼‰
        R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, _, _ = \
            self._prepare_calibration_data(valid_data, compute_reprojection_errors=False)

        if len(R_gripper2base) != len(R_target2cam) or len(R_gripper2base) < 3:
            print(f"âŒ æ ‡å®šæ•°æ®å‡†å¤‡å¤±è´¥")
            return False

        # æä¾›å¤šç§æ ‡å®šç®—æ³•é€‰æ‹©
        calibration_methods = [
            {'method': cv2.CALIB_HAND_EYE_TSAI, 'name': 'Tsai', 'description': 'ç»å…¸ç®—æ³•ï¼Œé€‚ç”¨äºå¤šæ•°æƒ…å†µ'},
            {'method': cv2.CALIB_HAND_EYE_PARK, 'name': 'Park', 'description': 'å¯¹å™ªå£°é²æ£’æ€§å¥½'},
            {'method': cv2.CALIB_HAND_EYE_HORAUD, 'name': 'Horaud', 'description': 'é«˜ç²¾åº¦ï¼Œéœ€è¦æ›´å¤šæ•°æ®'},
            {'method': cv2.CALIB_HAND_EYE_ANDREFF, 'name': 'Andreff', 'description': 'å¿«é€Ÿç®—æ³•'},
            {'method': cv2.CALIB_HAND_EYE_DANIILIDIS, 'name': 'Daniilidis', 'description': 'æ•°å€¼ç¨³å®šæ€§å¥½'}
        ]

        print(f"\nğŸ”§ æ‰§è¡Œæ‰‹çœ¼æ ‡å®š...")
        print(f"   æ•°æ®ç‚¹æ•°: {len(R_gripper2base)}")
        print("\né€‰æ‹©æ ‡å®šç®—æ³•:")
        for i, method_info in enumerate(calibration_methods, 1):
            print(f"  {i}. {method_info['name']} - {method_info['description']}")

        '''
        choice = input("é€‰æ‹©ç®—æ³• (1-5) [é»˜è®¤1-Tsai]: ").strip()
        if choice == '':
            choice = '1'  # é»˜è®¤ä½¿ç”¨Tsaiç®—æ³•
        '''
        choice = '1'  #'1'
        try:
            method_index = int(choice) - 1
            if 0 <= method_index < len(calibration_methods):
                selected_method = calibration_methods[method_index]
            else:
                print("âš ï¸  æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨Parkç®—æ³•")
                selected_method = calibration_methods[1]
        except ValueError:
            print("âš ï¸  æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨Parkç®—æ³•")
            selected_method = calibration_methods[1]

        print(f"\nä½¿ç”¨ {selected_method['name']} ç®—æ³•è¿›è¡Œæ ‡å®š...")

        try:
            # æ‰§è¡ŒOpenCVæ‰‹çœ¼æ ‡å®š
            R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
                R_gripper2base, t_gripper2base,
                R_target2cam, t_target2cam,
                method=selected_method['method']
            )

            print("\n" + "="*60)
            print("âœ… æ‰‹çœ¼æ ‡å®šå®Œæˆï¼")
            print("="*60)

            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’å’Œå››å…ƒæ•°
            rotation = R.from_matrix(R_cam2gripper)
            quaternion = rotation.as_quat()  # [x, y, z, w]
            euler_angles = rotation.as_euler('xyz', degrees=True)

            # è½¬æ¢å¹³ç§»å‘é‡
            t_cam2gripper_mm = t_cam2gripper.flatten() * 1000.0
            t_cam2gripper_m = t_cam2gripper.flatten()

            print("\nğŸ“Š æ ‡å®šç»“æœ:")
            print(f"   å¹³ç§»å‘é‡ (mm): X={t_cam2gripper_mm[0]:.2f}, Y={t_cam2gripper_mm[1]:.2f}, Z={t_cam2gripper_mm[2]:.2f}")
            print(f"   æ¬§æ‹‰è§’ (åº¦):    Rx={euler_angles[0]:.2f}Â°, Ry={euler_angles[1]:.2f}Â°, Rz={euler_angles[2]:.2f}Â°")
            print(f"   å››å…ƒæ•° (xyzw):  x={quaternion[0]:.4f}, y={quaternion[1]:.4f}, z={quaternion[2]:.4f}, w={quaternion[3]:.4f}")

            # è®¡ç®—è¯¯å·®è¯„ä¼°
            print("\n" + "="*60)
            print("ğŸ“Š æ ‡å®šè´¨é‡åˆ†æ")
            print("="*60)

            rotation_errors = []
            translation_errors = []

            for i, data in enumerate(valid_data):
                # ä½¿ç”¨æ ‡å®šç»“æœé¢„æµ‹
                R_target2base_pred = R_gripper2base[i] @ R_cam2gripper @ R_target2cam[i]
                t_target2base_pred = R_gripper2base[i] @ (R_cam2gripper @ t_target2cam[i] + t_cam2gripper) + t_gripper2base[i]

                # ç¬¬ä¸€å¸§ä½œä¸ºå‚è€ƒ
                if i == 0:
                    R_target2base_ref = R_target2base_pred.copy()
                    t_target2base_ref = t_target2base_pred.copy()

                # è®¡ç®—ç›¸å¯¹è¯¯å·®
                R_error = R_target2base_ref.T @ R_target2base_pred
                angle_error_rad = np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1))
                angle_error_deg = np.degrees(angle_error_rad)
                rotation_errors.append(angle_error_deg)

                t_error = t_target2base_pred - t_target2base_ref
                translation_error_mm = np.linalg.norm(t_error) * 1000.0
                translation_errors.append(translation_error_mm)

                status = "âœ…" if translation_error_mm < 5.0 and angle_error_deg < 2.0 else "âš ï¸"
                print(f"{status} å¸§ {data['frame_id']:2d}: æ—‹è½¬è¯¯å·® {angle_error_deg:6.3f}Â°  å¹³ç§»è¯¯å·® {translation_error_mm:7.3f}mm")

            # ç»Ÿè®¡
            avg_rotation_error = np.mean(rotation_errors)
            avg_translation_error = np.mean(translation_errors)

            print("\nğŸ“ˆ è¯¯å·®ç»Ÿè®¡:")
            print(f"   å¹³å‡æ—‹è½¬è¯¯å·®:  {avg_rotation_error:.3f}Â°")
            print(f"   å¹³å‡å¹³ç§»è¯¯å·®:  {avg_translation_error:.3f}mm")

            # è´¨é‡è¯„ä¼°
            if avg_rotation_error < 1.0 and avg_translation_error < 5.0:
                quality = "ğŸŒŸ ä¼˜ç§€"
            elif avg_rotation_error < 2.0 and avg_translation_error < 10.0:
                quality = "ğŸ‘ è‰¯å¥½"
            elif avg_rotation_error < 5.0 and avg_translation_error < 20.0:
                quality = "âš ï¸  å¯æ¥å—"
            else:
                quality = "âŒ éœ€è¦æ”¹è¿›"

            print(f"\nğŸ¯ æ ‡å®šè´¨é‡: {quality}")

            # ä¿å­˜æ ‡å®šç»“æœ
            result_data = {
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'method': f'CALIB_HAND_EYE_{selected_method["name"].upper()}',
                'data_points': len(R_gripper2base),
                'transformation': {
                    'rotation_matrix': R_cam2gripper.tolist(),
                    'translation_m': t_cam2gripper_m.tolist(),
                    'translation_mm': t_cam2gripper_mm.tolist(),
                    'quaternion_xyzw': quaternion.tolist(),
                    'euler_xyz_deg': euler_angles.tolist()
                },
                'quality': {
                    'avg_rotation_error_deg': float(avg_rotation_error),
                    'avg_translation_error_mm': float(avg_translation_error),
                    'quality_level': quality
                },
                'camera_intrinsics': {
                    'camera_matrix': self.camera_matrix.tolist() if self.camera_matrix is not None else None,
                    'distortion_coefficients': self.dist_coeffs.tolist() if self.dist_coeffs is not None else None,
                    'calibration_board': {
                        'board_size': list(self.board_size),
                        'square_size_mm': self.chessboard_size_mm
                    }
                }
            }

            # ä¿å­˜JSON
            result_file = os.path.join(save_dir, "hand_eye_calibration_result.json")
            with open(result_file, 'w') as f:
                json.dump(result_data, f, indent=2)

            # ä¿å­˜YAMLï¼ˆROSæ ¼å¼ï¼‰
            yaml_result = {
                'hand_eye_calibration': {
                    'translation': {
                        'x': float(t_cam2gripper_m[0]),
                        'y': float(t_cam2gripper_m[1]),
                        'z': float(t_cam2gripper_m[2])
                    },
                    'rotation': {
                        'x': float(quaternion[0]),
                        'y': float(quaternion[1]),
                        'z': float(quaternion[2]),
                        'w': float(quaternion[3])
                    }
                }
            }

            yaml_file = os.path.join(save_dir, "hand_eye_calibration_result.yaml")
            with open(yaml_file, 'w') as f:
                yaml.dump(yaml_result, f, default_flow_style=False)

            print(f"\nğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜åˆ°:")
            print(f"   - {result_file}")
            print(f"   - {yaml_file}")

            print("\nâœ… æ‰‹çœ¼æ ‡å®šæµç¨‹å®Œæˆï¼")
            return True

        except Exception as e:
            print(f"\nâŒ æ ‡å®šå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def verify_reference_position(self):
        """éªŒè¯å‚è€ƒä½ç½®çš„é‡å¤ç²¾åº¦"""
        print("\n   ğŸ” éªŒè¯å‚è€ƒä½ç½®é‡å¤ç²¾åº¦...")

        # ç§»åŠ¨åˆ°å‚è€ƒä½ç½®
        self.robot.arm.set_position(
            x=self.reference_position[0], y=self.reference_position[1], z=self.reference_position[2],
            roll=self.reference_position[3], pitch=self.reference_position[4], yaw=self.reference_position[5],
            wait=True, speed=50, use_gripper_center=False
        )
        time.sleep(2.0)  # ç­‰å¾…ç¨³å®š

        # è·å–å®é™…ä½ç½®
        _, actual_pose = self.robot.arm.get_position(return_gripper_center=False)
        actual_pose = actual_pose if isinstance(actual_pose, list) else actual_pose.tolist()

        # è®¡ç®—åå·®
        deviation = [
            abs(actual_pose[0] - self.reference_position[0]),
            abs(actual_pose[1] - self.reference_position[1]),
            abs(actual_pose[2] - self.reference_position[2]),
            abs(actual_pose[3] - self.reference_position[3]),
            abs(actual_pose[4] - self.reference_position[4]),
            abs(actual_pose[5] - self.reference_position[5])
        ]

        max_pos_deviation = max(deviation[:3])
        max_rot_deviation = max(deviation[3:])

        self.position_deviations.append({
            'pos_deviation': max_pos_deviation,
            'rot_deviation': max_rot_deviation
        })

        if max_pos_deviation > 2.0:
            print(f"   âš ï¸  å‚è€ƒä½ç½®åå·®è¾ƒå¤§: ä½ç½® {max_pos_deviation:.2f}mm, è§’åº¦ {max_rot_deviation:.2f}Â°")
            print(f"      å»ºè®®æ£€æŸ¥æœºæ¢°è‡‚é‡å¤å®šä½ç²¾åº¦")
        else:
            print(f"   âœ… å‚è€ƒä½ç½®åå·®æ­£å¸¸: ä½ç½® {max_pos_deviation:.2f}mm, è§’åº¦ {max_rot_deviation:.2f}Â°")

        return max_pos_deviation, max_rot_deviation

    def replay_trajectory(self, data_dir, return_to_zero=True):
        """é‡æ’­ä¿å­˜çš„è½¨è¿¹å¹¶é‡æ–°é‡‡é›†å›¾åƒè¿›è¡Œæ ‡å®š

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            return_to_zero: æ˜¯å¦æ¯æ¬¡éƒ½å…ˆå›é›¶ç‚¹ï¼ˆé»˜è®¤Trueï¼Œæé«˜é‡å¤ç²¾åº¦ï¼‰
        """
        print("\n" + "="*60)
        print("ğŸ”„ è½¨è¿¹é‡æ’­æ¨¡å¼")
        print("="*60)

        if return_to_zero:
            print("âš¡ æ¨¡å¼ï¼šé«˜ç²¾åº¦æ¨¡å¼ - æ¯ä¸ªä½ç½®éƒ½å…ˆå›é›¶ç‚¹")
            print("   æµç¨‹ï¼šé›¶ç‚¹ â†’ ä½ç½®1 â†’ é›¶ç‚¹ â†’ ä½ç½®2 â†’ é›¶ç‚¹ â†’ ä½ç½®3 ...")
            print("   ä¼˜åŠ¿ï¼šæ¶ˆé™¤ç´¯ç§¯è¯¯å·®ï¼Œæé«˜é‡å¤å®šä½ç²¾åº¦")
            print("   ğŸ“ æ¯5ä¸ªä½ç½®è¿”å›å‚è€ƒä½ç½®éªŒè¯é‡å¤ç²¾åº¦")
        else:
            print("âš¡ æ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å¼ - è¿ç»­ç§»åŠ¨")
            print("   æµç¨‹ï¼šä½ç½®1 â†’ ä½ç½®2 â†’ ä½ç½®3 ...")
            print("   ä¼˜åŠ¿ï¼šé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½æœ‰ç´¯ç§¯è¯¯å·®")

        # å®šä¹‰å‚è€ƒä½ç½®ï¼ˆç”¨äºéªŒè¯é‡å¤ç²¾åº¦ï¼‰
        self.reference_position = [300, 0, 300, 180, 60, 180]  # ç¨³å®šçš„å‚è€ƒä½ç½®
        self.reference_check_interval = 5  # æ¯5ä¸ªä½ç½®æ£€æŸ¥ä¸€æ¬¡
        self.position_deviations = []  # è®°å½•ä½ç½®åå·®

        # è¯»å–poses.txtæ–‡ä»¶
        poses_file = os.path.join(data_dir, "poses.txt")
        if not os.path.exists(poses_file):
            print(f"âŒ æ‰¾ä¸åˆ°ä½å§¿æ–‡ä»¶: {poses_file}")
            return False

        # è§£æä½å§¿æ•°æ®
        replay_poses = []
        with open(poses_file, 'r') as f:
            for line in f:
                if not line.startswith('#') and line.strip():
                    parts = line.strip().split()
                    if len(parts) == 7:
                        frame_id = int(parts[0])
                        roll_rad = float(parts[1])
                        pitch_rad = float(parts[2])
                        yaw_rad = float(parts[3])
                        x_m = float(parts[4])
                        y_m = float(parts[5])
                        z_m = float(parts[6])

                        # è½¬æ¢å›æ¯«ç±³å’Œåº¦
                        pose_mm_deg = [
                            x_m * 1000.0,
                            y_m * 1000.0,
                            z_m * 1000.0,
                            np.degrees(roll_rad),
                            np.degrees(pitch_rad),
                            np.degrees(yaw_rad)
                        ]
                        replay_poses.append({
                            'frame_id': frame_id,
                            'pose_mm_deg': pose_mm_deg,
                            'pose_m_rad': [x_m, y_m, z_m, roll_rad, pitch_rad, yaw_rad]
                        })

        if not replay_poses:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä½å§¿æ•°æ®")
            return False

        print(f"ğŸ“Š æ‰¾åˆ° {len(replay_poses)} ä¸ªä½å§¿ç‚¹")

        # åˆ›å»ºæ–°çš„ä¿å­˜ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(self.calibration_data_dir, f"replay_calibration_{timestamp}")
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸ“ é‡æ’­æ•°æ®ä¿å­˜ç›®å½•: {save_dir}/")

        # ä¿å­˜åŸå§‹æ•°æ®ç›®å½•å¼•ç”¨
        with open(os.path.join(save_dir, "replay_source.txt"), 'w') as f:
            f.write(f"Original data from: {data_dir}\n")
            f.write(f"Replay time: {timestamp}\n")

        # é‡æ’­æ”¶é›†çš„æ•°æ®
        collected_data = []
        poses_txt_path = os.path.join(save_dir, "poses.txt")

        # å†™å…¥æ–‡ä»¶å¤´
        with open(poses_txt_path, 'w') as f:
            f.write("# é‡æ’­æ‰‹çœ¼æ ‡å®šæ•°æ® - æœºæ¢°è‡‚æœ«ç«¯ä½å§¿\n")
            f.write("# æ ¼å¼: frame_id roll(rad) pitch(rad) yaw(rad) x(m) y(m) z(m)\n")
            f.write(f"# é‡æ’­æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸå§‹æ•°æ®: {data_dir}\n")
            f.write("#" + "-"*70 + "\n")

        # ä¿å­˜ç›¸æœºå†…å‚åˆ°æ•°æ®ç›®å½•
        if self.camera_matrix is not None and self.dist_coeffs is not None:
            intrinsics_path = os.path.join(save_dir, "camera_intrinsics.yaml")
            # è·å–å†…å‚æ¥æºä¿¡æ¯
            _, _, source = self.get_camera_intrinsics(save_to_dir=save_dir)
            if self.save_intrinsics_to_file(intrinsics_path, self.camera_matrix, self.dist_coeffs, source):
                print(f"ğŸ’¾ ç›¸æœºå†…å‚å·²ä¿å­˜åˆ°: {intrinsics_path}")

        print("\nğŸ¬ å¼€å§‹è½¨è¿¹é‡æ’­...")
        print("æ“ä½œè¯´æ˜:")
        print("  [ç©ºæ ¼] - æš‚åœ/ç»§ç»­")
        print("  [ESC]  - ç»“æŸé‡æ’­å¹¶æ ‡å®š")
        print("  [Q]    - é€€å‡º")
        print("-"*60)

        paused = False
        current_index = 0

        try:
            while current_index < len(replay_poses):
                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord(' '):
                    paused = not paused
                    if paused:
                        print("â¸ï¸  æš‚åœé‡æ’­")
                    else:
                        print("â–¶ï¸  ç»§ç»­é‡æ’­")
                    continue
                elif key == 27:  # ESC
                    print("\nğŸ“Š é‡æ’­ç»“æŸï¼Œå¼€å§‹æ ‡å®š...")
                    break
                elif key == ord('q') or key == ord('Q'):
                    print("\nâŒ ç”¨æˆ·å–æ¶ˆé‡æ’­")
                    return False

                if paused:
                    time.sleep(0.1)
                    continue

                # è·å–å½“å‰è¦é‡æ’­çš„ä½å§¿
                target_pose = replay_poses[current_index]
                pose_mm_deg = target_pose['pose_mm_deg']

                print(f"\nâ–¶ï¸  é‡æ’­ç¬¬ {target_pose['frame_id']}/{len(replay_poses)} ä¸ªä½ç½®...")

                if return_to_zero:
                    # é«˜ç²¾åº¦æ¨¡å¼ï¼šå…ˆå›åˆå§‹ä½ç½®ï¼Œå†å›é›¶ç‚¹ï¼Œæœ€ååˆ°ç›®æ ‡ä½ç½®
                    print(f"   ğŸ”„ [æ­¥éª¤1/3] å…ˆå›åˆ°åˆå§‹å®‰å…¨ä½ç½®...")
                    print(f"      åˆå§‹ä½ç½®: X={self.initial_position[0]:.1f}mm Y={self.initial_position[1]:.1f}mm Z={self.initial_position[2]:.1f}mm")
                    print(f"      åˆå§‹å§¿æ€: R={self.initial_position[3]:.1f}Â° P={self.initial_position[4]:.1f}Â° Y={self.initial_position[5]:.1f}Â°")


                    # å…ˆå›åˆ°åˆå§‹å®‰å…¨ä½ç½®
                    self.robot.arm.set_position(
                        x=self.initial_position[0], y=self.initial_position[1], z=self.initial_position[2],
                        roll=self.initial_position[3], pitch=self.initial_position[4], yaw=self.initial_position[5],
                        wait=True, speed=self.motion_config['normal_speed'], use_gripper_center=False
                    )
                    print(f"   ğŸ”„ [æ­¥éª¤2/3] ä»åˆå§‹ä½ç½®åˆ°æœºæ¢°è‡‚å…³èŠ‚é›¶ç‚¹...")
                    print(f"      å…³èŠ‚é›¶ç‚¹: æ‰€æœ‰å…³èŠ‚è§’åº¦å½’é›¶")
                    # å†ç§»åŠ¨åˆ°æœºæ¢°è‡‚å…³èŠ‚é›¶ç‚¹
                    self.robot.arm._go_zero()
                    time.sleep(3.0)  # å¢åŠ åˆ°2ç§’ï¼Œç¡®ä¿å®Œå…¨ç¨³å®š

                    # æ­¥éª¤3ï¼šä»é›¶ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                    print(f"   ğŸ“ [æ­¥éª¤3/3] ä»é›¶ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®...")
                else:
                    # å¿«é€Ÿæ¨¡å¼ï¼šç›´æ¥ç§»åŠ¨ï¼ˆæ›´å¿«ä½†å¯èƒ½ç´¯ç§¯è¯¯å·®ï¼‰
                    print(f"   ğŸ“ ç›´æ¥ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰...")

                print(f"      ç›®æ ‡ä½ç½®: X={pose_mm_deg[0]:.1f}mm Y={pose_mm_deg[1]:.1f}mm Z={pose_mm_deg[2]:.1f}mm")
                print(f"      ç›®æ ‡å§¿æ€: R={pose_mm_deg[3]:.1f}Â° P={pose_mm_deg[4]:.1f}Â° Y={pose_mm_deg[5]:.1f}Â°")

                self.robot.arm.set_position(
                    x=pose_mm_deg[0], y=pose_mm_deg[1], z=pose_mm_deg[2],
                    roll=pose_mm_deg[3], pitch=pose_mm_deg[4], yaw=pose_mm_deg[5],
                    wait=True, speed=self.motion_config['capture_speed'], use_gripper_center=False
                )

                # ç­‰å¾…æœºå™¨è‡‚å®Œå…¨ç¨³å®šï¼ˆä½¿ç”¨é…ç½®çš„ç­‰å¾…æ—¶é—´ï¼‰
                print("   â±ï¸  ç­‰å¾…ç¨³å®š...")
                time.sleep(self.motion_config['stability_wait'])  # ä½¿ç”¨é…ç½®å€¼(5.0ç§’)

                # è·å–å®é™…åˆ°è¾¾çš„ä½ç½®ï¼Œç”¨äºéªŒè¯
                _, actual_pose = self.robot.arm.get_position(return_gripper_center=False)
                actual_pose = actual_pose if isinstance(actual_pose, list) else actual_pose.tolist()

                # å°†å®é™…ä½ç½®è½¬æ¢ä¸ºç±³å’Œå¼§åº¦ï¼ˆç”¨äºæ ‡å®šï¼‰
                actual_pose_m_rad = [
                    actual_pose[0] / 1000.0,  # mm to m
                    actual_pose[1] / 1000.0,
                    actual_pose[2] / 1000.0,
                    np.radians(actual_pose[3]),  # deg to rad
                    np.radians(actual_pose[4]),
                    np.radians(actual_pose[5])
                ]

                # è®¡ç®—ä½ç½®è¯¯å·®
                pos_error = [
                    abs(actual_pose[0] - pose_mm_deg[0]),
                    abs(actual_pose[1] - pose_mm_deg[1]),
                    abs(actual_pose[2] - pose_mm_deg[2])
                ]

                if max(pos_error) > 2.0:  # å¦‚æœä½ç½®è¯¯å·®è¶…è¿‡2mm
                    print(f"   âš ï¸  ä½ç½®è¯¯å·®è¾ƒå¤§: X={pos_error[0]:.2f}mm Y={pos_error[1]:.2f}mm Z={pos_error[2]:.2f}mm")
                else:
                    print(f"   âœ… åˆ°è¾¾ç›®æ ‡ä½ç½®ï¼ˆè¯¯å·®<2mmï¼‰")

                # è·å–ç›¸æœºå›¾åƒ
                frames = self.pipeline.wait_for_frames()
                color_frame = frames.get_color_frame()

                if color_frame:
                    color_image = np.asanyarray(color_frame.get_data())
                    display_image = color_image.copy()

                    # æ£€æµ‹æ£‹ç›˜æ ¼
                    gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
                    ret, corners = cv2.findChessboardCorners(gray, self.board_size, None)

                    if ret:
                        corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), self.criteria)
                        cv2.drawChessboardCorners(display_image, self.board_size, corners_refined, ret)
                        cv2.putText(display_image, "DETECTED", (10, 30),
                                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
                    else:
                        cv2.putText(display_image, "NOT DETECTED", (10, 30),
                                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)

                    # æ˜¾ç¤ºè¿›åº¦
                    cv2.putText(display_image, f"Replay: {current_index+1}/{len(replay_poses)}", (10, 70),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
                    cv2.putText(display_image, "[SPACE] Pause  [ESC] Stop  [Q] Quit",
                               (10, display_image.shape[0]-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                    cv2.imshow("Replay Mode", display_image)

                    # ä¿å­˜å›¾åƒ
                    frame_id = target_pose['frame_id']
                    image_filename = f"{frame_id}_Color.png"
                    image_path = os.path.join(save_dir, image_filename)
                    cv2.imwrite(image_path, color_image)

                    # ä¿å­˜å®é™…ä½å§¿ï¼ˆè€Œä¸æ˜¯ç›®æ ‡ä½å§¿ï¼‰
                    with open(poses_txt_path, 'a') as f:
                        f.write(f"{frame_id} {actual_pose_m_rad[3]:.6f} {actual_pose_m_rad[4]:.6f} {actual_pose_m_rad[5]:.6f} "
                               f"{actual_pose_m_rad[0]:.6f} {actual_pose_m_rad[1]:.6f} {actual_pose_m_rad[2]:.6f}\n")

                    # è®°å½•æ•°æ®ï¼ˆä½¿ç”¨å®é™…ä½ç½®ï¼‰
                    collected_data.append({
                        'frame_id': frame_id,
                        'image_path': image_path,
                        'pose': actual_pose_m_rad,  # ä½¿ç”¨å®é™…ä½ç½®
                        'pose_original': actual_pose,  # ä½¿ç”¨å®é™…ä½ç½®ï¼ˆmmå’Œåº¦ï¼‰
                        'has_chessboard': ret,
                        'corners': corners_refined if ret else None
                    })

                    print(f"   âœ… å·²é‡‡é›†ç¬¬ {frame_id} å¸§æ•°æ®")
                    if ret:
                        print(f"   ğŸ“· æ£‹ç›˜æ ¼: æ£€æµ‹æˆåŠŸ")
                    else:
                        print(f"   âš ï¸  æ£‹ç›˜æ ¼: æœªæ£€æµ‹åˆ°")

                    # å®šæœŸéªŒè¯å‚è€ƒä½ç½®ï¼ˆæ¯5ä¸ªä½ç½®æ£€æŸ¥ä¸€æ¬¡ï¼‰
                    if current_index % self.reference_check_interval == 0 and current_index > 0:
                        print(f"\n   ğŸ“ å®šæœŸéªŒè¯å‚è€ƒä½ç½® (ç¬¬ {current_index+1} ä¸ªä½ç½®å)")
                        pos_dev, rot_dev = self.verify_reference_position()

                        # å¦‚æœåå·®å¤ªå¤§ï¼Œæç¤ºç”¨æˆ·
                        if pos_dev > 3.0:
                            print(f"   âš ï¸  è­¦å‘Šï¼šç´¯ç§¯è¯¯å·®è¾ƒå¤§ï¼Œå»ºè®®é‡æ–°å¼€å§‹æ ‡å®š")
                            print(f"      å½“å‰åå·®: ä½ç½® {pos_dev:.2f}mm, è§’åº¦ {rot_dev:.2f}Â°")
                            user_input = input("   æ˜¯å¦ç»§ç»­? (y/n): ")
                            if user_input.lower() != 'y':
                                print("   âŒ ç”¨æˆ·é€‰æ‹©åœæ­¢")
                                break

                current_index += 1
                time.sleep(0.5)  # æ§åˆ¶é‡æ’­é€Ÿåº¦

        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­é‡æ’­")

        finally:
            cv2.destroyAllWindows()

        # æ˜¾ç¤ºä½ç½®åå·®ç»Ÿè®¡
        if hasattr(self, 'position_deviations') and self.position_deviations:
            print("\n" + "="*60)
            print("ğŸ“Š å‚è€ƒä½ç½®é‡å¤ç²¾åº¦ç»Ÿè®¡")
            print("="*60)
            avg_pos_dev = np.mean([d['pos_deviation'] for d in self.position_deviations])
            avg_rot_dev = np.mean([d['rot_deviation'] for d in self.position_deviations])
            max_pos_dev = np.max([d['pos_deviation'] for d in self.position_deviations])
            max_rot_dev = np.max([d['rot_deviation'] for d in self.position_deviations])

            print(f"éªŒè¯æ¬¡æ•°: {len(self.position_deviations)}")
            print(f"å¹³å‡åå·®: ä½ç½® {avg_pos_dev:.2f}mm, è§’åº¦ {avg_rot_dev:.2f}Â°")
            print(f"æœ€å¤§åå·®: ä½ç½® {max_pos_dev:.2f}mm, è§’åº¦ {max_rot_dev:.2f}Â°")

            if max_pos_dev > 2.0:
                print("\nâš ï¸  è­¦å‘Š: æœºæ¢°è‡‚é‡å¤å®šä½ç²¾åº¦ä¸è¶³")
                print("   å»ºè®®æ£€æŸ¥:")
                print("   â€¢ æœºæ¢°è‡‚é½¿è½®é—´éš™")
                print("   â€¢ è´Ÿè½½æ˜¯å¦è¿‡é‡")
                print("   â€¢ è¿åŠ¨é€Ÿåº¦æ˜¯å¦è¿‡å¿«")
            else:
                print("\nâœ… æœºæ¢°è‡‚é‡å¤å®šä½ç²¾åº¦è‰¯å¥½")

        # é‡æ’­å®Œæˆåè¿›è¡Œæ ‡å®š
        if len(collected_data) >= 3:
            print(f"\nğŸ“Š é‡æ’­å®Œæˆï¼Œé‡‡é›†äº† {len(collected_data)} å¸§æ•°æ®")
            print("ğŸ”§ å¼€å§‹æ‰‹çœ¼æ ‡å®š...")
            self.perform_manual_calibration(collected_data, save_dir)
        else:
            print(f"\nâš ï¸  æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼ˆ{len(collected_data)} å¸§ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ ‡å®š")

        return True

    def compute_calibration_only(self, data_dir, use_optimization=True):
        """ä»…åŸºäºå·²æœ‰æ•°æ®è¿›è¡Œæ ‡å®šè®¡ç®—ï¼Œä¸ç§»åŠ¨æœºæ¢°è‡‚

        Args:
            data_dir: æ•°æ®ç›®å½•
            use_optimization: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆRANSAC+å¤šç®—æ³•+è¿­ä»£ï¼‰
        """
        print("\n" + "="*60)
        print("ğŸ§® çº¯è®¡ç®—é‡æ’­æ¨¡å¼ - ä¸ç§»åŠ¨æœºæ¢°è‡‚")
        if use_optimization:
            print("âœ¨ å¯ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆRANSAC+å¤šç®—æ³•èåˆ+è¿­ä»£ä¼˜åŒ–ï¼‰")
        print("="*60)

        # è¯»å–poses.txtæ–‡ä»¶
        poses_file = os.path.join(data_dir, "poses.txt")
        if not os.path.exists(poses_file):
            print(f"âŒ æ‰¾ä¸åˆ°ä½å§¿æ–‡ä»¶: {poses_file}")
            return False

        # è§£æä½å§¿æ•°æ®
        poses_data = []
        with open(poses_file, 'r') as f:
            for line in f:
                if not line.startswith('#') and line.strip():
                    parts = line.strip().split()
                    if len(parts) == 7:
                        frame_id = int(parts[0])
                        roll_rad = float(parts[1])
                        pitch_rad = float(parts[2])
                        yaw_rad = float(parts[3])
                        x_m = float(parts[4])
                        y_m = float(parts[5])
                        z_m = float(parts[6])

                        poses_data.append({
                            'frame_id': frame_id,
                            'pose': [x_m, y_m, z_m, roll_rad, pitch_rad, yaw_rad]
                        })

        if not poses_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä½å§¿æ•°æ®")
            return False

        print(f"ğŸ“Š æ‰¾åˆ° {len(poses_data)} ä¸ªä½å§¿æ•°æ®")

        # ä½¿ç”¨é€šç”¨å‡½æ•°è·å–ç›¸æœºå†…å‚
        self.camera_matrix, self.dist_coeffs, source = self.get_camera_intrinsics(data_dir=data_dir, save_to_dir=data_dir)
        print(f"ğŸ“· å†…å‚æ¥æº: {source}")

        # æ£€æµ‹æ£‹ç›˜æ ¼å¹¶æ”¶é›†æœ‰æ•ˆæ•°æ®
        valid_data = []
        print("\næ£€æµ‹æ£‹ç›˜æ ¼...")

        for pose_data in poses_data:
            frame_id = pose_data['frame_id']
            image_path = os.path.join(data_dir, f"{frame_id}_Color.png")

            if not os.path.exists(image_path):
                print(f"âš ï¸  å¸§ {frame_id}: æ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶")
                continue

            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # æ£€æµ‹æ£‹ç›˜æ ¼
            ret, corners = cv2.findChessboardCorners(
                gray, self.board_size,
                flags=cv2.CALIB_CB_ADAPTIVE_THRESH +
                      cv2.CALIB_CB_NORMALIZE_IMAGE +
                      cv2.CALIB_CB_FILTER_QUADS
            )

            if ret:
                corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), self.criteria)
                valid_data.append({
                    'frame_id': frame_id,
                    'pose': pose_data['pose'],
                    'corners': corners_refined,
                    'image': image,
                    'image_path': image_path  # æ·»åŠ image_pathä»¥å…¼å®¹perform_manual_calibration
                })
                print(f"âœ… å¸§ {frame_id}: æ£‹ç›˜æ ¼æ£€æµ‹æˆåŠŸ")
            else:
                print(f"âš ï¸  å¸§ {frame_id}: æ£‹ç›˜æ ¼æ£€æµ‹å¤±è´¥")

        if len(valid_data) < 3:
            print(f"\nâŒ æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ç»„ï¼Œå½“å‰åªæœ‰ {len(valid_data)} ç»„")
            return False

        print(f"\nğŸ“Š æœ‰æ•ˆæ•°æ®: {len(valid_data)}/{len(poses_data)} ç»„")

        # Yawåå·®è¿‡æ»¤ - ä½¿ç”¨initial_positionçš„Yawä½œä¸ºå‚è€ƒ
        reference_yaw_deg = self.initial_position[5]  # åº¦
        print(f"\nğŸ“ ä½¿ç”¨å‚è€ƒYaw: {reference_yaw_deg:.1f}Â° (æ¥è‡ªinitial_position)")

        '''
        valid_data = self.filter_by_yaw_deviation(
            valid_data,
            reference_yaw_deg=reference_yaw_deg,
            max_deviation=30.0
        )

        if len(valid_data) < 3:
            print(f"\nâŒ Yawè¿‡æ»¤åæ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ç»„ï¼Œå½“å‰åªæœ‰ {len(valid_data)} ç»„")
            print(f"ğŸ’¡ å»ºè®®: é‡æ–°é‡‡é›†æ•°æ®ï¼Œä¿æŒYawåœ¨ {reference_yaw_deg-30:.1f}Â° ~ {reference_yaw_deg+30:.1f}Â° èŒƒå›´å†…")
            return False

        print(f"\nâœ… Yawè¿‡æ»¤åæœ‰æ•ˆæ•°æ®: {len(valid_data)} ç»„")
        '''
        
        # åˆ›å»ºæ–°çš„ä¿å­˜ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(self.calibration_data_dir, f"compute_only_calibration_{timestamp}")
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {save_dir}/")

        # ä¿å­˜æºæ•°æ®ä¿¡æ¯
        with open(os.path.join(save_dir, "source_info.txt"), 'w') as f:
            f.write(f"Source data: {data_dir}\n")
            f.write(f"Compute time: {timestamp}\n")
            f.write(f"Valid frames: {len(valid_data)}\n")
            f.write(f"Camera intrinsics source: {source}\n")

        # ä¿å­˜ç›¸æœºå†…å‚åˆ°æ•°æ®ç›®å½•
        if self.camera_matrix is not None and self.dist_coeffs is not None:
            intrinsics_path = os.path.join(save_dir, "camera_intrinsics.yaml")
            if self.save_intrinsics_to_file(intrinsics_path, self.camera_matrix, self.dist_coeffs, source):
                print(f"ğŸ’¾ ç›¸æœºå†…å‚å·²ä¿å­˜åˆ°: {intrinsics_path}")

        # æ‰§è¡Œæ ‡å®šè®¡ç®—
        print("\nğŸ”§ å¼€å§‹æ ‡å®šè®¡ç®—...")
        

        # æ ¹æ®é€‰é¡¹ä½¿ç”¨ä¸åŒçš„æ ‡å®šæ–¹æ³•
        if use_optimization:
            calibration_success = self.perform_optimized_calibration(valid_data, save_dir)
        else:
            calibration_success = self.perform_manual_calibration(valid_data, save_dir)

        if calibration_success:
            print("\nâœ… çº¯è®¡ç®—æ ‡å®šå®Œæˆï¼")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")

        return calibration_success

    def load_and_replay(self, data_dir, compute_only=False):
        """åŠ è½½å·²æœ‰æ•°æ®ç›®å½•å¹¶é‡æ’­è½¨è¿¹æˆ–ä»…è®¡ç®—"""
        print("\n" + "="*60)
        print("ğŸ“‚ åŠ è½½å†å²æ•°æ®")
        print("="*60)

        if not os.path.exists(data_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return False

        poses_file = os.path.join(data_dir, "poses.txt")
        if not os.path.exists(poses_file):
            print(f"âŒ æ‰¾ä¸åˆ°ä½å§¿æ–‡ä»¶: {poses_file}")
            return False

        print(f"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}")

        # æ˜¾ç¤ºåŸå§‹æ ‡å®šä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        result_file = os.path.join(data_dir, "hand_eye_calibration_result.json")
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                result = json.load(f)
            print(f"ğŸ“Š åŸå§‹æ ‡å®šä¿¡æ¯:")
            print(f"   æ—¶é—´: {result.get('timestamp', 'Unknown')}")
            print(f"   æ•°æ®ç‚¹: {result.get('data_points', 'Unknown')}")
            if 'quality' in result:
                print(f"   è´¨é‡: {result['quality'].get('quality_level', 'Unknown')}")

        # å¦‚æœæ˜¯çº¯è®¡ç®—æ¨¡å¼ï¼Œç›´æ¥è¿›è¡Œè®¡ç®—
        if compute_only:
            return self.compute_calibration_only(data_dir)

        choice  = 'y'
        mode_choice = '1'
        '''
        # è¯¢é—®æ˜¯å¦ç»§ç»­
        print("\næ˜¯å¦ä½¿ç”¨æ­¤æ•°æ®è¿›è¡Œè½¨è¿¹é‡æ’­?")
        choice = input("ç»§ç»­? (y/n): ")
        if choice.lower() != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆ")
            return False

        # è¯¢é—®é‡æ’­æ¨¡å¼
        print("\né€‰æ‹©é‡æ’­æ¨¡å¼:")
        print("  1. é«˜ç²¾åº¦æ¨¡å¼ - æ¯æ¬¡ï¼šåˆå§‹ä½ç½®â†’é›¶ç‚¹â†’ç›®æ ‡ï¼ˆæ¨èï¼Œæ¶ˆé™¤ç´¯ç§¯è¯¯å·®ï¼‰")
        print("  2. å¿«é€Ÿæ¨¡å¼ - ç›´æ¥ç§»åŠ¨åˆ°ç›®æ ‡ï¼ˆæ›´å¿«ä½†å¯èƒ½æœ‰ç´¯ç§¯è¯¯å·®ï¼‰")
        mode_choice = input("é€‰æ‹©æ¨¡å¼ (1/2) [é»˜è®¤1]: ").strip()
        '''


        return_to_zero = True  # é»˜è®¤é«˜ç²¾åº¦æ¨¡å¼
        if mode_choice == "2":
            return_to_zero = False
            print("âš¡ å·²é€‰æ‹©å¿«é€Ÿæ¨¡å¼")
        else:
            print("âœ… å·²é€‰æ‹©é«˜ç²¾åº¦æ¨¡å¼")

        # æ‰§è¡Œé‡æ’­
        return self.replay_trajectory(data_dir, return_to_zero=return_to_zero)


def main():
    """ä¸»å‡½æ•° - æ”¯æŒæ‰‹åŠ¨æ¨¡å¼å’Œé‡æ’­æ¨¡å¼"""
    print("="*60)
    print("ğŸ¯ æ‰‹çœ¼æ ‡å®šç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("="*60)
    print("åŠŸèƒ½æ¨¡å¼:")
    print("  1. æ‰‹åŠ¨æ ‡å®š - æ‰‹åŠ¨ç§»åŠ¨æœºå™¨è‡‚é‡‡é›†æ•°æ®")
    print("  2. è½¨è¿¹é‡æ’­ - é‡æ’­å·²ä¿å­˜çš„è½¨è¿¹è¿›è¡Œæ ‡å®šï¼ˆéœ€è¦æœºæ¢°è‡‚ï¼‰")
    print("  3. çº¯è®¡ç®—é‡æ’­ - åŸºäºå·²æœ‰æ•°æ®é‡æ–°è®¡ç®—ï¼ˆä¸éœ€è¦æœºæ¢°è‡‚ï¼‰")
    print("  4. æŸ¥çœ‹å†å² - åˆ—å‡ºæ‰€æœ‰å†å²æ ‡å®šæ•°æ®")
    print("="*60)

    # é€‰æ‹©æ¨¡å¼
    mode = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1/2/3/4) [é»˜è®¤1]: ").strip()
    if not mode:
        mode = "1"

    scanner = ManualHandEyeCalibrator()

    try:
        if mode == "1":
            # æ‰‹åŠ¨æ ‡å®šæ¨¡å¼
            print("\n" + "="*60)
            print("ğŸ“ æ‰‹åŠ¨æ ‡å®šæ¨¡å¼")
            print("="*60)

            # è¿æ¥è®¾å¤‡ï¼ˆä¼šè‡ªåŠ¨è·å–ç›¸æœºå†…å‚ï¼‰
            if scanner.connect_devices():
                print("\nâœ… è®¾å¤‡è¿æ¥æˆåŠŸï¼")
                scanner.manual_calibration_mode()
            else:
                print("âŒ è®¾å¤‡è¿æ¥å¤±è´¥")

        elif mode == "2":
            # è½¨è¿¹é‡æ’­æ¨¡å¼
            print("\n" + "="*60)
            print("ğŸ”„ è½¨è¿¹é‡æ’­æ¨¡å¼")
            print("="*60)

            # åˆ—å‡ºå¯ç”¨çš„å†å²æ•°æ®ç›®å½•
            data_dirs = scanner.get_historical_data_dirs()

            if not data_dirs:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å†å²æ ‡å®šæ•°æ®")
                print("   è¯·å…ˆè¿è¡Œæ‰‹åŠ¨æ ‡å®šæ¨¡å¼ç”Ÿæˆæ•°æ®")
                return

            print("\nğŸ“‚ å¯ç”¨çš„å†å²æ•°æ®:")
            for i, dir_name in enumerate(data_dirs, 1):
                # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å®šç»“æœ
                result_file = os.path.join(dir_name, "hand_eye_calibration_result.json")
                poses_file = os.path.join(dir_name, "poses.txt")

                if os.path.exists(poses_file):
                    # ç»Ÿè®¡ä½å§¿æ•°é‡
                    pose_count = 0
                    with open(poses_file, 'r') as f:
                        for line in f:
                            if not line.startswith('#') and line.strip():
                                pose_count += 1

                    status = "âœ… å·²æ ‡å®š" if os.path.exists(result_file) else "ğŸ“ æœªæ ‡å®š"
                    print(f"  {i}. {dir_name} ({pose_count} ä½å§¿) {status}")
                else:
                    print(f"  {i}. {dir_name} (æ— æ•ˆ)")

            # é€‰æ‹©æ•°æ®ç›®å½•
            choice = input("\né€‰æ‹©è¦é‡æ’­çš„æ•°æ® (è¾“å…¥åºå·): ").strip()
            try:
                index = int(choice) - 1
                if 0 <= index < len(data_dirs):
                    selected_dir = data_dirs[index]
                    print(f"\nå·²é€‰æ‹©: {selected_dir}")

                    # è¿æ¥è®¾å¤‡ï¼ˆä¼šè‡ªåŠ¨è·å–ç›¸æœºå†…å‚ï¼‰
                    if scanner.connect_devices():
                        print("\nâœ… è®¾å¤‡è¿æ¥æˆåŠŸï¼")
                        scanner.load_and_replay(selected_dir)
                    else:
                        print("âŒ è®¾å¤‡è¿æ¥å¤±è´¥")
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

        elif mode == "3":
            # çº¯è®¡ç®—é‡æ’­æ¨¡å¼
            print("\n" + "="*60)
            print("ğŸ§® çº¯è®¡ç®—é‡æ’­æ¨¡å¼")
            print("="*60)
            print("æ­¤æ¨¡å¼ä¸éœ€è¦è¿æ¥æœºæ¢°è‡‚ï¼Œä»…åŸºäºå·²æœ‰æ•°æ®é‡æ–°è®¡ç®—æ ‡å®šç»“æœ")

            # åˆ—å‡ºå¯ç”¨çš„å†å²æ•°æ®ç›®å½•
            data_dirs = scanner.get_historical_data_dirs()

            if not data_dirs:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å†å²æ ‡å®šæ•°æ®")
                print("   è¯·å…ˆè¿è¡Œæ‰‹åŠ¨æ ‡å®šæ¨¡å¼ç”Ÿæˆæ•°æ®")
                return

            print("\nğŸ“‚ å¯ç”¨çš„å†å²æ•°æ®:")
            for i, dir_name in enumerate(data_dirs, 1):
                # æ£€æŸ¥æ˜¯å¦æœ‰ä½å§¿å’Œå›¾åƒæ–‡ä»¶
                poses_file = os.path.join(dir_name, "poses.txt")
                result_file = os.path.join(dir_name, "hand_eye_calibration_result.json")

                if os.path.exists(poses_file):
                    # ç»Ÿè®¡ä½å§¿æ•°é‡
                    pose_count = 0
                    image_count = 0
                    with open(poses_file, 'r') as f:
                        for line in f:
                            if not line.startswith('#') and line.strip():
                                pose_count += 1

                    # ç»Ÿè®¡å›¾åƒæ•°é‡
                    images = glob.glob(os.path.join(dir_name, "*_Color.png"))
                    image_count = len(images)

                    status = "âœ… å·²æ ‡å®š" if os.path.exists(result_file) else "ğŸ“ æœªæ ‡å®š"
                    print(f"  {i}. {dir_name} ({pose_count} ä½å§¿, {image_count} å›¾åƒ) {status}")
                else:
                    print(f"  {i}. {dir_name} (æ— æ•ˆ)")

            # é€‰æ‹©æ•°æ®ç›®å½•
            choice = input("\né€‰æ‹©è¦é‡æ–°è®¡ç®—çš„æ•°æ® (è¾“å…¥åºå·): ").strip()
            try:
                index = int(choice) - 1
                if 0 <= index < len(data_dirs):
                    selected_dir = data_dirs[index]
                    print(f"\nå·²é€‰æ‹©: {selected_dir}")

                    # ä¸éœ€è¦è¿æ¥è®¾å¤‡ï¼Œç›´æ¥è¿›è¡Œè®¡ç®—
                    scanner.compute_calibration_only(selected_dir)
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

        elif mode == "4":
            # æŸ¥çœ‹å†å²æ¨¡å¼
            print("\n" + "="*60)
            print("ğŸ“Š å†å²æ ‡å®šæ•°æ®")
            print("="*60)

            # åˆ—å‡ºæ‰€æœ‰æ•°æ®ç›®å½•
            data_dirs = scanner.get_historical_data_dirs()

            if not data_dirs:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å†å²æ ‡å®šæ•°æ®")
                return

            for dir_name in data_dirs:
                print(f"\nğŸ“ {dir_name}:")

                # æ˜¾ç¤ºæ ‡å®šç»“æœ
                result_file = os.path.join(dir_name, "hand_eye_calibration_result.json")
                if os.path.exists(result_file):
                    with open(result_file, 'r') as f:
                        result = json.load(f)

                    print(f"   æ—¶é—´: {result.get('timestamp', 'Unknown')}")
                    print(f"   æ–¹æ³•: {result.get('method', 'Unknown')}")
                    print(f"   æ•°æ®ç‚¹: {result.get('data_points', 'Unknown')}")

                    if 'transformation' in result:
                        trans = result['transformation']
                        if 'translation_mm' in trans:
                            t = trans['translation_mm']
                            print(f"   å¹³ç§»: X={t[0]:.2f}mm Y={t[1]:.2f}mm Z={t[2]:.2f}mm")
                        if 'euler_xyz_deg' in trans:
                            e = trans['euler_xyz_deg']
                            print(f"   æ—‹è½¬: Rx={e[0]:.2f}Â° Ry={e[1]:.2f}Â° Rz={e[2]:.2f}Â°")

                    if 'quality' in result:
                        q = result['quality']
                        print(f"   è´¨é‡: {q.get('quality_level', 'Unknown')}")
                        print(f"   è¯¯å·®: æ—‹è½¬={q.get('avg_rotation_error_deg', 0):.3f}Â° "
                              f"å¹³ç§»={q.get('avg_translation_error_mm', 0):.3f}mm")
                else:
                    # åªæ˜¾ç¤ºä½å§¿æ–‡ä»¶ä¿¡æ¯
                    poses_file = os.path.join(dir_name, "poses.txt")
                    if os.path.exists(poses_file):
                        pose_count = 0
                        with open(poses_file, 'r') as f:
                            for line in f:
                                if not line.startswith('#') and line.strip():
                                    pose_count += 1
                        print(f"   çŠ¶æ€: æœªæ ‡å®š")
                        print(f"   ä½å§¿æ•°: {pose_count}")
                    else:
                        print(f"   çŠ¶æ€: æ— æ•ˆæ•°æ®")

        else:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å¼é€‰æ‹©: {mode}")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        if mode in ["1", "2"]:
            scanner.disconnect_devices()


if __name__ == "__main__":
    main()
