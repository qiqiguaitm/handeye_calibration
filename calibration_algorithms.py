#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
calibration_algorithms.py - æ‰‹çœ¼æ ‡å®šç®—æ³•æ¨¡å—

æä¾›:
- å¤šç®—æ³•èåˆ (Tsai, Park, Horaud, Daniilidis, Andreff)
- è¿­ä»£éçº¿æ€§ä¼˜åŒ–
- æ ‡å®šç»“æœè¯„ä¼°
- è¯¯å·®åˆ†æ

Design: Linus "Good Taste" åŸåˆ™
- æ¯ä¸ªç®—æ³•ç‹¬ç«‹,å¯å•ç‹¬æµ‹è¯•
- å¤±è´¥æ—¶ä¼˜é›…é™çº§
- æ¸…æ™°çš„è¯¯å·®åº¦é‡
"""

import numpy as np
import cv2
from scipy.optimize import minimize
from scipy.spatial.transform import Rotation as R
import scipy.spatial.transform as sst            

# ============================================================================
# æ‰‹çœ¼æ ‡å®šç®—æ³•
# ============================================================================

class HandEyeCalibration:
    """æ‰‹çœ¼æ ‡å®šç®—æ³•é›†åˆ"""

    # ç±»å˜é‡ï¼šä¿å­˜å®Œæ•´çš„æ ‡å®šç»“æœï¼ˆåŒ…æ‹¬å‰¯ç»“æœï¼‰
    _calibration_details = {}

    @staticmethod
    def get_calibration_quality(translation_error_mm, rotation_error_deg=None, 
                               mode='eye_in_hand'):
        """è·å–æ ‡å®šè´¨é‡çš„emojiè¯„çº§
        
        Args:
            translation_error_mm: å¹³ç§»è¯¯å·®ï¼ˆæ¯«ç±³ï¼‰
            rotation_error_deg: æ—‹è½¬è¯¯å·®ï¼ˆåº¦ï¼Œå¯é€‰ï¼‰
            mode: 'eye_in_hand' æˆ– 'eye_to_hand'
            
        Returns:
            str: å¸¦emojiçš„è´¨é‡è¯„çº§å­—ç¬¦ä¸²
        """
        if mode == 'eye_to_hand':
            # Eye-to-Hand æ ‡å‡†ç•¥å®½æ¾ä¸€äº›ï¼Œä¸»è¦çœ‹å¹³ç§»è¯¯å·®
            if translation_error_mm < 3.0:
                return "ğŸŒŸ ä¼˜ç§€"
            elif translation_error_mm < 5.0:
                return "ğŸ‘ è‰¯å¥½"
            elif translation_error_mm < 10.0:
                return "âš ï¸  å¯æ¥å—"
            else:
                return "âŒ éœ€è¦æ”¹è¿›"
        else:
            # Eye-in-Hand éœ€è¦ç»¼åˆè€ƒè™‘å¹³ç§»å’Œæ—‹è½¬è¯¯å·®
            if rotation_error_deg is not None:
                # ç»¼åˆè¯„çº§ï¼šåŒæ—¶è€ƒè™‘å¹³ç§»å’Œæ—‹è½¬
                if translation_error_mm < 2.0 and rotation_error_deg < 0.3:
                    return "ğŸŒŸ ä¼˜ç§€"
                elif translation_error_mm < 5.0 and rotation_error_deg < 0.5:
                    return "ğŸ‘ è‰¯å¥½"
                elif translation_error_mm < 10.0 and rotation_error_deg < 1.0:
                    return "âš ï¸  å¯æ¥å—"
                else:
                    return "âŒ éœ€è¦æ”¹è¿›"
            else:
                # ä»…åŸºäºå¹³ç§»è¯¯å·®
                if translation_error_mm < 3.0:
                    return "ğŸŒŸ ä¼˜ç§€"
                elif translation_error_mm < 5.0:
                    return "ğŸ‘ è‰¯å¥½"
                elif translation_error_mm < 10.0:
                    return "âš ï¸  å¯æ¥å—"
                else:
                    return "âŒ éœ€è¦æ”¹è¿›"

    @staticmethod
    def format_transformation_result(R, t, transform_name="å˜æ¢"):
        """æ ¼å¼åŒ–å˜æ¢çŸ©é˜µç»“æœè¾“å‡º
        
        Args:
            R: æ—‹è½¬çŸ©é˜µ (3x3)
            t: å¹³ç§»å‘é‡ (3x1 æˆ– 3,)
            transform_name: å˜æ¢åç§°
        """
        # ç¡®ä¿å¹³ç§»å‘é‡æ˜¯1Dæ•°ç»„
        if t.ndim > 1:
            t = t.flatten()
        
        # è®¡ç®—æ¬§æ‹‰è§’
        from scipy.spatial.transform import Rotation as R_scipy
        r = R_scipy.from_matrix(R)
        euler_xyz = r.as_euler('xyz', degrees=True)
        quat_xyzw = r.as_quat()  # scipyè¿”å› [x, y, z, w] æ ¼å¼
        
        print(f"  {transform_name}:")
        print(f"    å¹³ç§»å‘é‡ (mm): X={t[0]*1000:.2f}, Y={t[1]*1000:.2f}, Z={t[2]*1000:.2f}")
        print(f"    æ¬§æ‹‰è§’ (åº¦):    Rx={euler_xyz[0]:.2f}Â°, Ry={euler_xyz[1]:.2f}Â°, Rz={euler_xyz[2]:.2f}Â°")
        print(f"    å››å…ƒæ•° (xyzw):  x={quat_xyzw[0]:.4f}, y={quat_xyzw[1]:.4f}, z={quat_xyzw[2]:.4f}, w={quat_xyzw[3]:.4f}")

    @staticmethod
    def invert_rt(R, t):
        """è®¡ç®—å˜æ¢çš„é€†"""
        R_inv = R.T
        t_inv = -R_inv @ t
        return R_inv, t_inv
    
    @staticmethod
    def invert_transformations(R_list, t_list):
        """å¯¹å˜æ¢åˆ—è¡¨æ±‚é€†: T^(-1) = [R^T | -R^T*t]

        ç”¨äº eye-to-hand: å°† gripper2base è½¬æ¢ä¸º base2gripper

        Args:
            R_list: æ—‹è½¬çŸ©é˜µåˆ—è¡¨
            t_list: å¹³ç§»å‘é‡åˆ—è¡¨

        Returns:
            (R_inv_list, t_inv_list): é€†å˜æ¢åˆ—è¡¨
        """
        R_inv_list = []
        t_inv_list = []

        for R, t in zip(R_list, t_list):
            # T^(-1) = [R^T | -R^T * t]
            R_inv, t_inv = HandEyeCalibration.invert_rt(R,t)
            R_inv_list.append(R_inv)
            t_inv_list.append(t_inv)

        return R_inv_list, t_inv_list
    
 

    @staticmethod
    def multi_algorithm_fusion(R_gripper2base, t_gripper2base,
                               R_target2cam, t_target2cam, verbose=True):
        """å¤šç®—æ³•èåˆ - é€‰æ‹©æœ€ä½³ç®—æ³•

        æµ‹è¯•5ç§ç»å…¸æ‰‹çœ¼æ ‡å®šç®—æ³•,é€‰æ‹©ä½å§¿é‡å¤æ€§æœ€å¥½çš„

        Args:
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_cam2gripper, t_cam2gripper, method_name) æˆ– (None, None, None)
        """
        methods = [
            (cv2.CALIB_HAND_EYE_TSAI, "Tsai"),
            (cv2.CALIB_HAND_EYE_PARK, "Park"),
            (cv2.CALIB_HAND_EYE_HORAUD, "Horaud"),
            (cv2.CALIB_HAND_EYE_DANIILIDIS, "Daniilidis"),
            (cv2.CALIB_HAND_EYE_ANDREFF, "Andreff")
        ]

        best_result = None
        best_score = float('inf')
        best_method = None

        for method_id, method_name in methods:
            try:
                R_test, t_test = cv2.calibrateHandEye(
                    R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam,
                    method=method_id
                )

                # è¯„ä¼°æ ‡å®šè´¨é‡: è°ƒç”¨ä¸“é—¨çš„è¯„ä¼°å‡½æ•°
                eval_result = HandEyeCalibration.evaluate_calibration(
                    R_test, t_test, R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam, verbose=False, mode='eye_in_hand'
                )
                
                avg_t_error = eval_result['translation_error_mm']['mean']
                avg_r_error = eval_result['rotation_error_deg']['mean']

                # ç»¼åˆè¯„åˆ†: å½’ä¸€åŒ–ååŠ æƒæ±‚å’Œ
                # å¹³ç§»: 5mm = 1.0, æ—‹è½¬: 0.5Â° = 1.0
                score = (avg_t_error / 5.0) + (avg_r_error / 0.5)

                if verbose:
                    print(f"   {method_name}: å¹³ç§»={avg_t_error:.3f}mm, æ—‹è½¬={avg_r_error:.3f}Â°, ç»¼åˆ={score:.3f}")
                    HandEyeCalibration.format_transformation_result(R_test, t_test, f"{method_name} Camera_to_Gripper")

                if score < best_score:
                    best_score = score
                    best_result = (R_test, t_test)
                    best_method = method_name

            except Exception as e:
                if verbose:
                    print(f"   {method_name}: å¤±è´¥ ({e})")

        if best_result:
            return best_result[0], best_result[1], best_method

        return None, None, None

    @staticmethod
    def multi_algorithm_fusion_eye_to_hand(R_gripper2base, t_gripper2base,
                                               R_target2cam, t_target2cam, verbose=True):
        """Eye-to-Hand å¤šç®—æ³•èåˆ - æ–°ç‰ˆæœ¬ä½¿ç”¨è™šæ‹ŸEye-in-Handæ–¹æ³•
        å°†Eye-to-Handé—®é¢˜è½¬æ¢ä¸ºEye-in-Handé—®é¢˜ï¼Œç„¶åä½¿ç”¨æ ‡å‡†ç®—æ³•æ±‚è§£
        
        Args:
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            tuple: (R_cam2base, t_cam2base, method_name) æˆ– (None, None, None)
        """
        
        # æµ‹è¯•å¤šç§æ ‡å‡†hand-eyeç®—æ³•
        methods = [
            (cv2.CALIB_HAND_EYE_TSAI, "Tsai"),
            (cv2.CALIB_HAND_EYE_PARK, "Park"),
            (cv2.CALIB_HAND_EYE_HORAUD, "Horaud"),
            (cv2.CALIB_HAND_EYE_DANIILIDIS, "Daniilidis"),
            (cv2.CALIB_HAND_EYE_ANDREFF, "Andreff")
        ]
        
        best_result = None
        best_score = float('inf')
        best_method = None
        
        # æ•°æ®é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºè™šæ‹ŸEye-in-Handæ ¼å¼
        R_base2gripper, t_base2gripper = HandEyeCalibration.invert_transformations(R_gripper2base, t_gripper2base)
        for method_id, method_name in methods:
            try:
                # ä½¿ç”¨æ ‡å‡†calibrateHandEyeè·å–cam2baseå˜æ¢
                R_cam2base, t_cam2base = cv2.calibrateHandEye(
                    R_base2gripper, t_base2gripper,
                    R_target2cam, t_target2cam,
                    method=method_id
                )
                
                # è®¡ç®— target2gripper å˜æ¢
                R_target2gripper_list = []
                t_target2gripper_list = []
                
                for i in range(len(R_base2gripper)):
                    # æ„é€ å˜æ¢çŸ©é˜µ
                    T_base2gripper_i = np.eye(4); T_base2gripper_i[:3,:3]=R_base2gripper[i]; T_base2gripper_i[:3,3:4]=t_base2gripper[i]
                    T_target2cam_i = np.eye(4); T_target2cam_i[:3,:3]=R_target2cam[i]; T_target2cam_i[:3,3:4]=t_target2cam[i]
                    T_cam2base = np.eye(4); T_cam2base[:3,:3]=R_cam2base; T_cam2base[:3,3:4]=t_cam2base
                    # è®¡ç®— ^eT_o = ^eT_b * ^bT_c * ^cT_o
                    T_target2gripper_i = T_base2gripper_i @ T_cam2base @ T_target2cam_i
                    R_target2gripper_list.append(T_target2gripper_i[:3,:3])
                    t_target2gripper_list.append(T_target2gripper_i[:3,3:4])

                # æ—‹è½¬çŸ©é˜µå¹³å‡ï¼ˆå››å…ƒæ•°å¹³å‡ï¼‰
                
                quats_target2gripper = [sst.Rotation.from_matrix(R).as_quat() for R in R_target2gripper_list]
                mean_quat_target2gripper = np.mean(quats_target2gripper, axis=0)
                mean_quat_target2gripper = mean_quat_target2gripper / np.linalg.norm(mean_quat_target2gripper)
                R_target2gripper_avg = sst.Rotation.from_quat(mean_quat_target2gripper).as_matrix()
                t_target2gripper_avg = np.mean(t_target2gripper_list, axis=0)
                R_target2gripper, t_target2gripper =R_target2gripper_avg, t_target2gripper_avg

                # è¯„ä¼°æ ‡å®šè´¨é‡
                eval_result = HandEyeCalibration.evaluate_calibration_eye_to_hand(
                    R_cam2base, t_cam2base, R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam, R_target2gripper, t_target2gripper,
                    verbose=False
                )

                avg_t_error = eval_result['translation_error_mm']['mean']
                avg_r_error = eval_result['rotation_error_deg']['mean']
                
                # ç»¼åˆè¯„åˆ†
                score = (avg_t_error / 5.0) + (avg_r_error / 0.5)
                
                if verbose:
                    print(f"   {method_name}: å¹³ç§»={avg_t_error:.3f}mm, æ—‹è½¬={avg_r_error:.3f}Â°, ç»¼åˆ={score:.3f}")
                    HandEyeCalibration.format_transformation_result(R_cam2base, t_cam2base, f"{method_name} Camera_to_Base")
                    HandEyeCalibration.format_transformation_result(R_target2gripper, t_target2gripper, f"{method_name} Target_to_Gripper")
                
                if score < best_score:
                    best_score = score
                    best_result = (R_cam2base, t_cam2base, R_target2gripper, t_target2gripper)
                    best_method = method_name
                
            except Exception as e:
                if verbose:
                    print(f"   {method_name}: å¤±è´¥ ({type(e).__name__}: {str(e)})")
        
        if best_result:
            # ä¿å­˜å®Œæ•´çš„æ ‡å®šç»“æœåˆ°ç±»å˜é‡
            HandEyeCalibration._calibration_details = {
                'R_cam2base': best_result[0],
                't_cam2base': best_result[1],
                'R_target2gripper': best_result[2],
                't_target2gripper': best_result[3],
                'method': best_method,
                'mode': 'eye_to_hand'
            }
            return best_result[0], best_result[1], best_method
        
        return None, None, None

    @staticmethod
    def multi_algorithm_fusion_eye_to_hand_todo(R_gripper2base, t_gripper2base,
                                            R_target2cam, t_target2cam, verbose=True):
        """Eye-to-Hand å¤šç®—æ³•èåˆ - ä½¿ç”¨ calibrateRobotWorldHandEye (OpenCV 4.7+)
        Args:
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨ (éœ€è¦æ±‚é€†)
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨ (éœ€è¦æ±‚é€†)
            R_target2cam: æ ‡é¶(ä¸–ç•Œåæ ‡ç³»)åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶(ä¸–ç•Œåæ ‡ç³»)åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_cam2base, t_cam2base, method_name) æˆ– (None, None, None)
                   æ³¨æ„: è¿”å›çš„æ˜¯ç›¸æœºåˆ°åŸºåº§çš„å˜æ¢,æ–¹ä¾¿åç»­ä½¿ç”¨
        """
        # å‡†å¤‡æ­£ç¡®çš„è¾“å…¥: éœ€è¦ base2gripper (gripper2base çš„é€†)
        R_base2gripper, t_base2gripper = HandEyeCalibration.invert_transformations(
            R_gripper2base, t_gripper2base
        )

        # ä½¿ç”¨ calibrateRobotWorldHandEye æ–¹æ³•
        methods = [
            (cv2.CALIB_ROBOT_WORLD_HAND_EYE_SHAH, "Shah"),
            (cv2.CALIB_ROBOT_WORLD_HAND_EYE_LI, "Li")
        ]

        best_result = None
        best_score = float('inf')
        best_method = None

        for method_id, method_name in methods:
            try:
                R_cam2base, t_cam2base,_,_ = cv2.calibrateRobotWorldHandEye(
                    R_world2cam=R_target2cam,
                    t_world2cam=t_target2cam,
                    R_base2gripper=R_gripper2base,
                    t_base2gripper=t_gripper2base,
                    method=method_id
                )

                # è®¡ç®— target2gripper å˜æ¢
                R_target2gripper_list = []
                t_target2gripper_list = []
                
                for i in range(len(R_base2gripper)):
                    # æ„é€ å˜æ¢çŸ©é˜µ
                    T_base2gripper_i = np.eye(4); T_base2gripper_i[:3,:3]=R_base2gripper[i]; T_base2gripper_i[:3,3:4]=t_base2gripper[i]
                    T_target2cam_i = np.eye(4); T_target2cam_i[:3,:3]=R_target2cam[i]; T_target2cam_i[:3,3:4]=t_target2cam[i]
                    T_cam2base = np.eye(4); T_cam2base[:3,:3]=R_cam2base; T_cam2base[:3,3:4]=t_cam2base
                    # è®¡ç®— ^eT_o = ^eT_b * ^bT_c * ^cT_o
                    T_target2gripper_i = T_base2gripper_i @ T_cam2base @ T_target2cam_i
                    R_target2gripper_list.append(T_target2gripper_i[:3,:3])
                    t_target2gripper_list.append(T_target2gripper_i[:3,3:4])

                # æ—‹è½¬çŸ©é˜µå¹³å‡ï¼ˆå››å…ƒæ•°å¹³å‡ï¼‰
                
                quats_target2gripper = [sst.Rotation.from_matrix(R).as_quat() for R in R_target2gripper_list]
                mean_quat_target2gripper = np.mean(quats_target2gripper, axis=0)
                mean_quat_target2gripper = mean_quat_target2gripper / np.linalg.norm(mean_quat_target2gripper)
                R_target2gripper_avg = sst.Rotation.from_quat(mean_quat_target2gripper).as_matrix()
                t_target2gripper_avg = np.mean(t_target2gripper_list, axis=0)
                R_target2gripper, t_target2gripper = R_target2gripper_avg, t_target2gripper_avg
 
                
                
                # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if np.linalg.norm(t_cam2base) < 1e-6:
                    if verbose:
                        print(f"   {method_name}: å¤±è´¥ (æ— æ•ˆç»“æœ: é›¶å¹³ç§»å‘é‡)")
                    continue

                
                eval_result = HandEyeCalibration.evaluate_calibration_eye_to_hand(
                    R_cam2base, t_cam2base, R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam, R_target2gripper, t_target2gripper,
                    verbose=False
                )
                
                avg_t_error = eval_result['translation_error_mm']['mean']
                avg_r_error = eval_result['rotation_error_deg']['mean']

                # ç»¼åˆè¯„åˆ†: å½’ä¸€åŒ–ååŠ æƒæ±‚å’Œ
                # å¹³ç§»: 5mm = 1.0, æ—‹è½¬: 0.5Â° = 1.0
                score = (avg_t_error / 5.0) + (avg_r_error / 0.5)

                if verbose:
                    print(f"   {method_name}: å¹³ç§»={avg_t_error:.3f}mm, æ—‹è½¬={avg_r_error:.3f}Â°, ç»¼åˆ={score:.3f}")
                    HandEyeCalibration.format_transformation_result(R_cam2base, t_cam2base, f"{method_name} Camera_to_Base")
                    HandEyeCalibration.format_transformation_result(R_target2gripper, t_target2gripper, f"{method_name} Target_to_Gripper")

                if score < best_score:
                    best_score = score
                    # ä¿å­˜ä¸»ç»“æœå’Œå‰¯ç»“æœ
                    best_result = (R_cam2base, t_cam2base, R_target2gripper, t_target2gripper)
                    best_method = method_name

            except Exception as e:
                if verbose:
                    print(f"   {method_name}: å¤±è´¥ ({type(e).__name__}: {str(e)})")

        if best_result:
            # ä¿å­˜å®Œæ•´çš„æ ‡å®šç»“æœï¼ˆåŒ…æ‹¬å‰¯ç»“æœï¼‰åˆ°ç±»å˜é‡
            HandEyeCalibration._calibration_details = {
                'R_cam2base': best_result[0],
                't_cam2base': best_result[1],
                'R_target2gripper': best_result[2],
                't_target2gripper': best_result[3],
                'method': best_method,
                'mode': 'eye_to_hand'
            }
            return best_result[0], best_result[1], best_method

        return None, None, None

    @staticmethod
    def multi_algorithm_fusion_with_mode(R_gripper2base, t_gripper2base,
                                          R_target2cam, t_target2cam,
                                          mode='eye_in_hand', verbose=True):
        """å¤šç®—æ³•èåˆ - æ”¯æŒ eye-in-hand å’Œ eye-to-hand

        Args:
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            mode: 'eye_in_hand' æˆ– 'eye_to_hand'
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_result, t_result, method_name) æˆ– (None, None, None)
        """
        if verbose:
            if mode == 'eye_in_hand':
                print(f"  æ ‡å®šæ¨¡å¼: Eye-in-Hand (ç›¸æœºåœ¨æœ«ç«¯)")
                print(f"  æ±‚è§£: camera_to_gripper")
            else:
                print(f"  æ ‡å®šæ¨¡å¼: Eye-to-Hand (ç›¸æœºå›ºå®š)")
                print(f"  æ±‚è§£: camera_to_base")

        if mode == 'eye_to_hand':
            return HandEyeCalibration.multi_algorithm_fusion_eye_to_hand(
                R_gripper2base, t_gripper2base,
                R_target2cam, t_target2cam, verbose=verbose
            )
        else:
            # Eye-in-hand: ä½¿ç”¨æ ‡å‡†æ¥å£
            return HandEyeCalibration.multi_algorithm_fusion(
                R_gripper2base, t_gripper2base,
                R_target2cam, t_target2cam, verbose=verbose
            )


    @staticmethod
    def levenberg_marquardt_optimization(R_initial, t_initial, R_gripper2base,
                                         t_gripper2base, R_target2cam, t_target2cam,
                                         verbose=True):
        """Levenberg-Marquardt éçº¿æ€§ä¼˜åŒ–

        ä½¿ç”¨ LM ç®—æ³•ä¼˜åŒ–æ ‡å®šç»“æœï¼Œæœ€å°åŒ–ä½å§¿é‡å¤æ€§æ®‹å·®

        Args:
            R_initial: åˆå§‹æ—‹è½¬çŸ©é˜µ
            t_initial: åˆå§‹å¹³ç§»å‘é‡
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_optimized, t_optimized)
        """
        from scipy.optimize import least_squares

        def residual_function(params):
            """æ®‹å·®å‡½æ•°: è®¡ç®—æ‰€æœ‰å¸§çš„ä½å§¿åå·®

            è¿”å›æ®‹å·®å‘é‡ï¼ŒLM ä¼šæœ€å°åŒ– sum(residuals^2)
            """
            # è§£æå‚æ•°
            rvec = params[:3]
            tvec = params[3:6]

            # è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            R_opt, _ = cv2.Rodrigues(rvec)
            t_opt = tvec.reshape(3, 1)

            # è®¡ç®—æ¯å¸§ä¸‹ target åœ¨ base åæ ‡ç³»çš„ä½å§¿
            R_preds = []
            t_preds = []

            for i in range(len(R_gripper2base)):
                R_pred = R_gripper2base[i] @ R_opt @ R_target2cam[i]
                t_pred = R_gripper2base[i] @ (R_opt @ t_target2cam[i] + t_opt) + t_gripper2base[i]
                R_preds.append(R_pred)
                t_preds.append(t_pred)

            # ä»¥ç¬¬ä¸€å¸§ä¸ºå‚è€ƒï¼Œè®¡ç®—æ‰€æœ‰å¸§çš„æ®‹å·®
            R_ref = R_preds[0]
            t_ref = t_preds[0]

            residuals = []
            for i in range(1, len(R_preds)):
                # æ—‹è½¬è¯¯å·® (åº¦)
                R_error = R_ref.T @ R_preds[i]
                angle_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))

                # å¹³ç§»è¯¯å·® (mm)
                t_error = np.linalg.norm(t_preds[i] - t_ref) * 1000

                # æ·»åŠ åˆ°æ®‹å·®å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼šæ—‹è½¬æƒé‡ 0.3ï¼Œå¹³ç§»æƒé‡ 0.7ï¼‰
                residuals.append(angle_error * 10.0 * 0.3)  # æ—‹è½¬: 1Â° = 10mm
                residuals.append(t_error * 0.7)               # å¹³ç§»: mm

            return np.array(residuals)

        # åˆå§‹å‚æ•°
        rvec_init, _ = cv2.Rodrigues(R_initial)
        tvec_init = t_initial.flatten()
        params_init = np.concatenate([rvec_init.flatten(), tvec_init])

        # è®¡ç®—åˆå§‹æ®‹å·®
        residuals_before = residual_function(params_init)
        error_before = np.sum(residuals_before**2)

        # è®¾ç½®è¾¹ç•Œ: å…è®¸å¾®è°ƒ (Â±5Â°, Â±10mm)
        lower_bounds = []
        upper_bounds = []
        for i in range(3):
            # æ—‹è½¬: Â±5Â° â‰ˆ Â±0.087 rad
            lower_bounds.append(rvec_init[i] - 0.087)
            upper_bounds.append(rvec_init[i] + 0.087)
        for i in range(3):
            # å¹³ç§»: Â±10mm = Â±0.01m
            lower_bounds.append(tvec_init[i] - 0.01)
            upper_bounds.append(tvec_init[i] + 0.01)

        # Levenberg-Marquardt ä¼˜åŒ–
        result = least_squares(
            residual_function,
            params_init,
            method='lm',  # Levenberg-Marquardt
            ftol=1e-8,
            xtol=1e-8,
            gtol=1e-8,
            max_nfev=100,
            verbose=0
        )

        # è§£æç»“æœ
        rvec_opt = result.x[:3]
        tvec_opt = result.x[3:6]
        R_opt, _ = cv2.Rodrigues(rvec_opt)
        t_opt = tvec_opt.reshape(3, 1)

        # è®¡ç®—ä¼˜åŒ–åæ®‹å·®
        residuals_after = residual_function(result.x)
        error_after = np.sum(residuals_after**2)

        # å¦‚æœä¼˜åŒ–åæ›´å·®ï¼Œé€€å›åˆå§‹å€¼
        if error_after > error_before:
            if verbose:
                print(f"   âš ï¸  LMä¼˜åŒ–æœªæ”¹å–„ç»“æœï¼Œä½¿ç”¨åˆå§‹å€¼")
                print(f"   ä¼˜åŒ–å‰: {error_before:.3f}, ä¼˜åŒ–å: {error_after:.3f}")
            return R_initial, t_initial
        else:
            improvement = (error_before - error_after) / error_before * 100
            if verbose:
                print(f"   âœ… LMä¼˜åŒ–æˆåŠŸ: {error_before:.3f} â†’ {error_after:.3f} (æ”¹å–„{improvement:.1f}%)")
                print(f"   è¿­ä»£æ¬¡æ•°: {result.nfev}, çŠ¶æ€: {result.status} ({result.message})")
            return R_opt, t_opt

    @staticmethod
    def levenberg_marquardt_optimization_eye_to_hand(R_initial, t_initial, R_gripper2base,
                                                      t_gripper2base, R_target2cam, t_target2cam,
                                                      verbose=True):
        """Eye-to-Hand Levenberg-Marquardt éçº¿æ€§ä¼˜åŒ–

        åŸºäºæ­£ç¡®çš„Eye-to-Handæ ‡å®šæ–¹ç¨‹åŒæ—¶ä¼˜åŒ– cam2base å’Œ target2gripper å˜æ¢
        
        æ ‡å®šæ–¹ç¨‹: T_gripper2base * T_target2gripper = T_cam2base * T_target2cam

        Args:
            R_initial: åˆå§‹æ—‹è½¬çŸ©é˜µ (cam2base)
            t_initial: åˆå§‹å¹³ç§»å‘é‡ (cam2base)
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            R_target2gripper_initial: åˆå§‹target2gripperæ—‹è½¬çŸ©é˜µï¼ˆå¯é€‰ï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_cam2base_optimized, t_cam2base_optimized)
        """
        from scipy.optimize import least_squares

        def residual_function(params):
            """
            æ®‹å·®å‡½æ•°ï¼šåŸºäºEye-to-Handæ ‡å®šæ–¹ç¨‹
            
            æ ‡å®šæ–¹ç¨‹: T_gripper2base * T_target2gripper = T_cam2base * T_target2cam
            
            params = [rvec_cam2base, t_cam2base, rvec_target2gripper, t_target2gripper] (12ä¸ªå‚æ•°)
            """
            # è§£æå‚æ•°
            rvec_cam2base = params[:3]
            t_cam2base = params[3:6].reshape(3, 1)
            rvec_target2gripper = params[6:9]
            t_target2gripper = params[9:12].reshape(3, 1)

            # è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
            R_cam2base, _ = cv2.Rodrigues(rvec_cam2base)
            R_target2gripper, _ = cv2.Rodrigues(rvec_target2gripper)

            residuals = []
            for i in range(len(R_gripper2base)):
                # å·¦è¾¹ï¼šT_gripper2base * T_target2gripper (è·¯å¾„1: Target â†’ Gripper â†’ Base)
                T_gripper2base = np.eye(4)
                T_gripper2base[:3, :3] = R_gripper2base[i]
                T_gripper2base[:3, 3:4] = t_gripper2base[i]
                
                T_target2gripper = np.eye(4)
                T_target2gripper[:3, :3] = R_target2gripper
                T_target2gripper[:3, 3:4] = t_target2gripper
                
                left = T_gripper2base @ T_target2gripper  # Target â†’ Gripper â†’ Base
                
                # å³è¾¹ï¼šT_cam2base * T_target2cam (è·¯å¾„2: Target â†’ Camera â†’ Base)
                T_cam2base = np.eye(4)
                T_cam2base[:3, :3] = R_cam2base
                T_cam2base[:3, 3:4] = t_cam2base
                
                T_target2cam = np.eye(4)
                T_target2cam[:3, :3] = R_target2cam[i]
                T_target2cam[:3, 3:4] = t_target2cam[i]
                
                right = T_cam2base @ T_target2cam  # Target â†’ Camera â†’ Base
                
                # è®¡ç®—å˜æ¢è¯¯å·®ï¼šä¸¤æ¡è·¯å¾„åº”è¯¥ç»™å‡ºç›¸åŒçš„Targetåœ¨Baseä¸­çš„ä½å§¿
                error_T = left - right
                
                # æ—‹è½¬è¯¯å·® (åº¦)
                R_error = error_T[:3, :3]
                angle_error = np.linalg.norm(R_error, 'fro') * 180 / np.pi
                
                # å¹³ç§»è¯¯å·® (mm)
                t_error = np.linalg.norm(error_T[:3, 3]) * 1000
                
                # æ·»åŠ åˆ°æ®‹å·®å‘é‡
                residuals.append(angle_error)  # æ—‹è½¬æƒé‡
                residuals.append(t_error)       # å¹³ç§»æƒé‡
            
            return np.array(residuals)

        # ä»æ ‡å®šç»“æœä¸­æå–target2gripperåˆå§‹å€¼
        R_target2gripper_initial = HandEyeCalibration._calibration_details['R_target2gripper']
        t_target2gripper_initial = HandEyeCalibration._calibration_details['t_target2gripper']

        # åˆå§‹å‚æ•°
        rvec_cam2base_init, _ = cv2.Rodrigues(R_initial)
        rvec_target2gripper_init, _ = cv2.Rodrigues(R_target2gripper_initial)
        
        params_init = np.concatenate([
            rvec_cam2base_init.ravel(),
            t_initial.ravel(),
            rvec_target2gripper_init.ravel(),
            t_target2gripper_initial.ravel()
        ])

        # è®¡ç®—åˆå§‹æ®‹å·®
        residuals_before = residual_function(params_init)
        error_before = np.sum(residuals_before**2)

        # è®¾ç½®è¾¹ç•Œ: å…è®¸å¾®è°ƒ
        lower_bounds = []
        upper_bounds = []
        # cam2baseæ—‹è½¬è¾¹ç•Œ (Â±5Â°)
        for i in range(3):
            lower_bounds.append(rvec_cam2base_init[i] - 0.087)
            upper_bounds.append(rvec_cam2base_init[i] + 0.087)
        # cam2baseå¹³ç§»è¾¹ç•Œ (Â±10mm)
        for i in range(3):
            lower_bounds.append(t_initial[i] - 0.01)
            upper_bounds.append(t_initial[i] + 0.01)
        # target2gripperæ—‹è½¬è¾¹ç•Œ (Â±10Â°)
        for i in range(3):
            lower_bounds.append(rvec_target2gripper_init[i] - 0.175)
            upper_bounds.append(rvec_target2gripper_init[i] + 0.175)
        # target2gripperå¹³ç§»è¾¹ç•Œ (Â±20mm)
        for i in range(3):
            lower_bounds.append(t_target2gripper_initial[i] - 0.02)
            upper_bounds.append(t_target2gripper_initial[i] + 0.02)

        # Levenberg-Marquardt ä¼˜åŒ–
        result = least_squares(
            residual_function,
            params_init,
            method='lm',  # Levenberg-Marquardt
            ftol=1e-10,
            xtol=1e-10,
            gtol=1e-10,
            max_nfev=1000,
            verbose=0
        )

        # è§£æç»“æœ
        rvec_cam2base_opt = result.x[:3]
        t_cam2base_opt = result.x[3:6].reshape(3, 1)
        rvec_target2gripper_opt = result.x[6:9]
        t_target2gripper_opt = result.x[9:12].reshape(3, 1)
        
        R_cam2base_opt, _ = cv2.Rodrigues(rvec_cam2base_opt)
        R_target2gripper_opt, _ = cv2.Rodrigues(rvec_target2gripper_opt)

        # è®¡ç®—ä¼˜åŒ–åæ®‹å·®
        residuals_after = residual_function(result.x)
        error_after = np.sum(residuals_after**2)

        # å¦‚æœä¼˜åŒ–åæ›´å·®ï¼Œé€€å›åˆå§‹å€¼
        if error_after > error_before:
            if verbose:
                print(f"   âš ï¸  LMä¼˜åŒ–æœªæ”¹å–„ç»“æœï¼Œä½¿ç”¨åˆå§‹å€¼")
                print(f"   ä¼˜åŒ–å‰: {error_before:.3f}, ä¼˜åŒ–å: {error_after:.3f}")
            return R_initial, t_initial
        else:
            improvement = (error_before - error_after) / error_before * 100
            if verbose:
                print(f"   âœ… LMä¼˜åŒ–æˆåŠŸ: {error_before:.3f} â†’ {error_after:.3f} (æ”¹å–„{improvement:.1f}%)")
                print(f"   è¿­ä»£æ¬¡æ•°: {result.nfev}, çŠ¶æ€: {result.status} ({result.message})")
                status = "æ”¶æ•›" if result.success else "æœªæ”¶æ•›"
                print(f"   ä¼˜åŒ–çŠ¶æ€: {status}")
            
            # æ›´æ–°æ ‡å®šç»“æœä¸­çš„target2gripperå˜æ¢
            HandEyeCalibration._calibration_details['R_target2gripper'] = R_target2gripper_opt
            HandEyeCalibration._calibration_details['t_target2gripper'] = t_target2gripper_opt
            
            return R_cam2base_opt, t_cam2base_opt

    @staticmethod
    def levenberg_marquardt_optimization_with_mode(R_initial, t_initial, R_gripper2base,
                                                    t_gripper2base, R_target2cam, t_target2cam,
                                                    mode='eye_in_hand', verbose=True):
        """Levenberg-Marquardt ä¼˜åŒ– - æ”¯æŒä¸¤ç§æ¨¡å¼

        Args:
            R_initial: åˆå§‹æ—‹è½¬çŸ©é˜µ
            t_initial: åˆå§‹å¹³ç§»å‘é‡
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            mode: 'eye_in_hand' æˆ– 'eye_to_hand'
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            tuple: (R_optimized, t_optimized)
        """
        # Eye-to-hand: ä½¿ç”¨ä¸“ç”¨ä¼˜åŒ–æ–¹æ³•
        if mode == 'eye_to_hand':
            return HandEyeCalibration.levenberg_marquardt_optimization_eye_to_hand(
                R_initial, t_initial, R_gripper2base, t_gripper2base,
                R_target2cam, t_target2cam, verbose=verbose
            )
            #return R_initial, t_initial
        else:
            # Eye-in-hand: ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–æ–¹æ³•
            return HandEyeCalibration.levenberg_marquardt_optimization(
                R_initial, t_initial, R_gripper2base, t_gripper2base,
                R_target2cam, t_target2cam, verbose=verbose
            )

    @staticmethod
    def evaluate_calibration_eye_to_hand(R_cam2base, t_cam2base, R_gripper2base,
                                          t_gripper2base, R_target2cam, t_target2cam,
                                          R_target2gripper, t_target2gripper,
                                          verbose=True, detail=False):
        """è¯„ä¼° Eye-to-Hand æ ‡å®šç»“æœè´¨é‡
        
        åŸºäºæ ‡å®šæ–¹ç¨‹ T_gripper2base * T_target2gripper = T_cam2base * T_target2cam è®¡ç®—æ®‹å·®
        
        Args:
            R_cam2base: ç›¸æœºåˆ°åŸºåº§çš„æ—‹è½¬çŸ©é˜µ
            t_cam2base: ç›¸æœºåˆ°åŸºåº§çš„å¹³ç§»å‘é‡
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            R_target2gripper: æ ‡é¶åˆ°æœºå™¨äººæœ«ç«¯çš„æ—‹è½¬çŸ©é˜µ
            t_target2gripper: æ ‡é¶åˆ°æœºå™¨äººæœ«ç«¯çš„å¹³ç§»å‘é‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        # è¯„ä¼°æ ‡å®šè´¨é‡ï¼šåŸºäºEye-to-Handæ ‡å®šæ–¹ç¨‹çš„æ®‹å·®
        t_errors = []
        r_errors = []
        
        for i in range(len(R_gripper2base)):
            # å·¦è¾¹ï¼šT_gripper2base * T_target2gripper (è·¯å¾„1: Target â†’ Gripper â†’ Base)
            R_gripper2base_i, t_gripper2base_i = R_gripper2base[i], t_gripper2base[i]
            T_gripper2base = np.eye(4)
            T_gripper2base[:3, :3] = R_gripper2base_i
            T_gripper2base[:3, 3:4] = t_gripper2base_i
            
            T_target2gripper = np.eye(4)
            T_target2gripper[:3, :3] = R_target2gripper
            T_target2gripper[:3, 3:4] = t_target2gripper
            
            left = T_gripper2base @ T_target2gripper  # Target â†’ Gripper â†’ Base
            
            # å³è¾¹ï¼šT_cam2base * T_target2cam (è·¯å¾„2: Target â†’ Camera â†’ Base)
            T_cam2base = np.eye(4)
            T_cam2base[:3, :3] = R_cam2base
            T_cam2base[:3, 3:4] = t_cam2base
            
            T_target2cam = np.eye(4)
            T_target2cam[:3, :3] = R_target2cam[i]
            T_target2cam[:3, 3:4] = t_target2cam[i]
            
            right = T_cam2base @ T_target2cam  # Target â†’ Camera â†’ Base
            
            # è®¡ç®—å˜æ¢è¯¯å·®ï¼šä¸¤æ¡è·¯å¾„åº”è¯¥ç»™å‡ºç›¸åŒçš„Targetåœ¨Baseä¸­çš„ä½å§¿
            error_T = left - right
            
            # æ—‹è½¬è¯¯å·® (åº¦) - ä½¿ç”¨FrobeniusèŒƒæ•°
            R_error = error_T[:3, :3]
            angle_error = np.linalg.norm(R_error, 'fro') * 180 / np.pi
            r_errors.append(angle_error)
            
            # å¹³ç§»è¯¯å·® (mm)
            t_error = np.linalg.norm(error_T[:3, 3]) * 1000
            t_errors.append(t_error)

        # ç»Ÿè®¡é‡
        result = {
            'translation_error_mm': {
                'mean': np.mean(t_errors),
                'std': np.std(t_errors),
                'max': np.max(t_errors),
                'median': np.median(t_errors)
            },
            'rotation_error_deg': {
                'mean': np.mean(r_errors),
                'std': np.std(r_errors),
                'max': np.max(r_errors),
                'median': np.median(r_errors)
            }
        }

        # è´¨é‡è¯„çº§
        avg_t = result['translation_error_mm']['mean']
        avg_r = result['rotation_error_deg']['mean']
        quality_emoji = HandEyeCalibration.get_calibration_quality(avg_t, avg_r, mode='eye_to_hand')
        result['quality'] = quality_emoji

        if verbose:
            print(f"\nğŸ“Š æ ‡å®šè´¨é‡è¯„ä¼° (Eye-to-Hand):")
            print(f"  éªŒè¯æ–¹æ³•: å‰å‘è¿åŠ¨å­¦ä¸ç›¸æœºè§‚æµ‹æ¯”è¾ƒ")
            print(f"    æ–¹æ³•1: ç›¸æœºç›´æ¥è§‚æµ‹targetä½å§¿")
            print(f"    æ–¹æ³•2: å‰å‘è¿åŠ¨å­¦é¢„æµ‹targetä½å§¿ (gripper2cam)")
            print(f"    æ¯”è¾ƒä¸¤ç§æ–¹æ³•åœ¨camåæ ‡ç³»ä¸‹çš„targetä½å§¿å·®å¼‚")
            print(f"  å¹³ç§»è¯¯å·®: {avg_t:.2f}Â±{result['translation_error_mm']['std']:.2f}mm (max={result['translation_error_mm']['max']:.2f}mm)")
            print(f"  æ—‹è½¬è¯¯å·®: {avg_r:.3f}Â±{result['rotation_error_deg']['std']:.3f}Â° (max={result['rotation_error_deg']['max']:.3f}Â°)")
            print(f"  è´¨é‡è¯„çº§: {quality_emoji}")

        # Detail mode: æ‰“å°æ¯å¸§è¯¦ç»†ä¿¡æ¯  
        if detail:
            print(f"\n============================================================")
            print(f"ğŸ“Š æœ€ç»ˆæ ‡å®šè´¨é‡åˆ†æ")
            print(f"============================================================")
            
            # é¦–å…ˆè®¡ç®—æ‰€æœ‰å¸§çš„targetåœ¨baseåæ ‡ç³»ä¸‹çš„ä½å§¿
            target_poses_in_base = []
            for i in range(len(R_gripper2base)):
                # è®¡ç®—targetåœ¨baseåæ ‡ç³»ä¸‹çš„ä½å§¿ï¼šT_gripper2base * T_target2gripper
                T_gb = np.eye(4)
                T_gb[:3, :3] = R_gripper2base[i]
                T_gb[:3, 3:4] = t_gripper2base[i]
                
                T_tg = np.eye(4)
                T_tg[:3, :3] = R_target2gripper
                T_tg[:3, 3:4] = t_target2gripper
                
                T_target_in_base = T_gb @ T_tg
                target_poses_in_base.append(T_target_in_base)
            
            for i in range(len(R_gripper2base)):
                # è®¡ç®—å½“å‰å¸§targetåœ¨baseåæ ‡ç³»çš„ä½å§¿è¯¦ç»†ä¿¡æ¯
                pos_mm = target_poses_in_base[i][:3, 3] * 1000  # è½¬æ¢ä¸ºæ¯«ç±³
                from scipy.spatial.transform import Rotation as R_scipy
                euler_deg = R_scipy.from_matrix(target_poses_in_base[i][:3, :3]).as_euler('xyz', degrees=True)
                
                pose_info = f" | ä½å§¿: X={pos_mm[0]:+6.1f}, Y={pos_mm[1]:+6.1f}, Z={pos_mm[2]:+6.1f}mm, R={euler_deg[0]:+5.1f}Â°, P={euler_deg[1]:+5.1f}Â°, Y={euler_deg[2]:+5.1f}Â°"
                
                if i == 0:
                    print(f"âœ… å¸§ {i+1:2d}: æ—‹è½¬è¯¯å·®  0.000Â°  å¹³ç§»è¯¯å·®   0.000mm{pose_info}")
                else:
                    # ä½¿ç”¨å·²è®¡ç®—çš„è¯¯å·®å€¼
                    t_error = t_errors[i-1]  # å› ä¸ºt_errorsä»ç¬¬äºŒå¸§å¼€å§‹
                    r_error = r_errors[i-1]  # å› ä¸ºr_errorsä»ç¬¬äºŒå¸§å¼€å§‹
                    
                    # çŠ¶æ€æŒ‡ç¤ºå™¨
                    quality_text = HandEyeCalibration.get_calibration_quality(t_error)
                    status = quality_text.split()[0]  # æå–emojiéƒ¨åˆ†
                    
                    print(f"{status} å¸§ {i+1:2d}: æ—‹è½¬è¯¯å·® {r_error:6.3f}Â°  å¹³ç§»è¯¯å·® {t_error:7.3f}mm{pose_info}")

        return result

    @staticmethod
    def evaluate_calibration(R_cam2gripper, t_cam2gripper, R_gripper2base,
                            t_gripper2base, R_target2cam, t_target2cam,
                            verbose=True, mode='eye_in_hand', detail=False):
        """è¯„ä¼°æ ‡å®šç»“æœè´¨é‡ - æ”¯æŒä¸¤ç§æ¨¡å¼

        è®¡ç®—ä½å§¿é‡å¤æ€§è¯¯å·®å’Œç¨³å®šæ€§

        Args:
            R_cam2gripper: ç›¸æœºåˆ°å¤¹çˆªçš„æ—‹è½¬çŸ©é˜µ (eye-in-hand) æˆ–ç›¸æœºåˆ°åŸºåº§ (eye-to-hand)
            t_cam2gripper: ç›¸æœºåˆ°å¤¹çˆªçš„å¹³ç§»å‘é‡ (eye-in-hand) æˆ–ç›¸æœºåˆ°åŸºåº§ (eye-to-hand)
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            mode: 'eye_in_hand' æˆ– 'eye_to_hand'
            detail: æ˜¯å¦æ˜¾ç¤ºæ¯å¸§è¯¦ç»†ä¿¡æ¯

        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        if mode == 'eye_to_hand':
            # ä»ç±»å˜é‡ä¸­è·å–å‰¯ç»“æœ
            details = HandEyeCalibration._calibration_details
            R_target2gripper = details.get('R_target2gripper')
            t_target2gripper = details.get('t_target2gripper')

            return HandEyeCalibration.evaluate_calibration_eye_to_hand(
                R_cam2gripper, t_cam2gripper, R_gripper2base,
                t_gripper2base, R_target2cam, t_target2cam,
                R_target2gripper, t_target2gripper,
                verbose=verbose, detail=detail
            )

        # Eye-in-hand: è®¡ç®—æ¯å¸§ä¸‹targetåœ¨baseåæ ‡ç³»çš„ä½å§¿
        R_preds = []
        t_preds = []

        for i in range(len(R_gripper2base)):
            R_pred = R_gripper2base[i] @ R_cam2gripper @ R_target2cam[i]
            t_pred = R_gripper2base[i] @ (R_cam2gripper @ t_target2cam[i] + t_cam2gripper) + t_gripper2base[i]
            R_preds.append(R_pred)
            t_preds.append(t_pred)

        # è®¡ç®—é‡å¤æ€§è¯¯å·®
        R_ref = R_preds[0]
        t_ref = t_preds[0]

        t_errors = []
        r_errors = []

        for i in range(1, len(R_preds)):
            # å¹³ç§»è¯¯å·® (mm)
            t_error = np.linalg.norm(t_preds[i] - t_ref) * 1000
            t_errors.append(t_error)

            # æ—‹è½¬è¯¯å·® (åº¦)
            R_error = R_ref.T @ R_preds[i]
            angle = np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1))
            r_error = np.degrees(angle)
            r_errors.append(r_error)

        # ç»Ÿè®¡é‡
        result = {
            'translation_error_mm': {
                'mean': np.mean(t_errors),
                'std': np.std(t_errors),
                'max': np.max(t_errors),
                'median': np.median(t_errors)
            },
            'rotation_error_deg': {
                'mean': np.mean(r_errors),
                'std': np.std(r_errors),
                'max': np.max(r_errors),
                'median': np.median(r_errors)
            }
        }

        # è´¨é‡è¯„çº§
        avg_t = result['translation_error_mm']['mean']
        avg_r = result['rotation_error_deg']['mean']
        quality_emoji = HandEyeCalibration.get_calibration_quality(avg_t, avg_r, mode='eye_in_hand')
        result['quality'] = quality_emoji

        if verbose:
            print(f"\nğŸ“Š æ ‡å®šè´¨é‡è¯„ä¼°:")
            print(f"  ä½å§¿é‡å¤æ€§:")
            print(f"    å¹³ç§»: {avg_t:.2f}Â±{result['translation_error_mm']['std']:.2f}mm (max={result['translation_error_mm']['max']:.2f}mm)")
            print(f"    æ—‹è½¬: {avg_r:.3f}Â±{result['rotation_error_deg']['std']:.3f}Â° (max={result['rotation_error_deg']['max']:.3f}Â°)")
            print(f"  è´¨é‡è¯„çº§: {quality_emoji}")

        # Detail mode: æ‰“å°æ¯å¸§è¯¦ç»†ä¿¡æ¯
        if detail:
            print(f"\n============================================================")
            print(f"ğŸ“Š æœ€ç»ˆæ ‡å®šè´¨é‡åˆ†æ")
            print(f"============================================================")
            
            for i in range(len(R_preds)):
                # è®¡ç®—å½“å‰å¸§çš„ä½å§¿è¯¦ç»†ä¿¡æ¯
                pos_mm = t_preds[i] * 1000  # è½¬æ¢ä¸ºæ¯«ç±³
                from scipy.spatial.transform import Rotation as R_scipy
                euler_deg = R_scipy.from_matrix(R_preds[i]).as_euler('xyz', degrees=True)
                
                pose_info = f" | ä½å§¿: X={pos_mm[0]:+6.1f}, Y={pos_mm[1]:+6.1f}, Z={pos_mm[2]:+6.1f}mm, R={euler_deg[0]:+5.1f}Â°, P={euler_deg[1]:+5.1f}Â°, Y={euler_deg[2]:+5.1f}Â°"
                
                if i == 0:
                    print(f"âœ… å¸§ {i+1:2d}: æ—‹è½¬è¯¯å·®  0.000Â°  å¹³ç§»è¯¯å·®   0.000mm{pose_info}")
                else:
                    # è®¡ç®—ä¸ç¬¬ä¸€å¸§çš„è¯¯å·®
                    t_error = np.linalg.norm(t_preds[i] - t_preds[0]) * 1000
                    R_error = R_preds[0].T @ R_preds[i]
                    angle_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))
                    
                    # çŠ¶æ€æŒ‡ç¤ºå™¨
                    quality_text = HandEyeCalibration.get_calibration_quality(t_error)
                    status = quality_text.split()[0]  # æå–emojiéƒ¨åˆ†
                    
                    print(f"{status} å¸§ {i+1:2d}: æ—‹è½¬è¯¯å·® {angle_error:6.3f}Â°  å¹³ç§»è¯¯å·® {t_error:7.3f}mm{pose_info}")

        return result


# ============================================================================
# è¯¯å·®åˆ†æå·¥å…·
# ============================================================================

class ErrorAnalyzer:
    """è¯¯å·®åˆ†æå·¥å…·"""

    @staticmethod
    def analyze_error_patterns(R_cam2gripper, t_cam2gripper, R_gripper2base,
                               t_gripper2base, R_target2cam, t_target2cam,
                               frame_ids):
        """åˆ†æè¯¯å·®æ¨¡å¼,è¯†åˆ«å¼‚å¸¸å¸§

        Args:
            R_cam2gripper: ç›¸æœºåˆ°å¤¹çˆªçš„æ—‹è½¬çŸ©é˜µ
            t_cam2gripper: ç›¸æœºåˆ°å¤¹çˆªçš„å¹³ç§»å‘é‡
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            R_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„æ—‹è½¬åˆ—è¡¨
            t_target2cam: æ ‡é¶åˆ°ç›¸æœºçš„å¹³ç§»åˆ—è¡¨
            frame_ids: å¸§IDåˆ—è¡¨

        Returns:
            dict: è¯¯å·®åˆ†æç»“æœ
        """
        # è®¡ç®—æ¯å¸§çš„ä½å§¿é¢„æµ‹
        R_preds = []
        t_preds = []

        for i in range(len(R_gripper2base)):
            R_pred = R_gripper2base[i] @ R_cam2gripper @ R_target2cam[i]
            t_pred = R_gripper2base[i] @ (R_cam2gripper @ t_target2cam[i] + t_cam2gripper) + t_gripper2base[i]
            R_preds.append(R_pred)
            t_preds.append(t_pred)

        # ä»¥ç¬¬ä¸€å¸§ä¸ºå‚è€ƒ
        R_ref = R_preds[0]
        t_ref = t_preds[0]

        # è®¡ç®—æ¯å¸§è¯¯å·®
        frame_errors = []
        for i in range(1, len(R_preds)):
            t_error = np.linalg.norm(t_preds[i] - t_ref) * 1000
            R_error = R_ref.T @ R_preds[i]
            r_error = np.degrees(np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1)))

            frame_errors.append({
                'frame_id': frame_ids[i],
                'translation_error_mm': t_error,
                'rotation_error_deg': r_error,
                'total_error': t_error + r_error * 10.0  # 1Â° = 10mm
            })

        # æŒ‰è¯¯å·®æ’åº
        frame_errors.sort(key=lambda x: x['total_error'], reverse=True)

        # è¯†åˆ«å¼‚å¸¸å¸§ (è¶…è¿‡ä¸­ä½æ•° + 2å€MAD)
        errors = np.array([fe['total_error'] for fe in frame_errors])
        median = np.median(errors)
        mad = np.median(np.abs(errors - median))
        threshold = median + 2.0 * mad

        outliers = [fe for fe in frame_errors if fe['total_error'] > threshold]

        return {
            'frame_errors': frame_errors,
            'outliers': outliers,
            'outlier_threshold': threshold,
            'worst_frames': frame_errors[:5]  # æœ€å·®çš„5å¸§
        }

    @staticmethod
    def check_motion_diversity(R_gripper2base, t_gripper2base, verbose=True):
        """æ£€æŸ¥è¿åŠ¨å¤šæ ·æ€§

        Args:
            R_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„æ—‹è½¬åˆ—è¡¨
            t_gripper2base: æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§çš„å¹³ç§»åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            dict: è¿åŠ¨å¤šæ ·æ€§åˆ†æç»“æœ
        """
        # è®¡ç®—ä½ç§»èŒƒå›´
        positions = np.array([t.flatten() for t in t_gripper2base])
        pos_range = positions.max(axis=0) - positions.min(axis=0)
        pos_range_mm = pos_range * 1000

        # è®¡ç®—æ—‹è½¬èŒƒå›´
        euler_angles = []
        for R_mat in R_gripper2base:
            r = R.from_matrix(R_mat)
            euler = r.as_euler('xyz', degrees=True)
            euler_angles.append(euler)

        euler_angles = np.array(euler_angles)
        rot_range = euler_angles.max(axis=0) - euler_angles.min(axis=0)

        result = {
            'position_range_mm': pos_range_mm,
            'rotation_range_deg': rot_range,
            'sufficient_motion': (pos_range_mm > 50).all() and (rot_range > 10).all()
        }

        if verbose:
            print(f"\nğŸ¯ è¿åŠ¨å¤šæ ·æ€§:")
            print(f"  ä½ç§»èŒƒå›´: X={pos_range_mm[0]:.1f}mm, Y={pos_range_mm[1]:.1f}mm, Z={pos_range_mm[2]:.1f}mm")
            print(f"  æ—‹è½¬èŒƒå›´: Roll={rot_range[0]:.1f}Â°, Pitch={rot_range[1]:.1f}Â°, Yaw={rot_range[2]:.1f}Â°")
            print(f"  è¿åŠ¨å……åˆ†: {'âœ…' if result['sufficient_motion'] else 'âŒ'}")

        return result
